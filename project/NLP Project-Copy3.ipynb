{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631578ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk    \n",
    "# !pip install emoji        \n",
    "# !pip install autocorrec\\t    \n",
    "# !pip install xgboost\n",
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e5a27d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nltk.download('all-corpora')\n",
    "# nltk.download('punkt')  # for using word_tokenizer\n",
    "# nltk.download('wordnet')  # for using Lemmatizer\n",
    "# nltk.download('averaged_perceptron_tagger') # for language processing i.e tagging words with their parts of speech (POS)\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f454ef6d-b13b-495d-9af8-8b098ae96f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19e54a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import spacy\n",
    "import nltk\n",
    "import re   # regular expression\n",
    "import string\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import scipy.sparse as sp\n",
    "import joblib\n",
    "import emoji\n",
    "from autocorrect import Speller   # for correcting spelling\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize    # for tokenizing string into words\n",
    "from nltk.stem import WordNetLemmatizer    # for lemmatizing words\n",
    "from nltk.tag import pos_tag # for tagging words with their parts of speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d82ce6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e22a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d02e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.feature_selection import SelectKBest,chi2, mutual_info_classif\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "204dfd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# import xgboost\n",
    "# from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a38aca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9b53798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55fae504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/project_training_data_with_class_labels.csv',dtype=str,delimiter=',',quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aac43a2-648e-452d-b522-a6ee121d573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = ps.DataFrame(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2705797d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(808661, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73aae4ff-3b29-4156-9a7e-d6a07f824017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 808661 entries, 0 to 808660\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   ID               808661 non-null  object\n",
      " 1   Comments         808623 non-null  object\n",
      " 2   Parent Comments  808661 non-null  object\n",
      " 3    Class Labels    808661 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 24.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06e4d84d-5562-4392-9796-16d3048bbfd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                  0\n",
       "Comments           38\n",
       "Parent Comments     0\n",
       " Class Labels       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for null value\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b97a58a0-dc97-4726-84f5-24d24a8e4191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Comments', 'Parent Comments', ' Class Labels '], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "455b7529-d811-4836-9276-79dd25994b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Comments', 'Parent Comments', 'Class Labels'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.rename(columns={' Class Labels ' : 'Class Labels'},inplace=True) # changing col name\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7705e14-dab0-4d2c-87a2-69d2e674df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove data points whose class labels are not present\n",
    "# df_train.dropna(subset=['Comments'], inplace=True)      # then dropped that rows with no value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bde0f58-8aa9-4d9a-a2bc-b991db3c0aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= df_train.replace(to_replace = np.nan, value = '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1747ae66-dffd-4370-a36a-60834afb8c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                 0\n",
       "Comments           0\n",
       "Parent Comments    0\n",
       "Class Labels       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for null value\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad5b66c8-adf1-4b61-aaf0-e8409cfabac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "      <th>Text</th>\n",
       "      <th>Class Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ocxtitan</td>\n",
       "      <td>Central Illinois</td>\n",
       "      <td>Jesus; where do you live?</td>\n",
       "      <td>Jesus; where do you live? Central Illinois</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LeChuckly</td>\n",
       "      <td>To think - CNN used to be the acronym synonymo...</td>\n",
       "      <td>Even The CNN Staff Is Sick Of The Wall-To-Wall...</td>\n",
       "      <td>Even The CNN Staff Is Sick Of The Wall-To-Wall...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>throwitskrub8</td>\n",
       "      <td>But then again; you have to consider that all ...</td>\n",
       "      <td>agree to that part.It can also mean that gujra...</td>\n",
       "      <td>agree to that part.It can also mean that gujra...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresherthanyouuu</td>\n",
       "      <td>ughhhhh</td>\n",
       "      <td>If a guy told you he doesn't use social media ...</td>\n",
       "      <td>If a guy told you he doesn't use social media ...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_kushagra</td>\n",
       "      <td>I should've put the</td>\n",
       "      <td>No; it's just a programming bug. After all; th...</td>\n",
       "      <td>No; it's just a programming bug. After all; th...</td>\n",
       "      <td>sarcastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                           Comments  \\\n",
       "0          ocxtitan                                   Central Illinois   \n",
       "1         LeChuckly  To think - CNN used to be the acronym synonymo...   \n",
       "2     throwitskrub8  But then again; you have to consider that all ...   \n",
       "3  fresherthanyouuu                                            ughhhhh   \n",
       "4         _kushagra                                I should've put the   \n",
       "\n",
       "                                     Parent Comments  \\\n",
       "0                          Jesus; where do you live?   \n",
       "1  Even The CNN Staff Is Sick Of The Wall-To-Wall...   \n",
       "2  agree to that part.It can also mean that gujra...   \n",
       "3  If a guy told you he doesn't use social media ...   \n",
       "4  No; it's just a programming bug. After all; th...   \n",
       "\n",
       "                                                Text   Class Labels  \n",
       "0         Jesus; where do you live? Central Illinois  non-sarcastic  \n",
       "1  Even The CNN Staff Is Sick Of The Wall-To-Wall...  non-sarcastic  \n",
       "2  agree to that part.It can also mean that gujra...  non-sarcastic  \n",
       "3  If a guy told you he doesn't use social media ...  non-sarcastic  \n",
       "4  No; it's just a programming bug. After all; th...      sarcastic  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserting the column before Class Labels col.\n",
    "df_train.insert(loc = 3,\n",
    "        column = 'Text',\n",
    "        value = df_train['Parent Comments'] + \" \" +df_train['Comments'])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bda1c5a8-d7b1-4975-bcbe-1b59dfa42ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(808661, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "600c0908-55f1-4260-8a67-607cd1eed83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we are combining Parent Comment and Comment cols. into one cols.\n",
    "# df_train['Text'] = df_train['Parent Comments'] + \" \" +df_train['Comments']\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe69be2f-6031-4e86-8da7-6c1131466b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                 0\n",
       "Comments           0\n",
       "Parent Comments    0\n",
       "Text               0\n",
       "Class Labels       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for null value\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f06c0da3-a92c-4636-9ff0-d761a162e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we know that adding any thing with nan value gives nan. so in Text col having nan value we replace \n",
    "# # one time with comment col. value where value of parent comment col is nan and other with parent comment where\n",
    "# # comment col. is empty.\n",
    "\n",
    "# # fill the rows of Col. Text having no values with that of col. Parent Comments where Comment col. is empty\n",
    "# df_train['Text'][df_train['Comments'].isnull()] = df_train['Parent Comments'][df_train['Comments'].isnull()]\n",
    "\n",
    "# # fill the rows of Col. Text having no values with that of col. Comments where Parent Comment col. is empty\n",
    "# df_train['Text'][df_train['Parent Comments'].isnull()] = df_train['Comments'][df_train['Parent Comments'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f25f346-4d50-44c2-a230-f57df4745ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to check for null value\n",
    "# df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40700151-3c17-46dd-8544-fba8d3b2e821",
   "metadata": {},
   "source": [
    "Since here there is issue with the format of some rows. So we are manually correcting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b9149b2-3de9-45bc-9c92-e38474afafca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theyoungthaddeus</td>\n",
       "      <td>No one \"needs\" an assault foam dart blaster</td>\n",
       "      <td>Your son has to register those at the county j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just_an_asian_here</td>\n",
       "      <td>Cause all attractive women are uninteresting a...</td>\n",
       "      <td>Likely due to creative and interesting content.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foxprowl</td>\n",
       "      <td>Poser.</td>\n",
       "      <td>Jon Stewart is going to HBO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kd7rzv</td>\n",
       "      <td>Won't be long and Anet will start banning peop...</td>\n",
       "      <td>This post looks like bullshit market manipulat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ellefied</td>\n",
       "      <td>There goes my hope that Kubo does a Kojima as ...</td>\n",
       "      <td>Plus the Japanese typically do not talk shit w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                           Comments  \\\n",
       "0    theyoungthaddeus        No one \"needs\" an assault foam dart blaster   \n",
       "1  Just_an_asian_here  Cause all attractive women are uninteresting a...   \n",
       "2            Foxprowl                                             Poser.   \n",
       "3              kd7rzv  Won't be long and Anet will start banning peop...   \n",
       "4            Ellefied  There goes my hope that Kubo does a Kojima as ...   \n",
       "\n",
       "                                    Parent Comments   \n",
       "0  Your son has to register those at the county j...  \n",
       "1    Likely due to creative and interesting content.  \n",
       "2                        Jon Stewart is going to HBO  \n",
       "3  This post looks like bullshit market manipulat...  \n",
       "4  Plus the Japanese typically do not talk shit w...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/project_test_data.csv',dtype=str,delimiter=',',quoting=3)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b051283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202166, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44d9bd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                   0\n",
       "Comments            15\n",
       "Parent Comments      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for null value\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "540c0fd4-2122-431d-a1e9-60e063c92f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test= df_test.replace(to_replace = np.nan, value = '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "183955a2-8175-474d-857e-b818155eccd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                  0\n",
       "Comments            0\n",
       "Parent Comments     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for null value\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80f774ce-01d9-497f-960d-617046eeb630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Comments', 'Parent Comments '], dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d3a61bd-9873-4723-9791-b0e31ef3f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.rename(columns={'Parent Comments ': 'Parent Comments'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3306398f-bbb9-4ad9-bddc-9388ebaf06b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Comments', 'Parent Comments'], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97354327-3255-4122-8785-bed9ae434166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theyoungthaddeus</td>\n",
       "      <td>No one \"needs\" an assault foam dart blaster</td>\n",
       "      <td>Your son has to register those at the county j...</td>\n",
       "      <td>Your son has to register those at the county j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just_an_asian_here</td>\n",
       "      <td>Cause all attractive women are uninteresting a...</td>\n",
       "      <td>Likely due to creative and interesting content.</td>\n",
       "      <td>Likely due to creative and interesting content...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foxprowl</td>\n",
       "      <td>Poser.</td>\n",
       "      <td>Jon Stewart is going to HBO</td>\n",
       "      <td>Jon Stewart is going to HBO Poser.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kd7rzv</td>\n",
       "      <td>Won't be long and Anet will start banning peop...</td>\n",
       "      <td>This post looks like bullshit market manipulat...</td>\n",
       "      <td>This post looks like bullshit market manipulat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ellefied</td>\n",
       "      <td>There goes my hope that Kubo does a Kojima as ...</td>\n",
       "      <td>Plus the Japanese typically do not talk shit w...</td>\n",
       "      <td>Plus the Japanese typically do not talk shit w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                           Comments  \\\n",
       "0    theyoungthaddeus        No one \"needs\" an assault foam dart blaster   \n",
       "1  Just_an_asian_here  Cause all attractive women are uninteresting a...   \n",
       "2            Foxprowl                                             Poser.   \n",
       "3              kd7rzv  Won't be long and Anet will start banning peop...   \n",
       "4            Ellefied  There goes my hope that Kubo does a Kojima as ...   \n",
       "\n",
       "                                     Parent Comments  \\\n",
       "0  Your son has to register those at the county j...   \n",
       "1    Likely due to creative and interesting content.   \n",
       "2                        Jon Stewart is going to HBO   \n",
       "3  This post looks like bullshit market manipulat...   \n",
       "4  Plus the Japanese typically do not talk shit w...   \n",
       "\n",
       "                                                Text  \n",
       "0  Your son has to register those at the county j...  \n",
       "1  Likely due to creative and interesting content...  \n",
       "2                 Jon Stewart is going to HBO Poser.  \n",
       "3  This post looks like bullshit market manipulat...  \n",
       "4  Plus the Japanese typically do not talk shit w...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are combining Parent Comment and Comment cols. into one cols.\n",
    "df_test['Text'] = df_test['Parent Comments'] + \" \" +df_test['Comments']\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "310e11f6-981b-4262-badc-e772918f5fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD5CAYAAAA5v3LLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAb3klEQVR4nO3df5Bd5X3f8fcHLT9U2xISLFjWiogYuS6otjBbmak7rW15JNXORLgFez2p0aSakUvw1G48ScAzrWywGpTEJiEJ8siVisBxhEqcQXHBeI2MU0+xxOIogABFa4NBlorWXlkWySCz4ts/nu81dzdXz95dSbtI+rxmLvfc7znPc88V597PPc85d48iAjMzs6M5Y7JXwMzMXtscFGZmVuWgMDOzKgeFmZlVOSjMzKzKQWFmZlUdk70Cx9v5558fc+fOnezVMDM7qTz66KM/jojOVvNOuaCYO3cufX19k70aZmYnFUk/PNo8Dz2ZmVmVg8LMzKocFGZmVuWgMDOzKgeFmZlVOSjMzKzKQWFmZlVtB4WkKZL+RtLX8vFMSb2Sduf9jKZlb5TUL2mXpCVN9SskPZ7zbpOkrJ8t6e6sb5M0t6nN8nyO3ZKWH5dXbWZmbRvLD+4+ATwFTMvHNwAPRsQtkm7Ix78j6VKgB7gMeBPwTUlviYgjwFpgJfBd4D5gKXA/sAI4EBGXSOoB1gAfljQTWAV0AwE8KmlLRBw4plf9GjD3hv892atwSnn2lg9M9iqcUrx9Hj+nwrbZ1h6FpC7gA8D/aCovAzbm9Ebgqqb6pog4HBHPAP3AQkmzgGkR8XCUy+rdOaJNo697gEW5t7EE6I2IwQyHXkq4mJnZBGl36OkPgd8GXmmqXRgR+wDy/oKszwaeb1puT9Zm5/TI+rA2ETEEHATOq/RlZmYTZNSgkPQrwP6IeLTNPtWiFpX6eNs0r+NKSX2S+gYGBtpcTTMza0c7exTvAn5V0rPAJuC9kr4MvJDDSeT9/lx+DzCnqX0XsDfrXS3qw9pI6gCmA4OVvoaJiHUR0R0R3Z2dLf/4oZmZjdOoQRERN0ZEV0TMpRyk3hoR/wHYAjTOQloO3JvTW4CePJPpYmAesD2Hpw5JujKPP1w7ok2jr6vzOQJ4AFgsaUaeVbU4a2ZmNkGO5c+M3wJslrQCeA64BiAidkraDDwJDAHX5xlPANcBdwBTKWc73Z/19cBdkvopexI92degpJuBR3K5myJi8BjW2czMxmhMQRERDwEP5fRPgEVHWW41sLpFvQ+Y36L+Ehk0LeZtADaMZT3NzOz48S+zzcysykFhZmZVDgozM6tyUJiZWZWDwszMqhwUZmZW5aAwM7MqB4WZmVU5KMzMrMpBYWZmVQ4KMzOrclCYmVmVg8LMzKocFGZmVuWgMDOzKgeFmZlVjRoUks6RtF3S30raKemzWf+MpB9J2pG39ze1uVFSv6RdkpY01a+Q9HjOuy0viUpeNvXurG+TNLepzXJJu/O2HDMzm1DtXOHuMPDeiHhR0pnAdyQ1LmF6a0T8QfPCki6lXMr0MuBNwDclvSUvh7oWWAl8F7gPWEq5HOoK4EBEXCKpB1gDfFjSTGAV0A0E8KikLRFx4NhetpmZtWvUPYooXsyHZ+YtKk2WAZsi4nBEPAP0AwslzQKmRcTDERHAncBVTW025vQ9wKLc21gC9EbEYIZDLyVczMxsgrR1jELSFEk7gP2UD+5tOevjkh6TtEHSjKzNBp5var4na7NzemR9WJuIGAIOAudV+jIzswnSVlBExJGIWAB0UfYO5lOGkd4MLAD2AZ/PxdWqi0p9vG1+QdJKSX2S+gYGBiqvxMzMxmpMZz1FxE+Bh4ClEfFCBsgrwJeAhbnYHmBOU7MuYG/Wu1rUh7WR1AFMBwYrfY1cr3UR0R0R3Z2dnWN5SWZmNop2znrqlHRuTk8F3gc8ncccGj4IPJHTW4CePJPpYmAesD0i9gGHJF2Zxx+uBe5tatM4o+lqYGsex3gAWCxpRg5tLc6amZlNkHbOepoFbJQ0hRIsmyPia5LukrSAMhT0LPAxgIjYKWkz8CQwBFyfZzwBXAfcAUylnO3UOHtqPXCXpH7KnkRP9jUo6WbgkVzupogYHP/LNTOzsRo1KCLiMeDyFvWPVtqsBla3qPcB81vUXwKuOUpfG4ANo62nmZmdGP5ltpmZVTkozMysykFhZmZVDgozM6tyUJiZWZWDwszMqhwUZmZW5aAwM7MqB4WZmVU5KMzMrMpBYWZmVQ4KMzOrclCYmVmVg8LMzKocFGZmVuWgMDOzKgeFmZlVtXPN7HMkbZf0t5J2Svps1mdK6pW0O+9nNLW5UVK/pF2SljTVr5D0eM67La+dTV5f++6sb5M0t6nN8nyO3ZKWY2ZmE6qdPYrDwHsj4u3AAmCppCuBG4AHI2Ie8GA+RtKllGteXwYsBW7P620DrAVWAvPytjTrK4ADEXEJcCuwJvuaCawC3gksBFY1B5KZmZ14owZFFC/mwzPzFsAyYGPWNwJX5fQyYFNEHI6IZ4B+YKGkWcC0iHg4IgK4c0SbRl/3AItyb2MJ0BsRgxFxAOjl1XAxM7MJ0NYxCklTJO0A9lM+uLcBF0bEPoC8vyAXnw0839R8T9Zm5/TI+rA2ETEEHATOq/RlZmYTpK2giIgjEbEA6KLsHcyvLK5WXVTq423z6hNKKyX1SeobGBiorJqZmY3VmM56ioifAg9Rhn9eyOEk8n5/LrYHmNPUrAvYm/WuFvVhbSR1ANOBwUpfI9drXUR0R0R3Z2fnWF6SmZmNop2znjolnZvTU4H3AU8DW4DGWUjLgXtzegvQk2cyXUw5aL09h6cOSboyjz9cO6JNo6+rga15HOMBYLGkGXkQe3HWzMxsgnS0scwsYGOeuXQGsDkivibpYWCzpBXAc8A1ABGxU9Jm4ElgCLg+Io5kX9cBdwBTgfvzBrAeuEtSP2VPoif7GpR0M/BILndTRAweyws2M7OxGTUoIuIx4PIW9Z8Ai47SZjWwukW9D/hHxzci4iUyaFrM2wBsGG09zczsxPAvs83MrMpBYWZmVQ4KMzOrclCYmVmVg8LMzKocFGZmVuWgMDOzKgeFmZlVOSjMzKzKQWFmZlUOCjMzq3JQmJlZlYPCzMyqHBRmZlbloDAzsyoHhZmZVTkozMysqp1rZs+R9C1JT0naKekTWf+MpB9J2pG39ze1uVFSv6RdkpY01a+Q9HjOuy2vnU1eX/vurG+TNLepzXJJu/O2HDMzm1DtXDN7CPhURHxP0huARyX15rxbI+IPmheWdCnlmteXAW8CvinpLXnd7LXASuC7wH3AUsp1s1cAByLiEkk9wBrgw5JmAquAbiDyubdExIFje9lmZtauUfcoImJfRHwvpw8BTwGzK02WAZsi4nBEPAP0AwslzQKmRcTDERHAncBVTW025vQ9wKLc21gC9EbEYIZDLyVczMxsgozpGEUOCV0ObMvSxyU9JmmDpBlZmw0839RsT9Zm5/TI+rA2ETEEHATOq/Q1cr1WSuqT1DcwMDCWl2RmZqNoOygkvR74C+CTEfEzyjDSm4EFwD7g841FWzSPSn28bV4tRKyLiO6I6O7s7Ky9DDMzG6O2gkLSmZSQ+LOI+CpARLwQEUci4hXgS8DCXHwPMKepeRewN+tdLerD2kjqAKYDg5W+zMxsgrRz1pOA9cBTEfGFpvqspsU+CDyR01uAnjyT6WJgHrA9IvYBhyRdmX1eC9zb1KZxRtPVwNY8jvEAsFjSjBzaWpw1MzObIO2c9fQu4KPA45J2ZO3TwEckLaAMBT0LfAwgInZK2gw8STlj6vo84wngOuAOYCrlbKf7s74euEtSP2VPoif7GpR0M/BILndTRAyO54Wamdn4jBoUEfEdWh8ruK/SZjWwukW9D5jfov4ScM1R+toAbBhtPc3M7MTwL7PNzKzKQWFmZlUOCjMzq3JQmJlZlYPCzMyqHBRmZlbloDAzsyoHhZmZVTkozMysykFhZmZVDgozM6tyUJiZWZWDwszMqhwUZmZW5aAwM7MqB4WZmVW1cynUOZK+JekpSTslfSLrMyX1Stqd9zOa2twoqV/SLklLmupXSHo8592Wl0QlL5t6d9a3SZrb1GZ5PsduScsxM7MJ1c4exRDwqYj4Z8CVwPWSLgVuAB6MiHnAg/mYnNcDXAYsBW6XNCX7WguspFxHe17OB1gBHIiIS4BbgTXZ10xgFfBOYCGwqjmQzMzsxBs1KCJiX0R8L6cPAU8Bs4FlwMZcbCNwVU4vAzZFxOGIeAboBxZKmgVMi4iHIyKAO0e0afR1D7Ao9zaWAL0RMRgRB4BeXg0XMzObAGM6RpFDQpcD24ALI2IflDABLsjFZgPPNzXbk7XZOT2yPqxNRAwBB4HzKn2ZmdkEaTsoJL0e+AvgkxHxs9qiLWpRqY+3TfO6rZTUJ6lvYGCgsmpmZjZWbQWFpDMpIfFnEfHVLL+Qw0nk/f6s7wHmNDXvAvZmvatFfVgbSR3AdGCw0tcwEbEuIrojoruzs7Odl2RmZm1q56wnAeuBpyLiC02ztgCNs5CWA/c21XvyTKaLKQett+fw1CFJV2af145o0+jramBrHsd4AFgsaUYexF6cNTMzmyAdbSzzLuCjwOOSdmTt08AtwGZJK4DngGsAImKnpM3Ak5Qzpq6PiCPZ7jrgDmAqcH/eoATRXZL6KXsSPdnXoKSbgUdyuZsiYnB8L9XMzMZj1KCIiO/Q+lgBwKKjtFkNrG5R7wPmt6i/RAZNi3kbgA2jraeZmZ0Y/mW2mZlVOSjMzKzKQWFmZlUOCjMzq3JQmJlZlYPCzMyqHBRmZlbloDAzsyoHhZmZVTkozMysykFhZmZVDgozM6tyUJiZWZWDwszMqhwUZmZW5aAwM7MqB4WZmVW1c83sDZL2S3qiqfYZST+StCNv72+ad6Okfkm7JC1pql8h6fGcd1teN5u8tvbdWd8maW5Tm+WSduetcU1tMzObQO3sUdwBLG1RvzUiFuTtPgBJl1Kud31Ztrld0pRcfi2wEpiXt0afK4ADEXEJcCuwJvuaCawC3gksBFZJmjHmV2hmZsdk1KCIiL8GBtvsbxmwKSIOR8QzQD+wUNIsYFpEPBwRAdwJXNXUZmNO3wMsyr2NJUBvRAxGxAGgl9aBZWZmJ9CxHKP4uKTHcmiq8U1/NvB80zJ7sjY7p0fWh7WJiCHgIHBepS8zM5tA4w2KtcCbgQXAPuDzWVeLZaNSH2+bYSStlNQnqW9gYKCy2mZmNlbjCoqIeCEijkTEK8CXKMcQoHzrn9O0aBewN+tdLerD2kjqAKZThrqO1ler9VkXEd0R0d3Z2Tmel2RmZkcxrqDIYw4NHwQaZ0RtAXryTKaLKQett0fEPuCQpCvz+MO1wL1NbRpnNF0NbM3jGA8AiyXNyKGtxVkzM7MJ1DHaApL+HHg3cL6kPZQzkd4taQFlKOhZ4GMAEbFT0mbgSWAIuD4ijmRX11HOoJoK3J83gPXAXZL6KXsSPdnXoKSbgUdyuZsiot2D6mZmdpyMGhQR8ZEW5fWV5VcDq1vU+4D5LeovAdccpa8NwIbR1tHMzE4c/zLbzMyqHBRmZlbloDAzsyoHhZmZVTkozMysykFhZmZVDgozM6tyUJiZWZWDwszMqhwUZmZW5aAwM7MqB4WZmVU5KMzMrMpBYWZmVQ4KMzOrclCYmVnVqEEhaYOk/ZKeaKrNlNQraXfez2iad6Okfkm7JC1pql8h6fGcd1teEpW8bOrdWd8maW5Tm+X5HLslNS6XamZmE6idPYo7gKUjajcAD0bEPODBfIykSymXMr0s29wuaUq2WQuspFxHe15TnyuAAxFxCXArsCb7mkm57Oo7gYXAquZAMjOziTFqUETEX1OuZd1sGbAxpzcCVzXVN0XE4Yh4BugHFkqaBUyLiIcjIoA7R7Rp9HUPsCj3NpYAvRExGBEHgF7+cWCZmdkJNt5jFBdGxD6AvL8g67OB55uW25O12Tk9sj6sTUQMAQeB8yp9mZnZBDreB7PVohaV+njbDH9SaaWkPkl9AwMDba2omZm1Z7xB8UIOJ5H3+7O+B5jTtFwXsDfrXS3qw9pI6gCmU4a6jtbXPxIR6yKiOyK6Ozs7x/mSzMyslfEGxRagcRbScuDepnpPnsl0MeWg9fYcnjok6co8/nDtiDaNvq4GtuZxjAeAxZJm5EHsxVkzM7MJ1DHaApL+HHg3cL6kPZQzkW4BNktaATwHXAMQETslbQaeBIaA6yPiSHZ1HeUMqqnA/XkDWA/cJamfsifRk30NSroZeCSXuykiRh5UNzOzE2zUoIiIjxxl1qKjLL8aWN2i3gfMb1F/iQyaFvM2ABtGW0czMztx/MtsMzOrclCYmVmVg8LMzKocFGZmVuWgMDOzKgeFmZlVOSjMzKzKQWFmZlUOCjMzq3JQmJlZlYPCzMyqHBRmZlbloDAzsyoHhZmZVTkozMysykFhZmZVDgozM6s6pqCQ9KykxyXtkNSXtZmSeiXtzvsZTcvfKKlf0i5JS5rqV2Q//ZJuy+tqk9fevjvr2yTNPZb1NTOzsTseexTviYgFEdGdj28AHoyIecCD+RhJl1Kuh30ZsBS4XdKUbLMWWAnMy9vSrK8ADkTEJcCtwJrjsL5mZjYGJ2LoaRmwMac3Alc11TdFxOGIeAboBxZKmgVMi4iHIyKAO0e0afR1D7CosbdhZmYT41iDIoBvSHpU0sqsXRgR+wDy/oKszwaeb2q7J2uzc3pkfVibiBgCDgLnHeM6m5nZGHQcY/t3RcReSRcAvZKerizbak8gKvVam+Edl5BaCXDRRRfV19jMzMbkmPYoImJv3u8H/hJYCLyQw0nk/f5cfA8wp6l5F7A3610t6sPaSOoApgODLdZjXUR0R0R3Z2fnsbwkMzMbYdxBIel1kt7QmAYWA08AW4Dludhy4N6c3gL05JlMF1MOWm/P4alDkq7M4w/XjmjT6OtqYGsexzAzswlyLENPFwJ/mceWO4CvRMTXJT0CbJa0AngOuAYgInZK2gw8CQwB10fEkezrOuAOYCpwf94A1gN3Seqn7En0HMP6mpnZOIw7KCLiB8DbW9R/Aiw6SpvVwOoW9T5gfov6S2TQmJnZ5PAvs83MrMpBYWZmVQ4KMzOrclCYmVmVg8LMzKocFGZmVuWgMDOzKgeFmZlVOSjMzKzKQWFmZlUOCjMzq3JQmJlZlYPCzMyqHBRmZlbloDAzsyoHhZmZVTkozMys6qQICklLJe2S1C/phsleHzOz08lrPigkTQH+FPi3wKXARyRdOrlrZWZ2+njNBwWwEOiPiB9ExM+BTcCySV4nM7PTRsdkr0AbZgPPNz3eA7yzeQFJK4GV+fBFSbsmaN1OB+cDP57slRiN1kz2Gtgkec1vnyfRtvlLR5txMgSFWtRi2IOIdcC6iVmd04ukvojonuz1MGvF2+fEOBmGnvYAc5oedwF7J2ldzMxOOydDUDwCzJN0saSzgB5gyySvk5nZaeM1P/QUEUOSPg48AEwBNkTEzklerdOJh/Tstczb5wRQRIy+lJmZnbZOhqEnMzObRA4KO24kvW7E47dO1rqYjaQ0Wl3SdEnnNM17m6TOkcu36utU5aCw4+l3GxN54sHNk7guZsNEaqPeA1zcmAdcAbxx5PKt+jpV+RjFKUjSGZT/t0dyuoPye5QjETGUy5wDHAHOpmz7fy9pGuWEASLiQLY9j/xCEREvND3HG7OtKKcrvxH4PvDm7O95STMjYjCXPxe4KJcfjIjmH1HaSS7/1E5ExCv5+EzKdvNKRLyctXOAV8jtJiJ+dpS+zqJsdwIOR8RPJJ0NXJh9HmlsP5LeAEwFXgecFRG7JL0p2wM8FxEHJc0DplO27x9ExEC2nw1cQvlt1j7gp8A9wFbg28B2YCZwMCIOSeoA3pPP+TLw9dMhMF7zZz1Z+yRNBzZT3lAh6XPAP6f8nayzgO8C1+Wb+CHg/wDvAL6av2b/HGWbeFHSEsrvV76YNUn6XET05t/a+hIwRHnj/jbwAcqbbSPwmKRPUX4xe0a+mVcD84HDlDfhLSf4n8NOsPzw/RplW/pXwA8lLQO6gdso280zkn4d+HvKB+93ctlpkn4jIr41ok8B/xVYSvkgfgpYQdlOV1E+6EPSpyOiD7iWsgewN9v/FvDHwDTK9vl5SQ9Q/nLDRZSQek7SJynvic/nPZRQ6KXsTSwB3kr58rMWuAu4G/jPwLuBQ7l+Wynb9KktInw7RW7Ah4AvNj1+PTAtpzuArwDvp3xT+zbw+znvbMqfSfnlprZnAGfmvLMpb9SdOe93gM80LdvYMz3YVDsT+ElOLwfun+x/H9+O+/Y2D/g58JZ8vInywf0E8I6srQL+KKe/DXw2pxcBD7Xo80xgPzBlRP1s4BxKAFxN+SYPJUSeblruDuC3mvvL+zcAsyh7DzsoYdZJCZipI55rLfCvmx6vB36lafnXTfa//UTfvEdxankMuEXS7wL3Ub69fSS/PTXeZI8C91O+/f+vbDcf2B0RP5B0RkS8EhGv5PDSn1L+BsxhXv2F/FZgnaSplL2Sb+RxvfKfMmQVvLrHujCXF+UDYOhE/QPYhHqFMozzd/n//HuUocepEfG9XObLlG/iULaJe3P6SeBN8Is94Q7KUOhByrf6eyR9E/haRPwQ+KfA71OGgX4OnNvUZ2/2czZwOfCbOURERLycQ1n/jbIdNoZb30x5LzwIfFHS08Bf5Xq9DujM4TQoezFTgLnAY1GGac8GhiLiyLH8A54sfDD7FBIRT1PeKE9QhpE+A9wKLI6It1GGhc6J8jUpeHWX+WVKkBCvjjEL+C/Ajoh4B/AvKW8wIuKRiLgc+L/AZynf6hrDUI0+Gs8BZdz3kigcEqeWn+d9UIJjGsM/V86ifDg3L0PeN5b7AuXLy/rcRn6NMvx0PvBwfuh/FrgjIv4F8Gvktph9Nvofogxx/VJEDDVta28BPhQR/yYi3kv5QtWRz/VRyl6PgL9qbP/AP0TEkQyCxvZ8gPyyFBGHT5eQAAfFKUXSLMqH/1eAPwTeRgmBoTy970O8+v98StP0U8B0Se+RNEPSRZRveC8DRyRdSAmNM/N5LpP0y8A2yjGRN+Yb7KeS3iqpi/Lmen32vwVYJmmhpLdLetuJ+1ewCdT4UCW/fEDZI/ixpHfnwetryW/8lG2ucUppkMcGImJFRCyMiH8v6QxJC4H/Rzk+9g+UA8eHgDdImgt8kvxik302vsAcoey9fELS/DytdS5lO96f299S4H3AWZLOzWMqnZS9759mn98HFuX74fWU7ficiOgHvi/pNyX9qqRr8njfKc9DT6eWtwG/R3kDvwT8J+AqypDAM8C3KG8agBfJb2K5e34NZWz2n1C+lS0Fbqe88RZR3ux/1/Q8n6R8g9sP/Mes/x7wP4G/iYjfkLQj+98m6XbK0MFh4BuUb3V2cmvea4Sy3b1MOSb1Jcq29DRljxPKh33jW/gRYLDRUJIybKZQTnw4N2etjnK20S2U4w//Dvg60J/zB4Ef/WKFIv5I0mrgTyjb55ooJ2BsAdZQQuCPKUHUQQmyqbnuH8tuvgz8d+A6yjb/XcofJwX4dcpp4FdQ3ieNobRTmk+PNTOzKg89mZlZlYPCzMyqHBRmZlbloDAzsyoHhZmZVTkozMysykFhZmZVDgozM6v6/1bh+1aAqskBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['Class Labels'].value_counts().plot(kind = 'bar',rot = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47d4c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# english Stopwords\n",
    "# stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c3aef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # used this dictionary for expanding conracted words. this is taken from Github\n",
    "# CONTRACTIONS = {\n",
    "#     \"I'm\": \"I am\",\n",
    "#     \"I'm'a\": \"I am about to\",\n",
    "#     \"I'm'o\": \"I am going to\",\n",
    "#     \"I've\": \"I have\",\n",
    "#     \"I'll\": \"I will\",\n",
    "#     \"I'll've\": \"I will have\",\n",
    "#     \"I'd\": \"I would\",\n",
    "#     \"I'd've\": \"I would have\",\n",
    "#     \"Whatcha\": \"What are you\",\n",
    "#     \"amn't\": \"am not\",\n",
    "#     \"ain't\": \"are not\",\n",
    "#     \"aren't\": \"are not\",\n",
    "#     \"'cause\": \"because\",\n",
    "#     \"can't\": \"cannot\",\n",
    "#     \"can't've\": \"cannot have\",\n",
    "#     \"could've\": \"could have\",\n",
    "#     \"couldn't\": \"could not\",\n",
    "#     \"couldn't've\": \"could not have\",\n",
    "#     \"daren't\": \"dare not\",\n",
    "#     \"daresn't\": \"dare not\",\n",
    "#     \"dasn't\": \"dare not\",\n",
    "#     \"didn't\": \"did not\",\n",
    "#     \"didn’t\": \"did not\",\n",
    "#     \"don't\": \"do not\",\n",
    "#     \"don’t\": \"do not\",\n",
    "#     \"doesn't\": \"does not\",\n",
    "#     \"e'er\": \"ever\",\n",
    "#     \"everyone's\": \"everyone is\",\n",
    "#     \"finna\": \"fixing to\",\n",
    "#     \"gimme\": \"give me\",\n",
    "#     \"gon't\": \"go not\",\n",
    "#     \"gonna\": \"going to\",\n",
    "#     \"gotta\": \"got to\",\n",
    "#     \"hadn't\": \"had not\",\n",
    "#     \"hadn't've\": \"had not have\",\n",
    "#     \"hasn't\": \"has not\",\n",
    "#     \"haven't\": \"have not\",\n",
    "#     \"he've\": \"he have\",\n",
    "#     \"he's\": \"he is\",\n",
    "#     \"he'll\": \"he will\",\n",
    "#     \"he'll've\": \"he will have\",\n",
    "#     \"he'd\": \"he would\",\n",
    "#     \"he'd've\": \"he would have\",\n",
    "#     \"here's\": \"here is\",\n",
    "#     \"how're\": \"how are\",\n",
    "#     \"how'd\": \"how did\",\n",
    "#     \"how'd'y\": \"how do you\",\n",
    "#     \"how's\": \"how is\",\n",
    "#     \"how'll\": \"how will\",\n",
    "#     \"isn't\": \"is not\",\n",
    "#     \"it's\": \"it is\",\n",
    "#     \"'tis\": \"it is\",\n",
    "#     \"'twas\": \"it was\",\n",
    "#     \"it'll\": \"it will\",\n",
    "#     \"it'll've\": \"it will have\",\n",
    "#     \"it'd\": \"it would\",\n",
    "#     \"it'd've\": \"it would have\",\n",
    "#     \"kinda\": \"kind of\",\n",
    "#     \"let's\": \"let us\",\n",
    "#     \"luv\": \"love\",\n",
    "#     \"ma'am\": \"madam\",\n",
    "#     \"may've\": \"may have\",\n",
    "#     \"mayn't\": \"may not\",\n",
    "#     \"might've\": \"might have\",\n",
    "#     \"mightn't\": \"might not\",\n",
    "#     \"mightn't've\": \"might not have\",\n",
    "#     \"must've\": \"must have\",\n",
    "#     \"mustn't\": \"must not\",\n",
    "#     \"mustn't've\": \"must not have\",\n",
    "#     \"needn't\": \"need not\",\n",
    "#     \"needn't've\": \"need not have\",\n",
    "#     \"ne'er\": \"never\",\n",
    "#     \"o'\": \"of\",\n",
    "#     \"o'clock\": \"of the clock\",\n",
    "#     \"ol'\": \"old\",\n",
    "#     \"oughtn't\": \"ought not\",\n",
    "#     \"oughtn't've\": \"ought not have\",\n",
    "#     \"o'er\": \"over\",\n",
    "#     \"shan't\": \"shall not\",\n",
    "#     \"sha'n't\": \"shall not\",\n",
    "#     \"shalln't\": \"shall not\",\n",
    "#     \"shan't've\": \"shall not have\",\n",
    "#     \"she's\": \"she is\",\n",
    "#     \"she'll\": \"she will\",\n",
    "#     \"she'd\": \"she would\",\n",
    "#     \"she'd've\": \"she would have\",\n",
    "#     \"should've\": \"should have\",\n",
    "#     \"shouldn't\": \"should not\",\n",
    "#     \"shouldn't've\": \"should not have\",\n",
    "#     \"so've\": \"so have\",\n",
    "#     \"so's\": \"so is\",\n",
    "#     \"somebody's\": \"somebody is\",\n",
    "#     \"someone's\": \"someone is\",\n",
    "#     \"something's\": \"something is\",\n",
    "#     \"sux\": \"sucks\",\n",
    "#     \"that're\": \"that are\",\n",
    "#     \"that's\": \"that is\",\n",
    "#     \"that'll\": \"that will\",\n",
    "#     \"that'd\": \"that would\",\n",
    "#     \"that'd've\": \"that would have\",\n",
    "#     \"em\": \"them\",\n",
    "#     \"there're\": \"there are\",\n",
    "#     \"there's\": \"there is\",\n",
    "#     \"there'll\": \"there will\",\n",
    "#     \"there'd\": \"there would\",\n",
    "#     \"there'd've\": \"there would have\",\n",
    "#     \"these're\": \"these are\",\n",
    "#     \"they're\": \"they are\",\n",
    "#     \"they've\": \"they have\",\n",
    "#     \"they'll\": \"they will\",\n",
    "#     \"they'll've\": \"they will have\",\n",
    "#     \"they'd\": \"they would\",\n",
    "#     \"they'd've\": \"they would have\",\n",
    "#     \"this's\": \"this is\",\n",
    "#     \"those're\": \"those are\",\n",
    "#     \"to've\": \"to have\",\n",
    "#     \"wanna\": \"want to\",\n",
    "#     \"wasn't\": \"was not\",\n",
    "#     \"we're\": \"we are\",\n",
    "#     \"we've\": \"we have\",\n",
    "#     \"we'll\": \"we will\",\n",
    "#     \"we'll've\": \"we will have\",\n",
    "#     \"we'd\": \"we would\",\n",
    "#     \"we'd've\": \"we would have\",\n",
    "#     \"weren't\": \"were not\",\n",
    "#     \"what're\": \"what are\",\n",
    "#     \"what'd\": \"what did\",\n",
    "#     \"what've\": \"what have\",\n",
    "#     \"what's\": \"what is\",\n",
    "#     \"what'll\": \"what will\",\n",
    "#     \"what'll've\": \"what will have\",\n",
    "#     \"when've\": \"when have\",\n",
    "#     \"when's\": \"when is\",\n",
    "#     \"where're\": \"where are\",\n",
    "#     \"where'd\": \"where did\",\n",
    "#     \"where've\": \"where have\",\n",
    "#     \"where's\": \"where is\",\n",
    "#     \"which's\": \"which is\",\n",
    "#     \"who're\": \"who are\",\n",
    "#     \"who've\": \"who have\",\n",
    "#     \"who's\": \"who is\",\n",
    "#     \"who'll\": \"who will\",\n",
    "#     \"who'll've\": \"who will have\",\n",
    "#     \"who'd\": \"who would\",\n",
    "#     \"who'd've\": \"who would have\",\n",
    "#     \"why're\": \"why are\",\n",
    "#     \"why'd\": \"why did\",\n",
    "#     \"why've\": \"why have\",\n",
    "#     \"why's\": \"why is\",\n",
    "#     \"will've\": \"will have\",\n",
    "#     \"won't\": \"will not\",\n",
    "#     \"won't've\": \"will not have\",\n",
    "#     \"would've\": \"would have\",\n",
    "#     \"wouldn't\": \"would not\",\n",
    "#     \"wouldn't've\": \"would not have\",\n",
    "#     \"y'all\": \"you all\",\n",
    "#     \"y'all're\": \"you all are\",\n",
    "#     \"y'all've\": \"you all have\",\n",
    "#     \"y'all'd\": \"you all would\",\n",
    "#     \"y'all'd've\": \"you all would have\",\n",
    "#     \"you're\": \"you are\",\n",
    "#     \"you've\": \"you have\",\n",
    "#     \"you'll've\": \"you shall have\",\n",
    "#     \"you'll\": \"you will\",\n",
    "#     \"you'd\": \"you would\",\n",
    "#     \"you'd've\": \"you would have\"\n",
    "#  }\n",
    "# with open('CONTRACTIONS.pkl', 'wb') as f:\n",
    "#     pickle.dump(CONTRACTIONS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "733d0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #I created a dictionary for emoticons\n",
    "# EMOTICONS = {\n",
    "#     u\":‑)\":\"Happy\",\n",
    "#     u\":-))\":\"Very Happy\",\n",
    "#     u\":-)))\":\"Very very Happy\",\n",
    "#     u\":)\":\"Happy\",\n",
    "#     u\":))\":\"Very Happy\",\n",
    "#     u\":)))\":\"Very very Happy\",\n",
    "#     u\":-]\":\"Happy\",\n",
    "#     u\":]\":\"Happy\",\n",
    "#     u\":-3\":\"Happy\",\n",
    "#     u\":3\":\"Happy\",\n",
    "#     u\":->\":\"Happy\",\n",
    "#     u\":>\":\"Happy\",\n",
    "#     u\"8-)\":\"Happy\",\n",
    "#     u\":o)\":\"Happy\",\n",
    "#     u\":-}\":\"Happy\",\n",
    "#     u\":}\":\"Happy\",\n",
    "#     u\":-)\":\"Happy\",\n",
    "#     u\":c)\":\"Happy\",\n",
    "#     u\":^)\":\"Happy\",\n",
    "#     u\"=]\":\"Happy\",\n",
    "#     u\"=)\":\"Happy\",\n",
    "#     u\":‑D\":\"Laughing\",\n",
    "#     u\":D\":\"Laughing\",\n",
    "#     u\"8‑D\":\"Laughing\",\n",
    "#     u\"8D\":\"Laughing\",\n",
    "#     u\"X‑D\":\"Laughing\",\n",
    "#     u\"XD\":\"Laughing\",\n",
    "#     u\"=D\":\"Laughing\",\n",
    "#     u\"=3\":\"Laughing\",\n",
    "#     u\"B^D\":\"Laughing\",\n",
    "#     u\":-))\":\"Very happy\",\n",
    "#     u\"<3\":\"love\",\n",
    "#     u\":-(\":\"sad\",\n",
    "#     u\":‑(\":\"sad\",\n",
    "#     u\":(\":\"sad\",\n",
    "#     u\":‑c\":\"sad\",\n",
    "#     u\":c\":\"sad\",\n",
    "#     u\":‑<\":\"sad\",\n",
    "#     u\":<\":\"sad\",\n",
    "#     u\":‑[\":\"sad\",\n",
    "#     u\":[\":\"sad\",\n",
    "#     u\":-||\":\"sad\",\n",
    "#     u\">:[\":\"sad\",\n",
    "#     u\":{\":\"sad\",\n",
    "#     u\":@\":\"sad\",\n",
    "#     u\">:(\":\"sad\",\n",
    "#     u\":'‑(\":\"Crying\",\n",
    "#     u\":'(\":\"Crying\",\n",
    "#     u\":'‑)\":\"Tears of happiness\",\n",
    "#     u\":')\":\"Tears of happiness\",\n",
    "#     u\"D‑':\":\"sad\",\n",
    "#     u\"D:<\":\"sad\",\n",
    "#     u\"D:\":\"sad\",\n",
    "#     u\"D8\":\"very sad\",\n",
    "#     u\"D;\":\"very sad\",\n",
    "#     u\"D=\":\"very sad\",\n",
    "#     u\"DX\":\"very sad\",\n",
    "#     u\":‑O\":\"Surprise\",\n",
    "#     u\":O\":\"Surprise\",\n",
    "#     u\":‑o\":\"Surprise\",\n",
    "#     u\":o\":\"Surprise\",\n",
    "#     u\":-0\":\"Sad\",\n",
    "#     u\"8‑0\":\"Yawn\",\n",
    "#     u\">:O\":\"Yawn\",\n",
    "#     u\":-*\":\"Kiss\",\n",
    "#     u\":*\":\"Kiss\",\n",
    "#     u\":X\":\"Kiss\",\n",
    "#     u\";‑)\":\"Wink\",\n",
    "#     u\";)\":\"Wink\",\n",
    "#     u\"*-)\":\"Wink\",\n",
    "#     u\"*)\":\"Wink\",\n",
    "#     u\";‑]\":\"Wink\",\n",
    "#     u\";]\":\"Wink\",\n",
    "#     u\";^)\":\"Wink\",\n",
    "#     u\":‑,\":\"Wink\",\n",
    "#     u\";D\":\"Wink\",\n",
    "#     u\":‑P\":\"fun\",\n",
    "#     u\":P\":\"fun\",\n",
    "#     u\"X‑P\":\"fun\",\n",
    "#     u\"XP\":\"fun\",\n",
    "#     u\":‑Þ\":\"fun\",\n",
    "#     u\":Þ\":\"fun\",\n",
    "#     u\":b\":\"fun\",\n",
    "#     u\"d:\":\"fun\",\n",
    "#     u\"=p\":\"fun\",\n",
    "#     u\">:P\":\"fun\",\n",
    "#     u\":‑/\":\"annoyed\",\n",
    "#     u\":/\":\"annoyed\",\n",
    "#     u\":-[.]\":\"annoyed\",\n",
    "#     u\">:[(\\)]\":\"annoyed\",\n",
    "#     u\">:/\":\"annoyed\",\n",
    "#     u\":[(\\)]\":\"annoyed\",\n",
    "#     u\"=/\":\"annoyed\",\n",
    "#     u\"=[(\\)]\":\"annoyed\",\n",
    "#     u\":L\":\"annoyed\",\n",
    "#     u\"=L\":\"annoyed\",\n",
    "#     u\":S\":\"annoyed\",\n",
    "#     u\":‑|\":\"indecision\",\n",
    "#     u\":|\":\"indecision\",\n",
    "#     u\":$\":\"Embarrassed\",\n",
    "#     u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "#     u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "#     u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "#     u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "#     u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "#     u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "#     u\"O:‑)\":\"Angel\",\n",
    "#     u\"O:)\":\"Angel\",\n",
    "#     u\"0:‑3\":\"Angel\",\n",
    "#     u\"0:3\":\"Angel\",\n",
    "#     u\"0:‑)\":\"Angel\",\n",
    "#     u\"0:)\":\"Angel\",\n",
    "#     u\":‑b\":\"fun\",\n",
    "#     u\"0;^)\":\"Angel\",\n",
    "#     u\">:‑)\":\"devilish\",\n",
    "#     u\">:)\":\"devilish\",\n",
    "#     u\"}:‑)\":\"devilish\",\n",
    "#     u\"}:)\":\"devilish\",\n",
    "#     u\"3:‑)\":\"devilish\",\n",
    "#     u\"3:)\":\"devilish\",\n",
    "#     u\">;)\":\"devilish\",\n",
    "#     u\"|;‑)\":\"Cool\",\n",
    "#     u\"|‑O\":\"Bored\",\n",
    "#     u\":‑J\":\"Tongue in cheek\",\n",
    "#     u\"#‑)\":\"Party all night\",\n",
    "#     u\"%‑)\":\"confused\",\n",
    "#     u\"%)\":\"confused\",\n",
    "#     u\":-###..\":\"Being sick\",\n",
    "#     u\":###..\":\"Being sick\",\n",
    "#     u\"<:‑|\":\"silent\",\n",
    "#     u\"(>_<)\":\"Troubled\",\n",
    "#     u\"(>_<)>\":\"Troubled\",\n",
    "#     u\"(';')\":\"Baby\",\n",
    "#     u\"(^^>``\":\"Nervous\",\n",
    "#     u\"(^_^;)\":\"Troubled\",\n",
    "#     u\"(-_-;)\":\"Nervous\",\n",
    "#     u\"(~_~;) (・.・;)\":\"Shy\",\n",
    "#     u\"(-_-)zzz\":\"Sleeping\",\n",
    "#     u\"(^_-)\":\"Wink\",\n",
    "#     u\"((+_+))\":\"Confused\",\n",
    "#     u\"(+o+)\":\"Confused\",\n",
    "#     u\"(o|o)\":\"Ultraman\",\n",
    "#     u\"^_^\":\"happy\",\n",
    "#     u\"(^_^)/\":\"happy\",\n",
    "#     u\"(^O^)／\":\"happy\",\n",
    "#     u\"(^o^)／\":\"happy\",\n",
    "#     u\"(__)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "#     u\"_(._.)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "#     u\"<(_ _)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "#     u\"<m(__)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "#     u\"m(__)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "#     u\"m(_ _)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "#     u\"('_')\":\"Sad\",\n",
    "#     u\"(/_;)\":\"Sad\",\n",
    "#     u\"(T_T) (;_;)\":\"Sad\",\n",
    "#     u\"(;_;\":\"Sad\",\n",
    "#     u\"(;_:)\":\"Sad\",\n",
    "#     u\"(;O;)\":\"Sad\",\n",
    "#     u\"(:_;)\":\"Sad\",\n",
    "#     u\"(ToT)\":\"Sad\",\n",
    "#     u\";_;\":\"Sad\",\n",
    "#     u\";-;\":\"Sad\",\n",
    "#     u\";n;\":\"Sad\",\n",
    "#     u\";;\":\"Sad\",\n",
    "#     u\"Q.Q\":\"Sad\",\n",
    "#     u\"T.T\":\"Sad\",\n",
    "#     u\"QQ\":\"Sad\",\n",
    "#     u\"Q_Q\":\"Sad\",\n",
    "#     u\"(-.-)\":\"Shame\",\n",
    "#     u\"(-_-)\":\"Shame\",\n",
    "#     u\"(一一)\":\"Shame\",\n",
    "#     u\"(；一_一)\":\"Shame\",\n",
    "#     u\"(=_=)\":\"Tired\",\n",
    "#     u\"(=^·^=)\":\"cat\",\n",
    "#     u\"(=^··^=)\":\"cat\",\n",
    "#     u\"=_^= \":\"cat\",\n",
    "#     u\"(..)\":\"Looking down\",\n",
    "#     u\"(._.)\":\"Looking down\",\n",
    "#     u\"^m^\":\"Giggling\",\n",
    "#     u\"(・・?\":\"Confusion\",\n",
    "#     u\"(?_?)\":\"Confusion\",\n",
    "#     u\">^_^<\":\"Normal Laugh\",\n",
    "#     u\"<^!^>\":\"Normal Laugh\",\n",
    "#     u\"^/^\":\"Normal Laugh\",\n",
    "#     u\"（*^_^*）\" :\"Normal Laugh\",\n",
    "#     u\"(^<^) (^.^)\":\"Normal Laugh\",\n",
    "#     u\"(^^)\":\"Normal Laugh\",\n",
    "#     u\"(^.^)\":\"Normal Laugh\",\n",
    "#     u\"(^_^.)\":\"Normal Laugh\",\n",
    "#     u\"(^_^)\":\"Normal Laugh\",\n",
    "#     u\"(^^)\":\"Normal Laugh\",\n",
    "#     u\"(^J^)\":\"Normal Laugh\",\n",
    "#     u\"(*^.^*)\":\"Normal Laugh\",\n",
    "#     u\"(^—^）\":\"Normal Laugh\",\n",
    "#     u\"(#^.^#)\":\"Normal Laugh\",\n",
    "#     u\"（^—^）\":\"Waving\",\n",
    "#     u\"(;_;)/~~~\":\"Waving\",\n",
    "#     u\"(^.^)/~~~\":\"Waving\",\n",
    "#     u\"(-_-)/~~~ ($··)/~~~\":\"Waving\",\n",
    "#     u\"(T_T)/~~~\":\"Waving\",\n",
    "#     u\"(ToT)/~~~\":\"Waving\",\n",
    "#     u\"(*^0^*)\":\"Excited\",\n",
    "#     u\"(*_*)\":\"Excited\",\n",
    "#     u\"(*_*;\":\"Excited\",\n",
    "#     u\"(+_+) (@_@)\":\"Excited\",\n",
    "#     u\"(*^^)v\":\"Cheerful\",\n",
    "#     u\"(^_^)v\":\"Cheerful\",\n",
    "#     u\"((d[-_-]b))\":\"Headphones,Listening to music\",\n",
    "#     u'(-\"-)':\"Worried\",\n",
    "#     u\"(ーー;)\":\"Worried\",\n",
    "#     u\"(^0_0^)\":\"Eyeglasses\",\n",
    "#     u\"(＾ｖ＾)\":\"Happy\",\n",
    "#     u\"(＾ｕ＾)\":\"Happy\",\n",
    "#     u\"(^)o(^)\":\"Happy\",\n",
    "#     u\"(^O^)\":\"Happy\",\n",
    "#     u\"(^o^)\":\"Happy\",\n",
    "#     u\")^o^(\":\"Happy\",\n",
    "#     u\":O o_O\":\"Surprised\",\n",
    "#     u\"o_0\":\"Surprised\",\n",
    "#     u\"o.O\":\"Surpised\",\n",
    "#     u\"(o.o)\":\"Surprised\",\n",
    "#     u\"oO\":\"Surprised\",\n",
    "#     u\"(*￣m￣)\":\"Dissatisfied\",\n",
    "#     u\"(‘A`)\":\"Deflated\"\n",
    "\n",
    "# }\n",
    "# with open('EMOTICONS.pkl', 'wb') as f:\n",
    "#     pickle.dump(EMOTICONS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "953b1a10-4146-410f-bff3-274559c8b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # english Stopwords\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    with open('EMOTICONS.pkl', 'rb') as f1:\n",
    "        EMOTICONS = pickle.load(f1)\n",
    "    with open('CONTRACTIONS.pkl','rb') as f2:\n",
    "        CONTRACTIONS = pickle.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e0b3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(tweet):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pickle\n",
    "    import time\n",
    "    import spacy\n",
    "    import nltk\n",
    "    import re   # regular expression\n",
    "    import string\n",
    "    import multiprocessing\n",
    "    from multiprocessing import Pool\n",
    "    import scipy.sparse as sp\n",
    "    import joblib\n",
    "    import emoji\n",
    "    from autocorrect import Speller   # for correcting spelling\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize    # for tokenizing string into words\n",
    "    from nltk.stem import WordNetLemmatizer    # for lemmatizing words\n",
    "    from nltk.tag import pos_tag # for tagging words with their parts of speech (POS)\n",
    "    \n",
    "    \n",
    "    # english Stopwords\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    with open('EMOTICONS.pkl', 'rb') as f1:\n",
    "        EMOTICONS = pickle.load(f1)\n",
    "    with open('CONTRACTIONS.pkl','rb') as f2:\n",
    "        CONTRACTIONS = pickle.load(f2)\n",
    "    \n",
    "    nan_tweet = 'NaN'  \n",
    "    # this code is to short unnecessary sentence, bec. some rows has unnecessary long repeated characters\n",
    "    # like 'HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH'\n",
    "    # we manually decide len = 10 any with len >10 is discarded\n",
    "        # convert all text lowercase\n",
    "    tweet = tweet.lower() \n",
    "    tweet = tweet.split()\n",
    "    tw = []\n",
    "    for t in tweet:\n",
    "        # removing digits only\n",
    "        if t.isnumeric():\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "        if len(t)<=20:\n",
    "            if len(set(t))<=1:\n",
    "                continue\n",
    "            if sum(c.isdigit() for c in t) > sum(c.isalpha() for c in t):\n",
    "                continue\n",
    "            tw.append(t)\n",
    "    tweet = ' '.join(tw)   \n",
    "    # remove any urls\n",
    "    tweet = re.sub(r\"www\\S+|http\\S+|\", \"\",tweet, flags = re.MULTILINE)\n",
    "    # remove square bracket including its content if\n",
    "    tweet = re.sub(r'\\[|\\]',\" \",tweet)\n",
    "    # to remove new line character\n",
    "    tweet = re.sub(r'\\n', \" \", tweet)\n",
    "    # remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r\"\\@\\w+|\\#\", \"\",tweet)\n",
    "    # replace emojis with its meaning\n",
    "    tweet = (emoji.demojize(tweet, delimiters=(\" \", \"\"))).replace('_',' ')\n",
    "    # expand contractions\n",
    "    splitted_string = tweet.split()\n",
    "    for index, text in enumerate(splitted_string):\n",
    "        if text in CONTRACTIONS.keys():\n",
    "            splitted_string[index] = CONTRACTIONS[text]\n",
    "    tweet = ' '.join(splitted_string)\n",
    "    # replace emoticons with its meaning\n",
    "    splitted_tweet = tweet.split()\n",
    "    for index, word in enumerate(splitted_tweet):\n",
    "        if word in EMOTICONS.keys():\n",
    "            splitted_tweet[index] = EMOTICONS[word]\n",
    "    tweet = ' '.join(splitted_tweet)\n",
    "    # remove tags\n",
    "    tweet = re.sub(re.compile('<.*?>'), '', tweet)\n",
    "    # remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    # remove stopwords\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [word for word in tweet_tokens if word not in stopwords_list]\n",
    "    # spelling correction\n",
    "    correct_words = []\n",
    "    # initialize Speller object for english language\n",
    "    spell_corrector = Speller(lang='en')\n",
    "    for word in filtered_words:\n",
    "        correct_word = spell_corrector(word)\n",
    "        correct_words.append(correct_word)\n",
    "    # lemmatizing\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemma_words = []\n",
    "    for word, tag in pos_tag(correct_words):      # Part-of-speech constants for ADJ,VERB,ADV = 'a', 'r', 'v'\n",
    "        if tag.startswith('JJ'):      # for adjectives\n",
    "            lemma_word = wnl.lemmatize(word, pos='a')\n",
    "            lemma_words.append(lemma_word)\n",
    "        elif tag.startswith('VB'):   # for verbs\n",
    "            lemma_word = wnl.lemmatize(word, pos='v')\n",
    "            lemma_words.append(lemma_word)\n",
    "        elif tag.startswith('RB'):   # for adverbs\n",
    "            lemma_word = wnl.lemmatize(word, pos='r')\n",
    "            lemma_words.append(lemma_word)\n",
    "        else:\n",
    "            lemma_word = word\n",
    "            lemma_words.append(lemma_word)\n",
    "        \n",
    "    tweet = \" \".join(lemma_words)\n",
    "    if len(tweet) == 0:   # if after pre-processing sent. has no letter\n",
    "        tweet = nan_tweet\n",
    "    # f1.close(),f2.close()\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e907f43f-0068-454f-93eb-598387bf5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_parallel_apply import DataFrameParallel, apply_on_df_col_parallel, apply_on_series_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f532bd-0659-4cee-8e25-43afe9208339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35da4c1c-eab1-47ff-83d9-8d5f059f6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocesspandas import applyparallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2362936a-0e68-4735-a5db-fab88779f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['Text'][0:100].apply_parallel(preprocess_text, num_processes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da41a61d-1607-4781-b0a1-f40a4c29a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DataFrameParallel(df_train, n_cores=4 , pbar=True)['Text'].apply(preprocess_text)\n",
    "# r = apply_on_df_col_parallel(df_train[0:10], 'Text', preprocess_text, 4, pbar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67408d83-8d1f-4f03-8dda-dbc963fb4b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "294ff223-35c5-474b-8116-d8d811757a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "# 24000:250000 error somewhere\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True,nb_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be66bb51-1077-492d-b3c1-33f83e40e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_cores = multiprocessing.cpu_count()\n",
    "# num_partitions = num_cores-2 # leave some cores for other processes\n",
    "# print(num_partitions)\n",
    "\n",
    "# def parallelize_dataframe(df, func):\n",
    "#     a = np.array_split(df, num_partitions)\n",
    "#     del df\n",
    "#     pool = Pool(num_cores)\n",
    "#     df = sp.vstack(pool.map(func,a), format='csr')\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "#     return df\n",
    "\n",
    "# # def test_func2(data,cv=CV,train=False):\n",
    "# #     X_bow = cv.transform(data)\n",
    "# #     return X_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d7907df6-49b0-41a0-8ef4-62c396989d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize_dataframe(df_train['Text'][0:100],preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ff584-3db6-4f3c-a135-18cfe85f5c77",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328226dc016f49ca9fa8bf4a7410dfb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=134777), Label(value='0 / 134777')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = time.time()\n",
    "xx = df_train['Text'].parallel_apply(preprocess_text) \n",
    "df_train.insert(loc = 4,\n",
    "        column = 'Pre Processed Text',\n",
    "        value = xx)\n",
    "# saving\n",
    "df_train.to_csv('processed_train.csv',index=False) # on train data, \n",
    "tt = time.time() - t\n",
    "print(f'{tt} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6eb80e-37f9-4828-9789-964919857a82",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5266970bf866421c909676a19ee831d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=101083), Label(value='0 / 101083')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = time.time()\n",
    "xtt = df_test['Text'].parallel_apply(preprocess_text) # on test data\n",
    "df_test.insert(loc = 4,\n",
    "        column = 'Pre Processed Text',\n",
    "        value = xtt)\n",
    "df_test.to_csv('processed_test.csv',index=False)\n",
    "tt = time.time() - t\n",
    "print(f'{tt} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dae800-7f7e-4528-afeb-a51c7e107f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf1af5-6d72-4b48-970c-be78d5fa5d6e",
   "metadata": {},
   "source": [
    "### Reading the pre-processed csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c8f51-cbc9-4e9e-aa0a-2bdbf7351295",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro_train = pd.read_csv('processed_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f1bd31-af72-4b98-a42c-4bc17027e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_prepro_train.shape)\n",
    "df_prepro_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e410647c-a815-4cd3-9c87-aead0497a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c725b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "df_prepro_test = pd.read_csv('processed_test.csv')\n",
    "print(df_prepro_test.shape)\n",
    "df_prepro_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2e255-b4d9-4573-a536-9f7892d3ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6dff77-ac97-4700-b15f-f4b9c92709ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "X = df_prepro_train['Pre Processed Text']\n",
    "y_trn = df_prepro_train['Class Labels']\n",
    "\n",
    "# Test data\n",
    "X_test = df_prepro_test['Pre Processed Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e5bf0-e447-494f-ba66-0c9a9f0c4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting Labels coloumn into numerical form of train data\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb816b-18c9-481a-937e-5abb848f1373",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43446706-769f-4c91-bee1-c7fc710c68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f8075-57cb-47f8-84b7-7e955f20256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train validation split of data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=3,stratify=y)\n",
    "X_train.shape,X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1a2b2-3400-402c-b0e7-a87dd934108b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c6c6da2-aeff-4372-bddf-d57d82f06cc7",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664ab426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(ngram_range=(1,3),max_features=60000) \n",
    "# X_train_bow = cv.fit_transform(X_train).toarray()\n",
    "# X_valid_bow = cv.transform(X_valid).toarray()\n",
    "# X_test_bow = cv.transform(X_test).toarray()\n",
    "\n",
    "# X_train_bow.shape,X_valid_bow.shape,X_test_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98cf60-5a93-4193-973e-57a1b72ff6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = CountVectorizer(ngram_range=(1,3),max_features=100000) # (1,3) means Unigrams, Bigrams and Trigrams\n",
    "X_train_bow = sp.csr_matrix(CV.fit_transform(X_train))  # compressing the data to use less memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7933e5c7-a170-45d7-97f7-ff9972ba2f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc0675-6861-4674-9af6-3fe3cc97e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = multiprocessing.cpu_count()\n",
    "num_partitions = num_cores-2 # leave some cores for other processes\n",
    "print(num_partitions)\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    a = np.array_split(df, num_partitions)\n",
    "    del df\n",
    "    pool = Pool(num_cores)\n",
    "    df = sp.vstack(pool.map(func,a), format='csr')\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def test_func2(data,cv=CV,train=False):\n",
    "    X_bow = cv.transform(data)\n",
    "    return X_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4f36a-9065-46d5-a447-9d117fa96000",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_bow = parallelize_dataframe(X_valid, test_func2)\n",
    "X_valid_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7423f50-9309-419a-b46b-eca5923bbf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bow = parallelize_dataframe(X_test, test_func2)\n",
    "X_test_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737dccc-ba37-401e-af6f-624e4a1deea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(X_train_bow.tocsr(), 'X_train_bow.joblib')\n",
    "joblib.dump(X_valid_bow.tocsr(), 'X_valid_bow.joblib')\n",
    "joblib.dump(X_test_bow.tocsr(), 'X_test_bow.joblib')\n",
    "del X_train_bow,X_valid_bow,X_test_bow # deleting the variable for freeing memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0521d4-78a4-4620-992d-c6c7b8baff4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5f7147c-5ae5-4504-8eea-6056ceec4b51",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcbdbe1-85dd-4cf5-b884-8eb4fd2454e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer(ngram_range=(1,3),max_features=60000,use_idf=True,smooth_idf=True)\n",
    "# X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "# X_valid_tfidf = tfidf.transform(X_valid).toarray()\n",
    "# X_test_tfidf = tfidf.transform(X_test).toarray()\n",
    "\n",
    "# X_train_tfidf.shape,X_valid_tfidf.shape,X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48714281-3791-42c1-be09-eacee071ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3),max_features=100000,use_idf=True,smooth_idf=True)\n",
    "X_train_tfidf = sp.csr_matrix(tfidf.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b1056-d45d-48f0-826b-6f32b93342f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d7ab8-264b-470a-a388-32b047a45d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = multiprocessing.cpu_count()\n",
    "num_partitions = num_cores-2 # leave some cores for other processes\n",
    "print(num_partitions)\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    a = np.array_split(df, num_partitions)\n",
    "    del df\n",
    "    pool = Pool(num_cores)\n",
    "    df = sp.vstack(pool.map(func,a), format='csr')\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def test_func2(data,cv=tfidf,train=False):\n",
    "    X_bow = cv.transform(data)\n",
    "    return X_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e6fe5a-9518-4fa4-bf25-5e0ef8b69d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_tfidf = parallelize_dataframe(X_valid, test_func2)\n",
    "X_valid_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ce764-53a9-4eaa-82e0-254178f4156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = parallelize_dataframe(X_test, test_func2)\n",
    "X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90268f-2f78-49a3-8150-6e34e25e8a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(X_train_tfidf.tocsr(), 'X_train_tfidf.joblib')\n",
    "joblib.dump(X_valid_tfidf.tocsr(), 'X_valid_tfidf.joblib')\n",
    "joblib.dump(X_test_tfidf.tocsr(), 'X_test_tfidf.joblib')\n",
    "del X_train_tfidf,X_valid_tfidf,X_test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac4669-0e68-409e-80f2-529a93445719",
   "metadata": {},
   "source": [
    "# word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d15a3-77f0-4c81-899a-d8d69ff23b8b",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3a97b-1ca0-4660-9625-a5e246061c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_spacy(X):\n",
    "    w2v_spacy = []\n",
    "    for item in X.values: # .values returns dataframe rows as list, eg. 1st sentence as ['he is good']\n",
    "        doc = nlp(item)\n",
    "        w2v_spacy.append(doc.vector)\n",
    "    w2v_spacy = np.array(w2v_spacy)\n",
    "    return w2v_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a12c1e-0026-4396-aa64-4dc9e718ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on train data\n",
    "w2v_spacy_train = word2vec_spacy(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601fb92-d2b5-4967-9d3d-63817d6ce137",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_spacy_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65ad18-1b41-4076-87f9-a3204240599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_spacy_train = sp.csr_matrix(w2v_spacy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b3b74c-28df-4169-85a8-7d5791aa91cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on validation data\n",
    "w2v_spacy_valid = word2vec_spacy(X_valid)\n",
    "w2v_spacy_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d0203-0a1b-4fc7-917c-dfd067286546",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_spacy_valid = sp.csr_matrix(w2v_spacy_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1cfff1-fa6b-4a1b-94b4-79631fffb099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on test data\n",
    "w2v_spacy_test = word2vec_spacy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447121b2-f156-4d8d-bcbb-4f22f04e825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_spacy_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423f7fa-2383-4341-bda1-dde44b2586f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_spacy_test = sp.csr_matrix(w2v_spacy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19162293-da87-4c7e-a885-fd090faa7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(w2v_spacy_train.tocsr(), 'w2v_spacy_train.joblib')\n",
    "joblib.dump(w2v_spacy_valid.tocsr(), 'w2v_spacy_valid.joblib')\n",
    "joblib.dump(w2v_spacy_test.tocsr(), 'w2v_spacy_test.joblib')\n",
    "del w2v_spacy_train,w2v_spacy_valid,w2v_spacy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ebf88-5351-44d5-bad1-930a76a87aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fdb3943-5f35-466c-a634-6a86b7eec9ea",
   "metadata": {},
   "source": [
    "### Custom Training - W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d88ed45-4696-4339-8b88-cbb14b9f8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00dd4644-6a5c-434f-94a5-28c21894ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences \n",
    "    def __iter__(self):\n",
    "        for line in self.sentences:\n",
    "            yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e36bd77e-0151-4eee-810c-575ede01cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MySentences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64c4613f-8e5e-4604-9057-feb6ac859c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train a bigram detector.\n",
    "# bigram_transformer = Phrases(sentences=sentences,connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "\n",
    "# # Apply the trained MWE detector to a corpus, using the result to train a Word2vec model.\n",
    "# model = Word2Vec(bigram_transformer[sentences],vector_size=300, min_count=1,workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "383f6427-b938-47e1-b480-fe1053147fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('word2vec_b300.model')#bigram included w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d11a8253-ecaf-4116-a16d-e6cf6dd6f05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_b300 = Word2Vec.load('word2vec_b300.model')\n",
    "w2v_b300.wv['good'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c594833-a3cc-48ec-8b97-e2d2dee380ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec,dim):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = dim\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec.keys()] or [np.zeros(self.dim)], axis=0) for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0ed7a72-6598-4e8a-8298-809df9fba362",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_b300_dict=dict(zip(w2v_b300.wv.index_to_key, w2v_b300.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039f6d3-487f-40f1-babf-d52584b511f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "etree_w2v_b300 = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v_b300_dict,300)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200,n_jobs=32))])\n",
    "Gau_w2v_b300 = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v_b300_dict,300)),\n",
    "    (\"GaussianNB\", GaussianNB())])\n",
    "SVC_w2v_b300 = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v_b300_dict,300)),\n",
    "    (\"SVC\", SVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa7ad79-1731-4b27-bf29-d312f9b241da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GaussianNB:',cross_val_score(Gau_w2v_b300,X_train,y_train,cv=5).mean())\n",
    "print('etree:',cross_val_score(Gau_w2v_b300,X_train,y_train,cv=5).mean())\n",
    "print('SVC:',cross_val_score(SVC_w2v_b300,X_train,y_train,cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ebf33-2fe7-4fc6-80b5-5a234ddda735",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698109da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "glove_input_file = 'glove/glove.twitter.27B.200d.txt'\n",
    "model_glove = KeyedVectors.load_word2vec_format(glove_input_file, binary=False,no_header=True)\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model_glove.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d5d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict=dict(zip(model.index_to_key, model.vectors))\n",
    "etree_glove = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(glove_dict,200)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200,n_jobs=32))])\n",
    "Gau_glove = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(glove_dict,200)),\n",
    "    (\"GaussianNB\", GaussianNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935232c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb671dfe-3acc-443c-bac5-f21a04911121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edecf975-48ca-46bd-a273-10bd18fb64e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9605a726-987a-4bda-8373-a488b11c28c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06ee87-ea1e-443b-bde7-c61f2a3501d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b06a94c6-8cff-4816-ba5d-18445b89a398",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3429407-3182-4ebb-898c-78fe52e512fe",
   "metadata": {},
   "source": [
    "### With Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e658243-d356-4b43-948e-9b7c719e3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def logistic(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    fsl = 0\n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    print('\\n\\t ---------- Training Logistic Regression Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline1 = Pipeline([('fs',fsl),\n",
    "                          ('clf1', SGDClassifier(loss='log_loss',alpha=0.0001))])\n",
    "    clf1_parameters = {\n",
    "        'fs__k' : [500,5000,10000],\n",
    "        'clf1__penalty':['l1', 'l2',],\n",
    "        }\n",
    "    grid_search1 = GridSearchCV(estimator=pipeline1, param_grid=clf1_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search1.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search1.best_score_}\")\n",
    "    clf1 = grid_search1.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf1) \n",
    "    predicted_class_labels1 = clf1.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels1))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7a75407-270c-4276-926b-59fe2b613fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    print('\\n\\t ---------- Training Decision Tree Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline2 = Pipeline([('fs', fsl),\n",
    "                          ('clf2', DecisionTreeClassifier(random_state=40))])\n",
    "    clf2_parameters = {\n",
    "        'fs__k' : [500,5000,10000],\n",
    "        'clf2__criterion':['entropy','gini',], \n",
    "        'clf2__max_features':['sqrt', 'log2',None],\n",
    "        'clf2__max_depth':[10,50,100,500],\n",
    "        'clf2__ccp_alpha':[0.002,0.01,0.1,]\n",
    "        }\n",
    "    grid_search2 = GridSearchCV(estimator=pipeline2, param_grid=clf2_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search2.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search2.best_score_}\")\n",
    "    clf2 = grid_search2.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf2)\n",
    "    predicted_class_labels2 = clf2.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels2))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4062c346-0457-46a8-ad89-6a3293925b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    print('\\n\\t ---------- Training KNN Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline3 = Pipeline([('fs', fsl),\n",
    "                          ('clf3', KNeighborsClassifier())])\n",
    "    clf3_parameters = {\n",
    "        'fs__k' : [500,3000,10000],\n",
    "        'clf3__n_neighbors': [3,5,10,20,35],\n",
    "        'clf3__weights':['uniform', 'distance',],\n",
    "        'clf3__p':[1,2,],\n",
    "        'clf3__metric':['euclidean', 'manhattan',] \n",
    "        }\n",
    "    grid_search3 = GridSearchCV(estimator=pipeline3, param_grid=clf3_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search3.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search3.best_score_}\")\n",
    "    clf3 = grid_search3.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf3) \n",
    "    predicted_class_labels3 = clf3.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels3))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "312880c9-7032-464e-bb93-cccb13d9e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianNBb(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    # Gaussian Naive Bayes\n",
    "    print('\\n\\t ---------- Training Gaussian Naive Bayes Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline4 = Pipeline([('fs', fsl),\n",
    "                          ('clf4', GaussianNB())])\n",
    "    clf4_parameters = {\n",
    "        'fs__k' : [500,3000,10000],\n",
    "        'clf4__var_smoothing': np.logspace(0,-9, num=60)\n",
    "        }\n",
    "    grid_search4 = GridSearchCV(estimator=pipeline4, param_grid=clf4_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search4.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search4.best_score_}\")\n",
    "    clf4 = grid_search4.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf4) \n",
    "    predicted_class_labels4 = clf4.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels4))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "62a9a628-b55f-4c29-b899-b760508ed1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultinomialNBb(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    print('\\n\\t ---------- Training Multinomial Naive Bayes Classifier ---------- \\n')  \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline5 = Pipeline([('fs', fsl),\n",
    "                          ('clf5', MultinomialNB(fit_prior=True, class_prior=None))])#('fs', fsl),\n",
    "    clf5_parameters = {\n",
    "        'fs__k' : [10000],#,3000,10000],\n",
    "        'clf5__alpha':[20]\n",
    "        }\n",
    "    grid_search5 = GridSearchCV(estimator=pipeline5, param_grid=clf5_parameters, n_jobs=-1, cv=2, scoring='f1_macro',verbose=1)\n",
    "    grid_search5.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search5.best_score_}\")\n",
    "    clf5 = grid_search5.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf5) \n",
    "    predicted_class_labels5 = clf5.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels5))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18242a-f3b3-4a7a-b85c-9304a68ed079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e90e3426-3da7-47a0-a243-9a101c1ca0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearSVCb(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    print('\\n\\t ---------- Training Linear SVC Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline8 = Pipeline([('fs', fsl),\n",
    "                          ('clf8', LinearSVC(class_weight='balanced'))])\n",
    "    clf8_parameters = {\n",
    "        'fs__k' : [500,3000,10000],\n",
    "        'clf8__C':[0.001,0.01,1,100,],\n",
    "        }  \n",
    "    grid_search8 = GridSearchCV(estimator=pipeline8, param_grid=clf8_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search8.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search8.best_score_}\")\n",
    "    clf8 = grid_search8.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf8) \n",
    "    predicted_class_labels8 = clf8.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels8))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "866e17ab-6d5f-4078-945b-ba45ad831a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVCb(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    print('\\n\\t ---------- Training SVM Classifier ---------- \\n') \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline9 = Pipeline([('fs', fsl),\n",
    "                          ('clf9', SVC(probability=True))])\n",
    "    clf9_parameters = {\n",
    "        'fs__k' : [500,3000,10000],\n",
    "        'clf9__C':[0.1,1,50,80,100,150],\n",
    "        'clf9__kernel':['poly','linear','sigmoid',],\n",
    "        }\n",
    "    grid_search9 = GridSearchCV(estimator=pipeline9, param_grid=clf9_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search9.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search9.best_score_}\")\n",
    "    clf9 = grid_search9.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf9) \n",
    "    predicted_class_labels9 = clf9.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels9))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "872f36c9-9237-41c0-9e97-8263e899f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    print('\\n\\t ---------- Training Random Forest Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline10 = Pipeline([('fs', fsl),\n",
    "                           ('clf10', RandomForestClassifier(class_weight='balanced'))])\n",
    "    clf10_parameters = {\n",
    "        'fs__k' : [500,3000,10000],\n",
    "        'clf10__criterion':['entropy','gini',],\n",
    "        'clf10__max_depth':[10,50,80,120],\n",
    "        'clf10__n_estimators':[30,50,],\n",
    "        'clf10__max_features':['sqrt','log2',None,] \n",
    "        } \n",
    "    grid_search10 = GridSearchCV(estimator=pipeline10, param_grid=clf10_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search10.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search10.best_score_}\")\n",
    "    clf10 = grid_search10.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf10) \n",
    "    predicted_class_labels10 = clf10.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels10))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d6906-819b-461c-bb94-ef7e8ade876e",
   "metadata": {},
   "source": [
    "##### bow training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d120bdf9-20cc-45e9-ad6f-2b85932ca0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = joblib.load('X_train_bow.joblib', mmap_mode='c')\n",
    "X_valid_bow = joblib.load('X_valid_bow.joblib', mmap_mode='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25732b7b-190b-4b6a-b6ba-196bfa8a8c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10f33edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t ---------- Training Logistic Regression Classifier ---------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/khadga_19024/khadga_19024/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score on Training set :  0.647427124607335\n",
      "\n",
      "\n",
      " The best set of parameters of the pipeline in Training Phase are: \n",
      "Pipeline(steps=[('fs',\n",
      "                 SelectKBest(k=10000,\n",
      "                             score_func=<function chi2 at 0x7fa19eaafa30>)),\n",
      "                ('clf1', SGDClassifier(loss='log_loss'))])\n",
      "\n",
      " *******  Scores on Validation Data  ******* \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.71      0.67     80866\n",
      "           1       0.67      0.59      0.63     80867\n",
      "\n",
      "    accuracy                           0.65    161733\n",
      "   macro avg       0.65      0.65      0.65    161733\n",
      "weighted avg       0.65      0.65      0.65    161733\n",
      "\n",
      "====================================================================\n",
      "Process Completed and time taken is : 14.34 minutes\n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "clf1bc = logistic(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9776e52-93d9-414e-9a51-73c4cbe8d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1bm = logistic(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b763af64-0c3d-48bf-be69-455d87093cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2bc = DecisionTree(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded35ba-62d3-47d3-8cf0-d383c0d97a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2bm = DecisionTree(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300aeb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3bc = KNN(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da75baa-0b02-487e-93b3-61b7da10f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3bm = KNN(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c25d54-04e5-4196-8ee3-c6619eec9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4bc = GaussianNBb(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e75aa9-2ec1-440c-9e28-d23582bdcf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4bm = GaussianNBb(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "059c76d8-7ac1-4157-bd3f-b30c292adbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t ---------- Training Multinomial Naive Bayes Classifier ---------- \n",
      "\n",
      "Fitting 2 folds for each of 150 candidates, totalling 300 fits\n",
      "Best score on Training set :  0.6364028963396402\n",
      "\n",
      "\n",
      " The best set of parameters of the pipeline in Training Phase are: \n",
      "Pipeline(steps=[('fs',\n",
      "                 SelectKBest(k=10000,\n",
      "                             score_func=<function chi2 at 0x7ff18f10bb50>)),\n",
      "                ('clf5', MultinomialNB(alpha=13))])\n",
      "\n",
      " *******  Scores on Validation Data  ******* \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.65      0.64     80866\n",
      "           1       0.64      0.63      0.64     80867\n",
      "\n",
      "    accuracy                           0.64    161733\n",
      "   macro avg       0.64      0.64      0.64    161733\n",
      "weighted avg       0.64      0.64      0.64    161733\n",
      "\n",
      "====================================================================\n",
      "Process Completed and time taken is : 0.3 minutes\n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "clf5bc = MultinomialNBb(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c76dc36f-0f9e-4098-a3f6-5e18b49e4e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t ---------- Training Multinomial Naive Bayes Classifier ---------- \n",
      "\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Best score on Training set :  0.6352937636565796\n",
      "\n",
      "\n",
      " The best set of parameters of the pipeline in Training Phase are: \n",
      "Pipeline(steps=[('fs',\n",
      "                 SelectKBest(k=10000,\n",
      "                             score_func=<function mutual_info_classif at 0x7ff18f5427a0>)),\n",
      "                ('clf5', MultinomialNB(alpha=20))])\n",
      "\n",
      " *******  Scores on Validation Data  ******* \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.67      0.65     80866\n",
      "           1       0.65      0.61      0.63     80867\n",
      "\n",
      "    accuracy                           0.64    161733\n",
      "   macro avg       0.64      0.64      0.64    161733\n",
      "weighted avg       0.64      0.64      0.64    161733\n",
      "\n",
      "====================================================================\n",
      "Process Completed and time taken is : 160.78 minutes\n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "clf5bm = MultinomialNBb(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf0c31e-e853-4f83-b390-3460b3ae6dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8bc = LinearSVCb(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd6f57a-9f02-4c31-b731-54cde7be9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8bm = LinearSVCb(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1572afa-c610-4892-b49b-1ed3b9e999ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9bc = SVCb(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56d6c1-b241-499f-a5c7-cc9892707bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9bm = SVCb(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89385cd7-cd92-4f24-bf87-c2d194755270",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10bc = RandomForest(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3992524d-3aad-4d44-be78-9dbcc2329316",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10bm = RandomForest(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ac6e8-90dd-4059-9e20-7061c98baab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_bow,X_valid_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b10c5-0817-4397-b2d8-f6e21f305e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d66e9-bf41-4d82-813e-009ace700d58",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a694d-1420-492a-be26-eaa2d5a9883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = joblib.load('X_train_tfidf.joblib', mmap_mode='c')\n",
    "X_valid_tfidf = joblib.load('X_valid_tfidf.joblib', mmap_mode='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a6c64-c1e0-49a3-b3c4-7bb77604f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1tc = logistic(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22929f-db7a-4493-924f-36036bf39b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1tm = logistic(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4cca78-607b-4713-b538-6cfc946235b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2tc = DecisionTree(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a258a-77fc-4f38-b804-69eb76cef9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2tm = DecisionTree(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a08893d-0d69-4945-9f9d-7f463066a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3tc = KNN(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63dbce-05cf-410b-b8d7-e6b53db1d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3tm = KNN(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9240e4-bc01-44e6-b380-7deab784d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4tc = GaussianNBb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca8879-7609-44a6-8f2c-6bafe0aa6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4tm = GaussianNBb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d252848f-7241-4ad2-b289-7cc70baf1819",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5tc = MultinomialNBb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe10576-a218-40ae-9b0c-7ca4a0224fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5tm = MultinomialNBb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa4884e-b898-4242-a5c7-a450e88cb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8tc = LinearSVCb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb685f67-5a73-477f-834b-f813fa4d47bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8tm = LinearSVCb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0259e95-4ba1-464c-a158-f986aba9b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9tc = SVCb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94b188-e13b-4720-bc27-0777e37265ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9tm = SVCb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82e641-2540-437c-8168-4654954abd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10tc = RandomForest(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ec61b-a22d-4402-a2dc-64c1f5ae35af",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10tm = RandomForest(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c183331-d250-4b6c-af39-70719178f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_tfidf,X_valid_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab6c25-ca12-4225-900c-e9570a3dc8bc",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250738ad-0a1b-4f1e-bb68-72e0720da137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticw2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Logistic Regression Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline1 = Pipeline([('clf1', LogisticRegression(class_weight='balanced'))])\n",
    "    clf1_parameters = {\n",
    "        'clf1__penalty':['l1', 'l2', 'elasticnet',],\n",
    "        'clf1__C':[10,0.01,0.001,0.003],     \n",
    "        'clf1__solver':['newton-cg','liblinear','sag',]\n",
    "        }\n",
    "    grid_search1 = GridSearchCV(estimator=pipeline1, param_grid=clf1_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search1.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search1.best_score_}\")\n",
    "    clf1 = grid_search1.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf1) \n",
    "    predicted_class_labels1 = clf1.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels1))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e05092-268b-4298-951b-fba34603fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTreew2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Decision Tree Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline2 = Pipeline([('clf2', DecisionTreeClassifier(random_state=40))])\n",
    "    clf2_parameters = {\n",
    "        'clf2__criterion':['entropy','gini',], \n",
    "        'clf2__max_features':['sqrt', 'log2',None],\n",
    "        'clf2__max_depth':[10,25,40,100],\n",
    "        'clf2__ccp_alpha':[0.002,0.01,0.1,]\n",
    "        }\n",
    "    grid_search2 = GridSearchCV(estimator=pipeline2, param_grid=clf2_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search2.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search2.best_score_}\")\n",
    "    clf2 = grid_search2.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf2)\n",
    "    predicted_class_labels2 = clf2.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels2))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3611085-7329-48b8-bd6b-4c8440a985da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNw2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training KNN Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline3 = Pipeline([('clf3', KNeighborsClassifier())])\n",
    "    clf3_parameters = {\n",
    "        'clf3__n_neighbors': [5,10,20,35,],      \n",
    "        'clf3__weights':['uniform', 'distance',],\n",
    "        'clf3__p':[1,2,],\n",
    "        'clf3__metric':['euclidean', 'manhattan',] \n",
    "        }\n",
    "    grid_search3 = GridSearchCV(estimator=pipeline3, param_grid=clf3_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search3.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search3.best_score_}\")\n",
    "    clf3 = grid_search3.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf3) \n",
    "    predicted_class_labels3 = clf3.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels3))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2abbe-cc53-440c-b5fe-2d6a28575164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianNBw2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    # Gaussian Naive Bayes\n",
    "    print('\\n\\t ---------- Training Gaussian Naive Bayes Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline4 = Pipeline([('clf4', GaussianNB())])\n",
    "    clf4_parameters = {\n",
    "        'clf4__var_smoothing': np.logspace(0,-9, num=60)\n",
    "        }\n",
    "    grid_search4 = GridSearchCV(estimator=pipeline4, param_grid=clf4_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search4.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search4.best_score_}\")\n",
    "    clf4 = grid_search4.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf4) \n",
    "    predicted_class_labels4 = clf4.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels4))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c03430-d20c-4474-8a61-d0d10aff7fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultinomialNBw2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Multinomial Naive Bayes Classifier ---------- \\n')  \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline5 = Pipeline([('clf5', MultinomialNB(fit_prior=True, class_prior=None))])\n",
    "    clf5_parameters = {\n",
    "        'clf5__alpha':[0,1,]\n",
    "        }\n",
    "    grid_search5 = GridSearchCV(estimator=pipeline5, param_grid=clf5_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search5.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search5.best_score_}\")\n",
    "    clf5 = grid_search5.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf5) \n",
    "    predicted_class_labels5 = clf5.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels5))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4248fc5a-d34c-4221-96df-d42e3f57918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearSVCw2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Linear SVC Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline8 = Pipeline([('clf8', LinearSVC(class_weight='balanced'))])\n",
    "    clf8_parameters = {\n",
    "        'clf8__C':[0.001,0.01,1,100,],\n",
    "        }  \n",
    "    grid_search8 = GridSearchCV(estimator=pipeline8, param_grid=clf8_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search8.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search8.best_score_}\")\n",
    "    clf8 = grid_search8.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf8) \n",
    "    predicted_class_labels8 = clf8.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels8))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5046dcce-a2c8-4ecd-a2f4-8069dd816200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVCw2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training SVM Classifier ---------- \\n') \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline9 = Pipeline([('clf9', SVC(probability=True))])\n",
    "    clf9_parameters = {\n",
    "        'clf9__C':[0.1,1,50,80,100,150],\n",
    "        'clf9__kernel':['poly','linear','sigmoid',],\n",
    "        }\n",
    "    grid_search9 = GridSearchCV(estimator=pipeline9, param_grid=clf9_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search9.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search9.best_score_}\")\n",
    "    clf9 = grid_search9.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf9) \n",
    "    predicted_class_labels9 = clf9.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels9))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d82b6b-3ff0-4ab9-ac4c-70d3ed78d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForestw2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Random Forest Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline10 = Pipeline([('clf10', RandomForestClassifier(class_weight='balanced'))])\n",
    "    clf10_parameters = {\n",
    "        'clf10__criterion':['entropy','gini',],\n",
    "        'clf10__max_depth':[10,30,80,120,],\n",
    "        'clf10__n_estimators':[30,50,],\n",
    "        'clf10__max_features':['sqrt','log2',None,] \n",
    "        } \n",
    "    grid_search10 = GridSearchCV(estimator=pipeline10, param_grid=clf10_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search10.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search10.best_score_}\")\n",
    "    clf10 = grid_search10.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf10) \n",
    "    predicted_class_labels10 = clf10.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels10))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dabe43-9317-4d69-b825-7e6f90997ea4",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2705d58c-176c-4d06-8856-7c1433879e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_spacy_train = joblib.load('w2v_spacy_train.joblib', mmap_mode='c')\n",
    "w2v_spacy_valid = joblib.load('w2v_spacy_valid.joblib', mmap_mode='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c660aac-e307-4cc0-9e3f-c92d60ee7b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1w = logisticw2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1b805-c7ed-403d-a547-98f2af6c70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2w = DecisionTreew2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce55bb3e-16c8-446d-baa9-4fe81a36e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3w = KNNw2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0632ef3-30b8-4bb2-9e0c-622ffc7c5c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4w = GaussianNBw2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee2c4a-15c3-4853-a64f-b40606b598e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5w = MultinomialNBw2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36864034-bd77-4299-9eaa-04fddb0365e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8w = LinearSVCw2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf04497-423c-4411-8fdf-ea30a6cd212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9w = SVCw2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e73262-effe-41a0-bbfc-ecdd8fb54afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10w = RandomForestw2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a918da4-a22b-47f6-985a-9ad919292a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "del w2v_spacy_train,w2v_spacy_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7c256-b660-4d89-8cd4-e3256a467207",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25760a1d-ca69-4cd6-bcca-984347760b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_spacy_train,glove_spacy_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c5966-516f-4121-a118-6f67792270cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1g = logisticw2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1bd1a-a8a0-4b79-b44e-4b08f288b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2g = DecisionTreew2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069bf096-9318-4e3f-aade-30a66c80466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3g = KNNw2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7440d6d5-d6cd-4a04-a133-82668d479192",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4g = GaussianNBw2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce145f78-fc37-455f-ab1f-daf6dd400424",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5g = MultinomialNBw2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719373de-f089-41e4-b110-ff39f00fe9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8g = LinearSVCw2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3faec-917f-40e3-b9a2-3c24763ce9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9g = SVCw2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee459e29-c359-4ddc-be8b-bc49be276fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10g = RandomForestw2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de72d5c-a1f8-480d-a1db-ef267908e11f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40e34ecc-662f-41db-9061-b2baba0b314a",
   "metadata": {},
   "source": [
    "### with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a32b0f-4d08-4934-bfaa-bbeb9a9656c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticpca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Logistic Regression Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline1 = Pipeline([('fs',PCA(n_components = 0.98)),\n",
    "                          ('clf1', LogisticRegression(class_weight='balanced'))])\n",
    "    clf1_parameters = {\n",
    "        'clf1__penalty':[l1', 'l2', 'elasticnet',],\n",
    "        'clf1__C':[10,0.01,0.001,0.003,],\n",
    "        'clf1__solver':['newton-cg','liblinear','sag',]\n",
    "        }\n",
    "    grid_search1 = GridSearchCV(estimator=pipeline1, param_grid=clf1_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search1.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search1.best_score_}\")\n",
    "    clf1 = grid_search1.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf1) \n",
    "    predicted_class_labels1 = clf1.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels1))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4686ea4-8d38-48a8-95f8-8ea63d5588d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTreepca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Decision Tree Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline2 = Pipeline([('fs', PCA(n_components = 0.98)),\n",
    "                          ('clf2', DecisionTreeClassifier(random_state=40))])\n",
    "    clf2_parameters = {\n",
    "        'clf2__criterion':['entropy','gini',], \n",
    "        'clf2__max_features':['sqrt', 'log2',None],\n",
    "        'clf2__max_depth':[10,25,40,100],\n",
    "        'clf2__ccp_alpha':[0.002,0.01,0.1,]\n",
    "        }\n",
    "    grid_search2 = GridSearchCV(estimator=pipeline2, param_grid=clf2_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search2.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search2.best_score_}\")\n",
    "    clf2 = grid_search2.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf2)\n",
    "    predicted_class_labels2 = clf2.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels2))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bfbaa3-62a0-4858-bef8-4730df7f473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNpca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training KNN Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline3 = Pipeline([('fs', PCA(n_components = 0.98)),\n",
    "                          ('clf3', KNeighborsClassifier())])\n",
    "    clf3_parameters = {\n",
    "        'clf3__n_neighbors': [3,10,20,35,],      \n",
    "        'clf3__weights':['uniform', 'distance',],\n",
    "        'clf3__p':[1,2,],\n",
    "        'clf3__metric':['euclidean', 'manhattan',] \n",
    "        }\n",
    "    grid_search3 = GridSearchCV(estimator=pipeline3, param_grid=clf3_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search3.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search3.best_score_}\")\n",
    "    clf3 = grid_search3.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf3) \n",
    "    predicted_class_labels3 = clf3.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels3))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad5cf3-1807-416a-8af3-0bc60ff9a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianNBpca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    # Gaussian Naive Bayes\n",
    "    print('\\n\\t ---------- Training Gaussian Naive Bayes Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline4 = Pipeline([('fs', PCA(n_components = 0.98)),\n",
    "                          ('clf4', GaussianNB())])\n",
    "    clf4_parameters = {\n",
    "        'clf4__var_smoothing': np.logspace(0,-9, num=60)\n",
    "        }\n",
    "    grid_search4 = GridSearchCV(estimator=pipeline4, param_grid=clf4_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search4.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search4.best_score_}\")\n",
    "    clf4 = grid_search4.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf4) \n",
    "    predicted_class_labels4 = clf4.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels4))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499b71a-43b2-4323-b3f8-27f9e3242cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultinomialNBpca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Multinomial Naive Bayes Classifier ---------- \\n')  \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline5 = Pipeline([('fs', PCA(n_components = 0.98)),\n",
    "                          ('clf5', MultinomialNB(fit_prior=True, class_prior=None))])\n",
    "    clf5_parameters = {\n",
    "        'clf5__alpha':[0,1,]\n",
    "        }\n",
    "    grid_search5 = GridSearchCV(estimator=pipeline5, param_grid=clf5_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search5.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search5.best_score_}\")\n",
    "    clf5 = grid_search5.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf5) \n",
    "    predicted_class_labels5 = clf5.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels5))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b754d39-b568-4bb8-bd7b-47945dc1fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearSVCpca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Linear SVC Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline8 = Pipeline([('fs', PCA(n_components = 0.98)),\n",
    "                          ('clf8', LinearSVC(class_weight='balanced'))])\n",
    "    clf8_parameters = {\n",
    "        'clf8__C':[0.001,0.01,1,100,],\n",
    "        }  \n",
    "    grid_search8 = GridSearchCV(estimator=pipeline8, param_grid=clf8_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search8.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search8.best_score_}\")\n",
    "    clf8 = grid_search8.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf8) \n",
    "    predicted_class_labels8 = clf8.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels8))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6bf36a-0080-47d6-8f73-4427b5a4cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVCpca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training SVM Classifier ---------- \\n') \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline9 = Pipeline([('fs', PCA(n_components = 0.98)),\n",
    "                          ('clf9', SVC(probability=True))])\n",
    "    clf9_parameters = {\n",
    "        'clf9__C':[0.1,1,50,80,100,150],\n",
    "        'clf9__kernel':['poly','linear','sigmoid',],\n",
    "        }\n",
    "    grid_search9 = GridSearchCV(estimator=pipeline9, param_grid=clf9_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search9.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search9.best_score_}\")\n",
    "    clf9 = grid_search9.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf9) \n",
    "    predicted_class_labels9 = clf9.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels9))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76752c74-653e-4041-97af-030a844b926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForestpca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Random Forest Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline10 = Pipeline([('fs', PCA(n_components = 0.98)),\n",
    "                           ('clf10', RandomForestClassifier(class_weight='balanced'))])\n",
    "    clf10_parameters = {\n",
    "        'clf10__criterion':['entropy','gini',],\n",
    "        'clf10__max_depth':[10,30,80,120,],\n",
    "        'clf10__n_estimators':[30,50,100,],\n",
    "        'clf10__max_features':['sqrt','log2',None,] \n",
    "        } \n",
    "    grid_search10 = GridSearchCV(estimator=pipeline10, param_grid=clf10_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search10.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search10.best_score_}\")\n",
    "    clf10 = grid_search10.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf10) \n",
    "    predicted_class_labels10 = clf10.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels10))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42187c-97ac-4071-ae82-9a95027c7baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1pca = logisticpca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c8474c-2425-4d58-930d-81150a574e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2pca = DecisionTreepca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598c388-0592-4ec5-b8d9-91733c2afe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3pca = KNNpca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b825be-1f4a-4ceb-86fc-41b2b010cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4pca = GaussianNBpca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554bcf3-e80d-467f-8006-5b459d9b9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5pca = MultinomialNBpca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c3748-0679-4107-a423-465de7c34449",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8pca = LinearSVCpca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b6939-978d-4a99-acaf-409bc892987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9pca = SVCpca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e225b7-3b85-45ed-9074-7f6ec50c94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10pca = RandomForestpca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9603bd7f-a292-4a81-913d-4d2a50afc156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e178d-01ad-4057-a655-ccc186cdd76d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7eaae-6933-48b2-90f5-c8e6ccd71b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491a0fb-23dc-47e8-b9d2-f915043a6f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2639019-cdde-4f64-9feb-e027b5ec0150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21f46c-820a-4b81-ac0e-bff95451e696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb3da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "print('\\n Total documents in the training set: '+str(len(trn_data))+'\\n')    \n",
    "print('\\n Total documents in the test set: '+str(len(tst_data))+'\\n') \n",
    "\n",
    "pr=precision_score(tst_cat, predicted, average='binary') \n",
    "print ('\\n Precision:'+str(pr)) \n",
    "\n",
    "rl=recall_score(tst_cat, predicted, average='binary') \n",
    "print ('\\n Recall:'+str(rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a669e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab5936",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1_mis = logistic_fs(x_trains,x_valids,y_train, y_valid,'MI') # by standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7d6e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1_anovas = logistic_fs(x_trains,x_valids,y_train, y_valid,'ANOVA') # Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f6eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895003f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c4eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df5918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4635ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get name of the 5 best features in decresing order i.e 1st feature is high import. then 2nd fet then 3rd so on\n",
    "X_train.columns[sel_five_cols.get_support()] # x_train is the same as my x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffl = logistic_fsf(x_trains,x_valids,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56275aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predl = ffl.predict(x_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving predicted class labels as txt file\n",
    "np.savetxt('Akash_Singh_test_class_labels.txt',predl,fmt='%d',delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c84942b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd43775-a976-4f79-9876-dd262f78515c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
