{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631578ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk    \n",
    "# !pip install emoji        \n",
    "# !pip install autocorrec\\t    \n",
    "# !pip install xgboost\n",
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e5a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')  # for using word_tokenizer\n",
    "# nltk.download('wordnet')  # for using Lemmatizer\n",
    "# nltk.download('averaged_perceptron_tagger') # for language processing i.e tagging words with their parts of speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19e54a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import spacy\n",
    "import nltk\n",
    "import re   # regular expression\n",
    "import string\n",
    "import emoji\n",
    "from autocorrect import Speller   # for correcting spelling\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize    # for tokenizing string into words\n",
    "from nltk.stem import WordNetLemmatizer    # for lemmatizing words\n",
    "from nltk.tag import pos_tag # for tagging words with their parts of speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d82ce6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e22a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d02e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest,chi2, mutual_info_classif,f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "204dfd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a38aca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9b53798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "187c8f51-cbc9-4e9e-aa0a-2bdbf7351295",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro_train = pd.read_csv('processed_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14f1bd31-af72-4b98-a42c-4bc17027e221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808661, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pre Processed Text</th>\n",
       "      <th>Class Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ocxtitan</td>\n",
       "      <td>Central Illinois</td>\n",
       "      <td>Jesus; where do you live?</td>\n",
       "      <td>Jesus; where do you live? Central Illinois</td>\n",
       "      <td>jesus live central illinois</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LeChuckly</td>\n",
       "      <td>To think - CNN used to be the acronym synonymo...</td>\n",
       "      <td>Even The CNN Staff Is Sick Of The Wall-To-Wall...</td>\n",
       "      <td>Even The CNN Staff Is Sick Of The Wall-To-Wall...</td>\n",
       "      <td>even cnn staff sick walltowall trump coverage ...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>throwitskrub8</td>\n",
       "      <td>But then again; you have to consider that all ...</td>\n",
       "      <td>agree to that part.It can also mean that gujra...</td>\n",
       "      <td>agree to that part.It can also mean that gujra...</td>\n",
       "      <td>agree parti also mean gujarat husbands good su...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresherthanyouuu</td>\n",
       "      <td>ughhhhh</td>\n",
       "      <td>If a guy told you he doesn't use social media ...</td>\n",
       "      <td>If a guy told you he doesn't use social media ...</td>\n",
       "      <td>guy tell use social media go head ughhhhh</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_kushagra</td>\n",
       "      <td>I should've put the</td>\n",
       "      <td>No; it's just a programming bug. After all; th...</td>\n",
       "      <td>No; it's just a programming bug. After all; th...</td>\n",
       "      <td>program bug android app music service still be...</td>\n",
       "      <td>sarcastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                           Comments  \\\n",
       "0          ocxtitan                                   Central Illinois   \n",
       "1         LeChuckly  To think - CNN used to be the acronym synonymo...   \n",
       "2     throwitskrub8  But then again; you have to consider that all ...   \n",
       "3  fresherthanyouuu                                            ughhhhh   \n",
       "4         _kushagra                                I should've put the   \n",
       "\n",
       "                                     Parent Comments  \\\n",
       "0                          Jesus; where do you live?   \n",
       "1  Even The CNN Staff Is Sick Of The Wall-To-Wall...   \n",
       "2  agree to that part.It can also mean that gujra...   \n",
       "3  If a guy told you he doesn't use social media ...   \n",
       "4  No; it's just a programming bug. After all; th...   \n",
       "\n",
       "                                                Text  \\\n",
       "0         Jesus; where do you live? Central Illinois   \n",
       "1  Even The CNN Staff Is Sick Of The Wall-To-Wall...   \n",
       "2  agree to that part.It can also mean that gujra...   \n",
       "3  If a guy told you he doesn't use social media ...   \n",
       "4  No; it's just a programming bug. After all; th...   \n",
       "\n",
       "                                  Pre Processed Text   Class Labels  \n",
       "0                        jesus live central illinois  non-sarcastic  \n",
       "1  even cnn staff sick walltowall trump coverage ...  non-sarcastic  \n",
       "2  agree parti also mean gujarat husbands good su...  non-sarcastic  \n",
       "3          guy tell use social media go head ughhhhh  non-sarcastic  \n",
       "4  program bug android app music service still be...      sarcastic  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_prepro_train.shape)\n",
    "df_prepro_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e410647c-a815-4cd3-9c87-aead0497a215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                     0\n",
       "Comments              38\n",
       "Parent Comments        0\n",
       "Text                   0\n",
       "Pre Processed Text     0\n",
       "Class Labels           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95c725b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202166, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pre Processed Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theyoungthaddeus</td>\n",
       "      <td>No one \"needs\" an assault foam dart blaster</td>\n",
       "      <td>Your son has to register those at the county j...</td>\n",
       "      <td>Your son has to register those at the county j...</td>\n",
       "      <td>son register county jungle gym mags hold ounce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just_an_asian_here</td>\n",
       "      <td>Cause all attractive women are uninteresting a...</td>\n",
       "      <td>Likely due to creative and interesting content.</td>\n",
       "      <td>Likely due to creative and interesting content...</td>\n",
       "      <td>likely due creative interesting content cause ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foxprowl</td>\n",
       "      <td>Poser.</td>\n",
       "      <td>Jon Stewart is going to HBO</td>\n",
       "      <td>Jon Stewart is going to HBO Poser.</td>\n",
       "      <td>jon stewart go hbo power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kd7rzv</td>\n",
       "      <td>Won't be long and Anet will start banning peop...</td>\n",
       "      <td>This post looks like bullshit market manipulat...</td>\n",
       "      <td>This post looks like bullshit market manipulat...</td>\n",
       "      <td>post looks like bullshit market manipulation w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ellefied</td>\n",
       "      <td>There goes my hope that Kubo does a Kojima as ...</td>\n",
       "      <td>Plus the Japanese typically do not talk shit w...</td>\n",
       "      <td>Plus the Japanese typically do not talk shit w...</td>\n",
       "      <td>plus japanese typically talk shit come busines...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                           Comments  \\\n",
       "0    theyoungthaddeus        No one \"needs\" an assault foam dart blaster   \n",
       "1  Just_an_asian_here  Cause all attractive women are uninteresting a...   \n",
       "2            Foxprowl                                             Poser.   \n",
       "3              kd7rzv  Won't be long and Anet will start banning peop...   \n",
       "4            Ellefied  There goes my hope that Kubo does a Kojima as ...   \n",
       "\n",
       "                                     Parent Comments  \\\n",
       "0  Your son has to register those at the county j...   \n",
       "1    Likely due to creative and interesting content.   \n",
       "2                        Jon Stewart is going to HBO   \n",
       "3  This post looks like bullshit market manipulat...   \n",
       "4  Plus the Japanese typically do not talk shit w...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Your son has to register those at the county j...   \n",
       "1  Likely due to creative and interesting content...   \n",
       "2                 Jon Stewart is going to HBO Poser.   \n",
       "3  This post looks like bullshit market manipulat...   \n",
       "4  Plus the Japanese typically do not talk shit w...   \n",
       "\n",
       "                                  Pre Processed Text  \n",
       "0  son register county jungle gym mags hold ounce...  \n",
       "1  likely due creative interesting content cause ...  \n",
       "2                           jon stewart go hbo power  \n",
       "3  post looks like bullshit market manipulation w...  \n",
       "4  plus japanese typically talk shit come busines...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "df_prepro_test = pd.read_csv('processed_test.csv')\n",
    "print(df_prepro_test.shape)\n",
    "df_prepro_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61e2e255-b4d9-4573-a536-9f7892d3ea98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                     0\n",
       "Comments              15\n",
       "Parent Comments        0\n",
       "Text                   0\n",
       "Pre Processed Text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35269736-9965-405f-9279-49a911a7802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tst.isna().sum() # problem: If the sentence is madeup of only stopwords, numbers etc., the sentence after preprocess becomes empty results in NaN\n",
    "# # fix, after preprocess check sentence length if its 0, return the orig sentence without preprocess. else preprocess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53f12d50-7b40-4b04-9560-4dfb92674f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df[['Comments','Parent Comments']][df['Comments'].isnull()]\n",
    "# X_tst[X_tst['Text'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd6dff77-ac97-4700-b15f-f4b9c92709ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "X = df_prepro_train['Pre Processed Text']\n",
    "y_trn = df_prepro_train['Class Labels']\n",
    "\n",
    "# Test data\n",
    "X_test = df_prepro_test['Pre Processed Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c68e5bf0-e447-494f-ba66-0c9a9f0c4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting Labels coloumn into numerical form of train data\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1bb816b-18c9-481a-937e-5abb848f1373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['non-sarcastic', 'sarcastic'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43446706-769f-4c91-bee1-c7fc710c68a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "156f8075-57cb-47f8-84b7-7e955f20256c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((646928,), (161733,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train validation split of data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=3,stratify=y)\n",
    "X_train.shape,X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c6da2-aeff-4372-bddf-d57d82f06cc7",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22e1126f-9ff3-496a-b990-b62a6828697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 = np.hstack(X.values) # since Count vec takes only list of items\n",
    "# X_tst1 = np.hstack(X_tst.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "664ab426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((646928, 5000), (161733, 5000), (202166, 5000))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,3),max_features=5000) # (1,3) means Unigrams, Bigrams and Trigrams\n",
    "X_train_bow = cv.fit_transform(X_train).toarray()\n",
    "X_valid_bow = cv.transform(X_valid).toarray()\n",
    "X_test_bow = cv.transform(X_test).toarray()\n",
    "\n",
    "X_train_bow.shape,X_valid_bow.shape,X_test_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe218df9-32a4-4373-b8a0-7fc581c12967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez_compressed(\"X_train_bow.npz\", X_train_bow)\n",
    "\n",
    "# loaded_array = np.load(\"X_train_bow.npy\")\n",
    "# print(loaded_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # convert array into dataframe\n",
    "# dbtr = pd.DataFrame(X_train_bow)\n",
    "# dbvd = pd.DataFrame(X_valid_bow)\n",
    "# dbtt = pd.DataFrame(X_test_bow)\n",
    "\n",
    "# # save the dataframe as a csv file\n",
    "# dbtr.to_csv(\"x_train_bow.csv\",index=False)\n",
    "# dbvd.to_csv(\"x_valid_bow.csv\",index=False)\n",
    "# dbtt.to_csv(\"x_test_bow.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f7147c-5ae5-4504-8eea-6056ceec4b51",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dcbdbe1-85dd-4cf5-b884-8eb4fd2454e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((646928, 60000), (161733, 60000), (202166, 60000))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3),max_features=60000,use_idf=True,smooth_idf=True)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "X_valid_tfidf = tfidf.transform(X_valid).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test).toarray()\n",
    "\n",
    "X_train_tfidf.shape,X_valid_tfidf.shape,X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837e48e-069a-434b-920f-76971ae28692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert array into dataframe\n",
    "# dbbtr = pd.DataFrame(X_train_tfidf)\n",
    "# dbbvd = pd.DataFrame(X_valid_tfidf)\n",
    "# dbbtt = pd.DataFrame(X_test_tfidf)\n",
    "\n",
    "# # save the dataframe as a csv file\n",
    "# dbbtr.to_csv(\"x_train_tfidf.csv\",index=False)\n",
    "# dbbvd.to_csv(\"x_valid_tfidf.csv\",index=False)\n",
    "# dbbtt.to_csv(\"x_test_tfidf.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac4669-0e68-409e-80f2-529a93445719",
   "metadata": {},
   "source": [
    "# word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d15a3-77f0-4c81-899a-d8d69ff23b8b",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb008abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1eab65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d4ebf-6206-42eb-af7f-367c9b4d1be4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test_bow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train,X_valid,\u001b[43mX_test_bow\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test_bow' is not defined"
     ]
    }
   ],
   "source": [
    "X_train,X_valid,X_test_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "94db4dd6-6ab5-4dca-9c4d-251cfb7f6d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "789798    drake nba millions benjamin franklin canadian ...\n",
       "667532    marijuana marijuana quite popularity history a...\n",
       "736283    separate oxygen hydrogen water ignite well wal...\n",
       "176253         things go bump night allow take picture lego\n",
       "801665    girlfriends birthday gift show us picture unde...\n",
       "                                ...                        \n",
       "114413    happen america september day change history im...\n",
       "77202     meth maybe sometimes method friday saturday mo...\n",
       "471315    extra chief trump responsible us attack skips ...\n",
       "748699    common times actually period time jewish troop...\n",
       "306343    people seem forget pence ultra right wing chri...\n",
       "Name: Pre Processed Text, Length: 646928, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9182846f-f417-4856-b076-a2f242b3e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences \n",
    "    def __iter__(self):\n",
    "        for line in self.sentences:\n",
    "            yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "04c3a97b-1ca0-4660-9625-a5e246061c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arr = []\n",
    "for item in X_train.values:\n",
    "    doc = nlp(item)\n",
    "    input_arr.append(doc.vector)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "36fc9b4f-cb63-4354-9665-c810c37451e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_arr\n",
    "# X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "812d85f6-146e-44e2-945a-cc95f0e24d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0c7d63c6-f1c3-464d-b6dd-85b9e08fc1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MySentences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c6e70544-29a8-4f1b-9758-7823aa5a68ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drake', 'nba', 'millions', 'benjamin', 'franklin', 'canadian', 'dollars']\n"
     ]
    }
   ],
   "source": [
    "for s in sentences:\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "27d6d774-41b8-489f-bd48-bd53267f66d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=sentences,vector_size=100,sg= 0,window=5,min_count=1,workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "14a12c1e-0026-4396-aa64-4dc9e718ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3e97ba97-bf4e-4630-87fc-d9530a683c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11633611"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0834f56c-b50c-40f2-8f22-a4d2e145144c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=144163, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c0a7e139-93a4-456f-88df-5383bb504a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.4755176e+00, -1.0003923e+00, -5.7706565e-01, -1.9328494e-01,\n",
       "         7.7569312e-01,  2.2920718e+00,  2.2325490e+00,  7.3089516e-01,\n",
       "         2.3061707e+00,  1.8381729e+00,  1.4842108e+00, -1.6856040e+00,\n",
       "        -3.4607871e+00, -3.5753337e-01, -4.0258390e-01,  1.3328685e+00,\n",
       "        -1.1337510e+00, -6.2122124e-01,  3.1225592e-01, -6.1956781e-01,\n",
       "         6.5621489e-01,  1.1669500e+00, -2.8425914e-01,  5.1055533e-01,\n",
       "        -9.3304992e-01, -1.0259293e+00,  8.0554563e-01, -1.0876219e+00,\n",
       "        -1.0551823e+00, -1.0465925e+00, -2.3096409e+00,  9.1883606e-01,\n",
       "         1.2544596e+00, -1.6090335e-01,  1.3296086e+00, -7.8464472e-01,\n",
       "        -8.2915300e-01,  1.1054187e+00, -1.3686762e+00,  9.8261929e-01,\n",
       "         1.2752582e+00, -1.3362261e+00,  9.9796218e-01,  1.8851825e+00,\n",
       "        -1.8528808e+00, -8.8108480e-01,  7.7025574e-01, -6.9624591e-01,\n",
       "         5.7853645e-01,  2.4737980e+00,  1.0349784e+00, -1.3738530e+00,\n",
       "        -3.2401863e-01,  1.7385583e+00, -8.5409081e-01,  1.1927493e+00,\n",
       "        -1.6226709e+00,  3.4863225e-01, -1.9988834e+00, -7.0056349e-01,\n",
       "        -8.9086115e-02, -9.2031056e-01,  1.1125011e+00,  2.7494129e-02,\n",
       "         1.0398085e-02,  5.3552753e-01, -1.9807398e+00,  8.9276773e-01,\n",
       "        -1.3692476e+00,  1.1413810e+00, -2.4683349e-01,  1.9218092e+00,\n",
       "        -1.4007070e+00,  2.8770706e-01,  1.6306045e+00, -1.8376783e+00,\n",
       "        -4.8764077e-01,  6.6259301e-01,  8.5805637e-01, -9.1850066e-01,\n",
       "        -4.2747045e+00, -1.4615953e+00, -1.6324900e-01, -3.1719157e-03,\n",
       "         1.6762437e+00,  2.9405710e-01,  3.1382020e+00,  9.6299517e-01,\n",
       "        -4.3905294e-01, -1.5351923e+00, -1.1144952e+00,  6.7727822e-01,\n",
       "        -4.1501322e-01,  1.8766665e-01, -1.4640794e+00,  6.8482533e-02,\n",
       "         2.4098435e-02, -5.4461735e-01,  1.5384191e+00, -9.9358088e-01],\n",
       "       [ 1.9541676e+00, -1.7468698e+00, -1.3211609e+00, -9.2718028e-02,\n",
       "         1.2686678e+00,  1.0429747e+00,  2.4287784e+00, -3.3090502e-02,\n",
       "         2.3953216e+00,  1.1012441e+00,  1.8799319e+00, -3.4709454e-01,\n",
       "        -2.9239714e+00, -4.7200289e-01,  1.4274885e-01,  1.1842614e+00,\n",
       "        -1.2626023e+00,  3.5550800e-01,  6.3236028e-01, -2.3108959e-01,\n",
       "        -1.7174637e-01,  1.4309220e+00,  6.9697499e-01, -1.5586907e-01,\n",
       "        -1.0301588e+00, -1.2274659e+00,  9.3417925e-01, -3.5651496e-01,\n",
       "        -1.5376896e+00, -1.4842352e+00, -1.8340865e-01,  3.0776775e-01,\n",
       "         1.8382748e+00, -8.0615413e-01,  3.0162156e-01, -1.2403101e+00,\n",
       "        -1.3414265e+00,  1.4220054e+00, -1.3217676e+00,  3.1650349e-01,\n",
       "         8.9004433e-01, -1.6278684e+00,  1.6096835e+00,  1.4526876e+00,\n",
       "        -8.3480197e-01, -8.3909959e-01,  6.6902667e-02, -1.0140165e+00,\n",
       "         4.4493163e-01,  1.4848107e+00, -7.4736971e-01, -8.4028190e-01,\n",
       "        -1.4536008e-01,  1.1680110e+00, -4.7360742e-01,  1.6641600e+00,\n",
       "        -1.0145200e+00, -1.8284963e-01, -1.3415146e+00, -1.3983363e+00,\n",
       "         2.7751476e-01, -9.7520983e-01,  1.9640411e-01,  7.7457297e-01,\n",
       "         4.5564681e-01,  5.2689642e-01, -1.7926545e+00, -4.1862053e-01,\n",
       "        -1.4070801e+00,  8.1296945e-01,  4.4535282e-01,  1.2956203e+00,\n",
       "        -8.7620270e-01, -1.8426056e-01,  1.7814888e+00, -1.8680617e+00,\n",
       "        -6.7049992e-01,  1.2300314e+00,  6.9530571e-01, -2.0129099e+00,\n",
       "        -1.6038852e+00, -4.4470590e-01, -1.4960137e-01,  4.4278398e-01,\n",
       "         1.6216204e+00,  1.5547889e+00,  2.1984856e+00,  7.8130138e-01,\n",
       "        -1.2488189e+00, -9.0722102e-01, -1.0049176e+00,  4.0217054e-01,\n",
       "        -4.6007299e-01,  4.6981892e-01, -1.6709806e+00, -4.7418308e-02,\n",
       "        -4.8703054e-01,  8.7808236e-02,  6.3464564e-01, -1.3891877e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['computer','laptop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a6a26fb4-4d8f-405d-9b33-82e9c47f398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word2vec(x):\n",
    "#     doc = nlp(x)\n",
    "#     vec = doc.vector\n",
    "#     return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8aa86411-9ff7-48c8-a944-80c05b7cc615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['vec'] = df['review'].apply(lambda x: word2vec(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc66cd02-f6e9-4322-8aaf-f1f1c3f96b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77b3b74c-28df-4169-85a8-7d5791aa91cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5be31915-df66-47b8-869c-02793f4bb3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v=dict(zip(model.wv.index_to_key, model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "623db553-097f-46c7-917a-671a38cd4b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5e1cfff1-fa6b-4a1b-94b4-79631fffb099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec,dim):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = dim\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec.keys()] or [np.zeros(self.dim)], axis=0) for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "38871eac-97fa-4e9e-a5a7-07f85d4b80f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0824815 ,  0.71639675,  0.21218523, -0.34808567,  0.77492297,\n",
       "        1.4794843 ,  0.6010719 ,  0.22179958,  1.6036116 ,  0.9052475 ,\n",
       "        0.87729186, -0.38980472, -2.2052183 , -0.5118392 , -0.55851465,\n",
       "        1.9769785 , -0.7538709 ,  0.6616144 , -0.48489776, -1.002662  ,\n",
       "        1.525203  , -0.16999859, -0.4207438 ,  1.139093  , -1.515057  ,\n",
       "        0.30103773,  0.20051995,  0.23532754,  0.20731485, -1.3388798 ,\n",
       "       -0.90691745,  1.703035  ,  2.0329447 , -0.80370677,  0.09067011,\n",
       "       -0.32836422, -0.44403347,  0.64649737, -1.2978325 ,  1.3318552 ,\n",
       "        1.2680016 , -0.3032618 ,  0.18964666,  1.4722872 , -1.2167127 ,\n",
       "        0.18335038,  0.19206388, -0.9325362 , -0.28691283,  0.9847708 ,\n",
       "        1.3986429 , -0.6484471 ,  0.3222345 ,  0.6789633 , -0.4748214 ,\n",
       "        0.70946497, -2.216866  , -0.75549024, -1.1875656 , -0.5204948 ,\n",
       "       -0.17813279, -0.7590742 ,  1.5861192 ,  0.05278382,  0.38371533,\n",
       "       -0.05811653, -1.7596617 ,  0.3337974 , -0.8002107 ,  0.35835427,\n",
       "       -0.60430896,  1.7696618 , -0.32513386, -0.49064678,  1.4002503 ,\n",
       "       -1.7897534 , -0.59278315, -0.20181972,  1.0709932 , -0.5111989 ,\n",
       "       -2.3518243 , -0.89271474, -0.10539907, -0.47897545, -0.21617419,\n",
       "        0.50755274,  1.1471093 ,  1.1263436 ,  0.47332776, -0.9347746 ,\n",
       "       -0.7791906 ,  0.23648573, -1.3413388 , -0.06077297, -0.59889746,\n",
       "        1.2687843 ,  0.15113397,  0.33563057,  0.84049916,  0.88338804],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([w2v[w] for w in ['word','computer'] if w in w2v.keys()],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c81557a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "etree_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v,100)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200,n_jobs=32))])\n",
    "# etree_w2v_tfidf = Pipeline([\n",
    "#     (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "#     (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef4dd6-ebfa-4188-b66b-7cea0c2f21c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3ad0784f-195a-4003-92f6-d6eda0744423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5270200072863425"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(etree_w2v,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "11d7e72c-0507-474f-928f-87a519da775e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0824815 ,  0.71639675,  0.21218523, -0.34808567,  0.77492297,\n",
       "         1.4794843 ,  0.6010719 ,  0.22179958,  1.6036116 ,  0.9052475 ,\n",
       "         0.87729186, -0.38980472, -2.2052183 , -0.5118392 , -0.55851465,\n",
       "         1.9769785 , -0.7538709 ,  0.6616144 , -0.48489776, -1.002662  ,\n",
       "         1.525203  , -0.16999859, -0.4207438 ,  1.139093  , -1.515057  ,\n",
       "         0.30103773,  0.20051995,  0.23532754,  0.20731485, -1.3388798 ,\n",
       "        -0.90691745,  1.703035  ,  2.0329447 , -0.80370677,  0.09067011,\n",
       "        -0.32836422, -0.44403347,  0.64649737, -1.2978325 ,  1.3318552 ,\n",
       "         1.2680016 , -0.3032618 ,  0.18964666,  1.4722872 , -1.2167127 ,\n",
       "         0.18335038,  0.19206388, -0.9325362 , -0.28691283,  0.9847708 ,\n",
       "         1.3986429 , -0.6484471 ,  0.3222345 ,  0.6789633 , -0.4748214 ,\n",
       "         0.70946497, -2.216866  , -0.75549024, -1.1875656 , -0.5204948 ,\n",
       "        -0.17813279, -0.7590742 ,  1.5861192 ,  0.05278382,  0.38371533,\n",
       "        -0.05811653, -1.7596617 ,  0.3337974 , -0.8002107 ,  0.35835427,\n",
       "        -0.60430896,  1.7696618 , -0.32513386, -0.49064678,  1.4002503 ,\n",
       "        -1.7897534 , -0.59278315, -0.20181972,  1.0709932 , -0.5111989 ,\n",
       "        -2.3518243 , -0.89271474, -0.10539907, -0.47897545, -0.21617419,\n",
       "         0.50755274,  1.1471093 ,  1.1263436 ,  0.47332776, -0.9347746 ,\n",
       "        -0.7791906 ,  0.23648573, -1.3413388 , -0.06077297, -0.59889746,\n",
       "         1.2687843 ,  0.15113397,  0.33563057,  0.84049916,  0.88338804]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MeanEmbeddingVectorizer(w2v).transform([['computer','word']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040b9d4-50fa-4c81-9399-9d112abb4436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "368ebf33-2fe7-4fc6-80b5-5a234ddda735",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698109da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d5d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935232c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0433c5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1490c780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "83427490",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GaussianNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gnb \u001b[38;5;241m=\u001b[39m \u001b[43mGaussianNB\u001b[49m()\n\u001b[1;32m      2\u001b[0m gnb\u001b[38;5;241m.\u001b[39mfit(X_train_bow,y_train)\n\u001b[1;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m gnb\u001b[38;5;241m.\u001b[39mpredict(X_test_bow)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GaussianNB' is not defined"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_bow,y_train)\n",
    "y_pred = gnb.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529be3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990c6dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626204d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b0bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.5.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7eb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863dbfff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1905eb62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf392d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3ea84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c624659d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877953e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70191e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "# Load the spacy model.\n",
    "nlp = en_core_web_sm.load()\n",
    "# Process a sentence using the model\n",
    "doc = nlp(X_train.values[0])\n",
    "print(doc.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa45e10",
   "metadata": {},
   "source": [
    "[ 0.15583833  0.22911808  0.13835862 -0.2090293  -0.00363565 -0.13118638\n",
    " -0.15676734  0.23040245 -0.24502586 -0.10665981  0.23412983 -0.21705897\n",
    "  0.13545002  0.13861737  0.12415285  0.21369664 -0.19687523 -0.10246313\n",
    " -0.03955808  0.27656794 -0.11084636 -0.28008527  0.3439968   0.0482447\n",
    " -0.06924975 -0.03105986 -0.54718924 -0.3975329   0.1704354  -0.16617411\n",
    "  0.09593288 -0.31931752  0.42519966  0.05527496  0.06130749 -0.3888249\n",
    " -0.14431256  0.07848645  0.07683445  0.3278561  -0.3507611  -0.1357377\n",
    "  0.28880984  0.09166662  0.0593424  -0.04693977  0.1484201   0.01081875\n",
    "  0.25666746  0.21095097  0.1744978   0.04549917  0.19118404 -0.25923675\n",
    " -0.06680956  0.25583443 -0.06317639 -0.2850735  -0.03264944  0.0042502\n",
    " -0.07356709 -0.09716224  0.08560921 -0.09941689 -0.12068396 -0.21283795\n",
    " -0.03129313 -0.32813746  0.42847297 -0.37240502  0.26251498 -0.11578684\n",
    " -0.03221569 -0.05079779  0.17090753 -0.12238634 -0.07934     0.22784583\n",
    "  0.20331696 -0.5104732   0.07128731 -0.03987676 -0.05812799  0.19840325\n",
    " -0.10885198 -0.09540041  0.2161586  -0.17952771  0.04469092  0.06091264\n",
    "  0.05722181 -0.02751416  0.19805025  0.32649586 -0.06467736  0.10155083]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c7771c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32193344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4421c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arr = np.array(input_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c1e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d6c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_arr = []\n",
    "for item in X_test.values:\n",
    "    doc = nlp(item)\n",
    "    input_test_arr.append(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbf279",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_arr = np.array(input_test_arr)\n",
    "input_test_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "216a5020",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GaussianNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [103], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gnb \u001b[38;5;241m=\u001b[39m \u001b[43mGaussianNB\u001b[49m()\n\u001b[1;32m      2\u001b[0m gnb\u001b[38;5;241m.\u001b[39mfit(input_arr,y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GaussianNB' is not defined"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(input_arr,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(input_test_arr)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf270f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea745ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3b3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c489d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b06a94c6-8cff-4816-ba5d-18445b89a398",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3429407-3182-4ebb-898c-78fe52e512fe",
   "metadata": {},
   "source": [
    "### With Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24777d36-0cc2-4dc3-a65e-de6a7ecf16cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "371380e6-1731-43ba-9d85-b70db854a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    fsl = 0\n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=100))\n",
    "    \n",
    "    print('\\n\\t ---------- Training Logistic Regression Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline1 = Pipeline([('fs',fsl),\n",
    "                          ('clf1', LogisticRegression(class_weight='balanced'))])\n",
    "    clf1_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf1__penalty':['none', 'l1', 'l2', 'elasticnet',],\n",
    "        'clf1__C':[10,0.01,0.001,0.003,0.004],     \n",
    "        'clf1__solver':['newton-cg','liblinear','sag',]\n",
    "        }\n",
    "    grid_search1 = GridSearchCV(estimator=pipeline1, param_grid=clf1_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search1.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search1.best_score_}\")\n",
    "    clf1 = grid_search1.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf1) \n",
    "    predicted_class_labels1 = clf1.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels1))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a7a75407-270c-4276-926b-59fe2b613fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    print('\\n\\t ---------- Training Decision Tree Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline2 = Pipeline([('fs', fsl),\n",
    "                          ('clf2', DecisionTreeClassifier(random_state=40))])\n",
    "    clf2_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf2__criterion':['entropy','gini',], \n",
    "        'clf2__max_features':['sqrt', 'log2',None],\n",
    "        'clf2__max_depth':[10,25,40,100],\n",
    "        'clf2__ccp_alpha':[0.002,0.004,0.009,0.01,0.1,]\n",
    "        }\n",
    "    grid_search2 = GridSearchCV(estimator=pipeline2, param_grid=clf2_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search2.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search2.best_score_}\")\n",
    "    clf2 = grid_search2.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf2)\n",
    "    predicted_class_labels2 = clf2.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels2))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4062c346-0457-46a8-ad89-6a3293925b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    print('\\n\\t ---------- Training KNN Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline3 = Pipeline([('fs', fsl),\n",
    "                          ('clf3', KNeighborsClassifier())])\n",
    "    clf3_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf3__n_neighbors': [1,2,3,5,10,20,25,35,36,38,],      \n",
    "        'clf3__weights':['uniform', 'distance',],\n",
    "        'clf3__p':[1,2,],\n",
    "        'clf3__metric':['euclidean', 'manhattan',] \n",
    "        }\n",
    "    grid_search3 = GridSearchCV(estimator=pipeline3, param_grid=clf3_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search3.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search3.best_score_}\")\n",
    "    clf3 = grid_search3.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf3) \n",
    "    predicted_class_labels3 = clf3.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels3))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "312880c9-7032-464e-bb93-cccb13d9e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianNB(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    # Gaussian Naive Bayes\n",
    "    print('\\n\\t ---------- Training Gaussian Naive Bayes Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline4 = Pipeline([('fs', fsl),\n",
    "                          ('clf4', GaussianNB())])\n",
    "    clf4_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf4__var_smoothing': np.logspace(0,-9, num=60)\n",
    "        }\n",
    "    grid_search4 = GridSearchCV(estimator=pipeline4, param_grid=clf4_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search4.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search4.best_score_}\")\n",
    "    clf4 = grid_search4.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf4) \n",
    "    predicted_class_labels4 = clf4.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels4))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "62a9a628-b55f-4c29-b899-b760508ed1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultinomialNB(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    print('\\n\\t ---------- Training Multinomial Naive Bayes Classifier ---------- \\n')  \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline5 = Pipeline([('fs', fsl),\n",
    "                          ('clf5', MultinomialNB(fit_prior=True, class_prior=None))])\n",
    "    clf5_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf5__alpha':[0,1,]\n",
    "        }\n",
    "    grid_search5 = GridSearchCV(estimator=pipeline5, param_grid=clf5_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search5.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search5.best_score_}\")\n",
    "    clf5 = grid_search5.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf5) \n",
    "    predicted_class_labels5 = clf5.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels5))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e90e3426-3da7-47a0-a243-9a101c1ca0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearSVC(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    print('\\n\\t ---------- Training Linear SVC Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline8 = Pipeline([('fs', fsl),\n",
    "                          ('clf8', LinearSVC(class_weight='balanced'))])\n",
    "    clf8_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf8__C':[0.001,0.01,1,100,],\n",
    "        }  \n",
    "    grid_search8 = GridSearchCV(estimator=pipeline8, param_grid=clf8_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search8.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search8.best_score_}\")\n",
    "    clf8 = grid_search8.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf8) \n",
    "    predicted_class_labels8 = clf8.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels8))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "866e17ab-6d5f-4078-945b-ba45ad831a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVC(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    print('\\n\\t ---------- Training SVM Classifier ---------- \\n') \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline9 = Pipeline([('fs', fsl),\n",
    "                          ('clf9', SVC(probability=True))])\n",
    "    clf9_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf9__C':[0.1,1,50,60,70,80,100,150],\n",
    "        'clf9__kernel':['poly','linear','sigmoid','rbf',],\n",
    "        }\n",
    "    grid_search9 = GridSearchCV(estimator=pipeline9, param_grid=clf9_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search9.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search9.best_score_}\")\n",
    "    clf9 = grid_search9.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf9) \n",
    "    predicted_class_labels9 = clf9.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels9))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "872f36c9-9237-41c0-9e97-8263e899f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000)) \n",
    "    \n",
    "    print('\\n\\t ---------- Training Random Forest Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline10 = Pipeline([('fs', fsl),\n",
    "                           ('clf10', RandomForestClassifier(class_weight='balanced'))])\n",
    "    clf10_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf10__criterion':['entropy','gini',],\n",
    "        'clf10__max_depth':[10,25,30,45,50,80,100,200,],\n",
    "        'clf10__n_estimators':[30,50,100,],\n",
    "        'clf10__max_features':['sqrt','log2',None,] \n",
    "        } \n",
    "    grid_search10 = GridSearchCV(estimator=pipeline10, param_grid=clf10_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search10.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search10.best_score_}\")\n",
    "    clf10 = grid_search10.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf10) \n",
    "    predicted_class_labels10 = clf10.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels10))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ef475d00-0ac6-4ae5-8741-0035a328dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000)) \n",
    "    \n",
    "    print('\\n\\t ---------- Training AdaBoost Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    be1 = SVC(kernel='poly',C=100,probability=True)              \n",
    "    be2 = LogisticRegression(solver='newton-cg',C=0.001)        \n",
    "    pipeline11 = Pipeline([('fs', fsl),\n",
    "                           ('clf11', AdaBoostClassifier(algorithm='SAMME.R'))])\n",
    "    clf11_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf11__base_estimator':[be1,be2,],\n",
    "        'clf11__n_estimators':[10, 30,50, 80,100, 300,], \n",
    "        'clf11__learning_rate':[0.0001, 0.001, 0.01, 0.1,],\n",
    "        'clf11__random_state':[40,]\n",
    "        }  \n",
    "    grid_search11 = GridSearchCV(estimator=pipeline11, param_grid=clf11_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search11.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search11.best_score_}\")\n",
    "    clf11 = grid_search11.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf11) \n",
    "    predicted_class_labels11 = clf11.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels11))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cea766d5-66ae-4952-a244-245540af722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    print('\\n\\t ---------- Training LDA Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline12 = Pipeline([('fs', fsl),\n",
    "                           ('clf12', LinearDiscriminantAnalysis())])\n",
    "    clf12_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf12__solver':['svd','lsqr', 'eigen',],\n",
    "        'clf12__shrinkage':['auto',0.0005,0.0008,0.001,0.005,0.01,0.1,None,]\n",
    "        } \n",
    "    grid_search12 = GridSearchCV(estimator=pipeline12, param_grid=clf12_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search12.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search12.best_score_}\")\n",
    "    clf12 = grid_search12.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf12) \n",
    "    predicted_class_labels12 = clf12.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels12)) \n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "62e7390d-0248-4cf5-b78d-d9a6f96ccd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    print('\\n\\t ---------- Training XGBoost Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline13 = Pipeline([('fs', fsl),\n",
    "                           ('clf13', XGBClassifier(booster = 'gbtree',objective='multi:softmax',num_class = 4,eval_metric = 'merror'))])\n",
    "    clf13_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf13__n_estimators':[20,50,80,100,200,], \n",
    "        'clf13__max_depth':[5,10,15,30,50,100,], \n",
    "        'clf13__learning_rate':[0.15, 0.2, 0.3,],\n",
    "        'clf13__gamma':[0,0.1,0.2,0.3,],\n",
    "        'clf13__min_child_weight':[1,3,5,7,30,], \n",
    "        'clf13__colsample_bytree':[0.3,0.6,0.9,1,2], \n",
    "        'clf13__reg_lambda':[0,0.1,0.2,0.4,0.8,],      \n",
    "        }\n",
    "    grid_search13 = GridSearchCV(estimator=pipeline13, param_grid=clf13_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search13.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search13.best_score_}\")\n",
    "    clf13 = grid_search13.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf13) \n",
    "    predicted_class_labels13 = clf13.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels13))   \n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d6906-819b-461c-bb94-ef7e8ade876e",
   "metadata": {},
   "source": [
    "##### bow training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09b9fbea-0b7a-4858-8fa8-468850536e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticw(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    # fsl = 0\n",
    "    # if fs == 'MI' :\n",
    "    #     fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    # elif fs == 'chi2':\n",
    "    # fsl =  SelectKBest(SelectKBest(chi2, k=100))\n",
    "    \n",
    "    print('\\n\\t ---------- Training Logistic Regression Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline1 =  LogisticRegression(class_weight='balanced')\n",
    "    # clf1_parameters = {\n",
    "    #     'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "    #     'clf1__penalty':['none', 'l1', 'l2', 'elasticnet',],\n",
    "    #     'clf1__C':[10,0.01,0.001,0.003,0.004],     \n",
    "    #     'clf1__solver':['newton-cg','liblinear','sag',]\n",
    "    #     }\n",
    "    # grid_search1 = GridSearchCV(estimator=pipeline1, param_grid={}, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    pipeline1.fit(x_train,y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"Best score on Training set :  {grid_search1.best_score_}\")\n",
    "    clf1 = grid_search1.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf1) \n",
    "    predicted_class_labels1 = clf1.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels1))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10f33edc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m m \u001b[38;5;241m=\u001b[39m LogisticRegression(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# k = Se\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_bow\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/khadga_19024/khadga_19024/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1233\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1231\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1233\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1258\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/khadga_19024/khadga_19024/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/khadga_19024/khadga_19024/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/khadga_19024/khadga_19024/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m = LogisticRegression(n_jobs=32,max_iter=10)\n",
    "# k = Se\n",
    "m.fit(X_train_bow,y_train)\n",
    "# clf1 = logisticw(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80daa4b6-f264-48ef-a19a-422133eadea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b763af64-0c3d-48bf-be69-455d87093cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = DecisionTree(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300aeb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = KNN(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c25d54-04e5-4196-8ee3-c6619eec9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = GaussianNB(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c76d8-7ac1-4157-bd3f-b30c292adbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5 = MultinomialNB(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf0c31e-e853-4f83-b390-3460b3ae6dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8 = LinearSVC(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1572afa-c610-4892-b49b-1ed3b9e999ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9 = SVC(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89385cd7-cd92-4f24-bf87-c2d194755270",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10 = RandomForest(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1a41e-aa45-4af9-b9e8-1a9fcfcb81b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf11 = AdaBoost(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2a53d-f0f5-4062-9c99-e2672077ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf12 = LDA(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d14c5-2c26-422b-bab4-f0386382b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf13 = XGBoost(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4089ef-5845-4634-a28a-3eda60472af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0670d7d-d8e4-4952-ae28-96faf8d384a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a32b0f-4d08-4934-bfaa-bbeb9a9656c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4686ea4-8d38-48a8-95f8-8ea63d5588d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bfbaa3-62a0-4858-bef8-4730df7f473d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad5cf3-1807-416a-8af3-0bc60ff9a0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb3da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "print('\\n Total documents in the training set: '+str(len(trn_data))+'\\n')    \n",
    "print('\\n Total documents in the test set: '+str(len(tst_data))+'\\n') \n",
    "\n",
    "pr=precision_score(tst_cat, predicted, average='binary') \n",
    "print ('\\n Precision:'+str(pr)) \n",
    "\n",
    "rl=recall_score(tst_cat, predicted, average='binary') \n",
    "print ('\\n Recall:'+str(rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a669e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab5936",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1_mis = logistic_fs(x_trains,x_valids,y_train, y_valid,'MI') # by standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7d6e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1_anovas = logistic_fs(x_trains,x_valids,y_train, y_valid,'ANOVA') # Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f6eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895003f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c4eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df5918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4635ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get name of the 5 best features in decresing order i.e 1st feature is high import. then 2nd fet then 3rd so on\n",
    "X_train.columns[sel_five_cols.get_support()] # x_train is the same as my x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffl = logistic_fsf(x_trains,x_valids,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56275aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predl = ffl.predict(x_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving predicted class labels as txt file\n",
    "np.savetxt('Akash_Singh_test_class_labels.txt',predl,fmt='%d',delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c84942b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
