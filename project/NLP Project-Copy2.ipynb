{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631578ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk    \n",
    "# !pip install emoji        \n",
    "# !pip install autocorrec\\t    \n",
    "# !pip install xgboost\n",
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e5a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')  # for using word_tokenizer\n",
    "# nltk.download('wordnet')  # for using Lemmatizer\n",
    "# nltk.download('averaged_perceptron_tagger') # for language processing i.e tagging words with their parts of speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e54a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import spacy\n",
    "import nltk\n",
    "import re   # regular expression\n",
    "import string\n",
    "import emoji\n",
    "from autocorrect import Speller   # for correcting spelling\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize    # for tokenizing string into words\n",
    "from nltk.stem import WordNetLemmatizer    # for lemmatizing words\n",
    "from nltk.tag import pos_tag # for tagging words with their parts of speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82ce6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e22a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d02e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest,chi2, mutual_info_classif,f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204dfd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a38aca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b53798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55fae504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/project_training_data_with_class_labels.csv',dtype=str,delimiter=',',quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2705797d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(808661, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73aae4ff-3b29-4156-9a7e-d6a07f824017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 808661 entries, 0 to 808660\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   ID               808661 non-null  object\n",
      " 1   Comments         808623 non-null  object\n",
      " 2   Parent Comments  808661 non-null  object\n",
      " 3    Class Labels    808661 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 24.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06e4d84d-5562-4392-9796-16d3048bbfd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                  0\n",
       "Comments           38\n",
       "Parent Comments     0\n",
       " Class Labels       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for null value\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b97a58a0-dc97-4726-84f5-24d24a8e4191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Comments', 'Parent Comments', ' Class Labels '], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "455b7529-d811-4836-9276-79dd25994b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Comments', 'Parent Comments', 'Class Labels'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.rename(columns={' Class Labels ' : 'Class Labels'},inplace=True) # changing col name\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7705e14-dab0-4d2c-87a2-69d2e674df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove data points whose class labels are not present\n",
    "# df_train.dropna(subset=['Comments'], inplace=True)      # then dropped that rows with no value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bde0f58-8aa9-4d9a-a2bc-b991db3c0aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= df_train.replace(to_replace = np.nan, value = '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1747ae66-dffd-4370-a36a-60834afb8c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                 0\n",
       "Comments           0\n",
       "Parent Comments    0\n",
       "Class Labels       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for null value\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad5b66c8-adf1-4b61-aaf0-e8409cfabac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "      <th>Text</th>\n",
       "      <th>Class Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ocxtitan</td>\n",
       "      <td>Central Illinois</td>\n",
       "      <td>Jesus; where do you live?</td>\n",
       "      <td>Jesus; where do you live? Central Illinois</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LeChuckly</td>\n",
       "      <td>To think - CNN used to be the acronym synonymo...</td>\n",
       "      <td>Even The CNN Staff Is Sick Of The Wall-To-Wall...</td>\n",
       "      <td>Even The CNN Staff Is Sick Of The Wall-To-Wall...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>throwitskrub8</td>\n",
       "      <td>But then again; you have to consider that all ...</td>\n",
       "      <td>agree to that part.It can also mean that gujra...</td>\n",
       "      <td>agree to that part.It can also mean that gujra...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresherthanyouuu</td>\n",
       "      <td>ughhhhh</td>\n",
       "      <td>If a guy told you he doesn't use social media ...</td>\n",
       "      <td>If a guy told you he doesn't use social media ...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_kushagra</td>\n",
       "      <td>I should've put the</td>\n",
       "      <td>No; it's just a programming bug. After all; th...</td>\n",
       "      <td>No; it's just a programming bug. After all; th...</td>\n",
       "      <td>sarcastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                           Comments  \\\n",
       "0          ocxtitan                                   Central Illinois   \n",
       "1         LeChuckly  To think - CNN used to be the acronym synonymo...   \n",
       "2     throwitskrub8  But then again; you have to consider that all ...   \n",
       "3  fresherthanyouuu                                            ughhhhh   \n",
       "4         _kushagra                                I should've put the   \n",
       "\n",
       "                                     Parent Comments  \\\n",
       "0                          Jesus; where do you live?   \n",
       "1  Even The CNN Staff Is Sick Of The Wall-To-Wall...   \n",
       "2  agree to that part.It can also mean that gujra...   \n",
       "3  If a guy told you he doesn't use social media ...   \n",
       "4  No; it's just a programming bug. After all; th...   \n",
       "\n",
       "                                                Text   Class Labels  \n",
       "0         Jesus; where do you live? Central Illinois  non-sarcastic  \n",
       "1  Even The CNN Staff Is Sick Of The Wall-To-Wall...  non-sarcastic  \n",
       "2  agree to that part.It can also mean that gujra...  non-sarcastic  \n",
       "3  If a guy told you he doesn't use social media ...  non-sarcastic  \n",
       "4  No; it's just a programming bug. After all; th...      sarcastic  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserting the column before Class Labels col.\n",
    "df_train.insert(loc = 3,\n",
    "        column = 'Text',\n",
    "        value = df_train['Parent Comments'] + \" \" +df_train['Comments'])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bda1c5a8-d7b1-4975-bcbe-1b59dfa42ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(808661, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "600c0908-55f1-4260-8a67-607cd1eed83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we are combining Parent Comment and Comment cols. into one cols.\n",
    "# df_train['Text'] = df_train['Parent Comments'] + \" \" +df_train['Comments']\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe69be2f-6031-4e86-8da7-6c1131466b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                 0\n",
       "Comments           0\n",
       "Parent Comments    0\n",
       "Text               0\n",
       "Class Labels       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for null value\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f06c0da3-a92c-4636-9ff0-d761a162e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we know that adding any thing with nan value gives nan. so in Text col having nan value we replace \n",
    "# # one time with comment col. value where value of parent comment col is nan and other with parent comment where\n",
    "# # comment col. is empty.\n",
    "\n",
    "# # fill the rows of Col. Text having no values with that of col. Parent Comments where Comment col. is empty\n",
    "# df_train['Text'][df_train['Comments'].isnull()] = df_train['Parent Comments'][df_train['Comments'].isnull()]\n",
    "\n",
    "# # fill the rows of Col. Text having no values with that of col. Comments where Parent Comment col. is empty\n",
    "# df_train['Text'][df_train['Parent Comments'].isnull()] = df_train['Comments'][df_train['Parent Comments'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f25f346-4d50-44c2-a230-f57df4745ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to check for null value\n",
    "# df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40700151-3c17-46dd-8544-fba8d3b2e821",
   "metadata": {},
   "source": [
    "Since here there is issue with the format of some rows. So we are manually correcting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b9149b2-3de9-45bc-9c92-e38474afafca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theyoungthaddeus</td>\n",
       "      <td>No one \"needs\" an assault foam dart blaster</td>\n",
       "      <td>Your son has to register those at the county j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just_an_asian_here</td>\n",
       "      <td>Cause all attractive women are uninteresting a...</td>\n",
       "      <td>Likely due to creative and interesting content.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foxprowl</td>\n",
       "      <td>Poser.</td>\n",
       "      <td>Jon Stewart is going to HBO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kd7rzv</td>\n",
       "      <td>Won't be long and Anet will start banning peop...</td>\n",
       "      <td>This post looks like bullshit market manipulat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ellefied</td>\n",
       "      <td>There goes my hope that Kubo does a Kojima as ...</td>\n",
       "      <td>Plus the Japanese typically do not talk shit w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                           Comments  \\\n",
       "0    theyoungthaddeus        No one \"needs\" an assault foam dart blaster   \n",
       "1  Just_an_asian_here  Cause all attractive women are uninteresting a...   \n",
       "2            Foxprowl                                             Poser.   \n",
       "3              kd7rzv  Won't be long and Anet will start banning peop...   \n",
       "4            Ellefied  There goes my hope that Kubo does a Kojima as ...   \n",
       "\n",
       "                                    Parent Comments   \n",
       "0  Your son has to register those at the county j...  \n",
       "1    Likely due to creative and interesting content.  \n",
       "2                        Jon Stewart is going to HBO  \n",
       "3  This post looks like bullshit market manipulat...  \n",
       "4  Plus the Japanese typically do not talk shit w...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/project_test_data.csv',dtype=str,delimiter=',',quoting=3)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b051283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202166, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44d9bd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                   0\n",
       "Comments            15\n",
       "Parent Comments      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for null value\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "540c0fd4-2122-431d-a1e9-60e063c92f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test= df_test.replace(to_replace = np.nan, value = '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "183955a2-8175-474d-857e-b818155eccd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                  0\n",
       "Comments            0\n",
       "Parent Comments     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for null value\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80f774ce-01d9-497f-960d-617046eeb630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Comments', 'Parent Comments '], dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d3a61bd-9873-4723-9791-b0e31ef3f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.rename(columns={'Parent Comments ': 'Parent Comments'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3306398f-bbb9-4ad9-bddc-9388ebaf06b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Comments', 'Parent Comments'], dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97354327-3255-4122-8785-bed9ae434166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theyoungthaddeus</td>\n",
       "      <td>No one \"needs\" an assault foam dart blaster</td>\n",
       "      <td>Your son has to register those at the county j...</td>\n",
       "      <td>Your son has to register those at the county j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just_an_asian_here</td>\n",
       "      <td>Cause all attractive women are uninteresting a...</td>\n",
       "      <td>Likely due to creative and interesting content.</td>\n",
       "      <td>Likely due to creative and interesting content...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foxprowl</td>\n",
       "      <td>Poser.</td>\n",
       "      <td>Jon Stewart is going to HBO</td>\n",
       "      <td>Jon Stewart is going to HBO Poser.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kd7rzv</td>\n",
       "      <td>Won't be long and Anet will start banning peop...</td>\n",
       "      <td>This post looks like bullshit market manipulat...</td>\n",
       "      <td>This post looks like bullshit market manipulat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ellefied</td>\n",
       "      <td>There goes my hope that Kubo does a Kojima as ...</td>\n",
       "      <td>Plus the Japanese typically do not talk shit w...</td>\n",
       "      <td>Plus the Japanese typically do not talk shit w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                           Comments  \\\n",
       "0    theyoungthaddeus        No one \"needs\" an assault foam dart blaster   \n",
       "1  Just_an_asian_here  Cause all attractive women are uninteresting a...   \n",
       "2            Foxprowl                                             Poser.   \n",
       "3              kd7rzv  Won't be long and Anet will start banning peop...   \n",
       "4            Ellefied  There goes my hope that Kubo does a Kojima as ...   \n",
       "\n",
       "                                     Parent Comments  \\\n",
       "0  Your son has to register those at the county j...   \n",
       "1    Likely due to creative and interesting content.   \n",
       "2                        Jon Stewart is going to HBO   \n",
       "3  This post looks like bullshit market manipulat...   \n",
       "4  Plus the Japanese typically do not talk shit w...   \n",
       "\n",
       "                                                Text  \n",
       "0  Your son has to register those at the county j...  \n",
       "1  Likely due to creative and interesting content...  \n",
       "2                 Jon Stewart is going to HBO Poser.  \n",
       "3  This post looks like bullshit market manipulat...  \n",
       "4  Plus the Japanese typically do not talk shit w...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are combining Parent Comment and Comment cols. into one cols.\n",
    "df_test['Text'] = df_test['Parent Comments'] + \" \" +df_test['Comments']\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "310e11f6-981b-4262-badc-e772918f5fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['Class Labels'].value_counts().plot(kind = 'bar',rot = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47d4c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# english Stopwords\n",
    "stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c3aef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used this dictionary for expanding conracted words. this is taken from Github\n",
    "CONTRACTIONS = {\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I'm'a\": \"I am about to\",\n",
    "    \"I'm'o\": \"I am going to\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"Whatcha\": \"What are you\",\n",
    "    \"amn't\": \"am not\",\n",
    "    \"ain't\": \"are not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"daren't\": \"dare not\",\n",
    "    \"daresn't\": \"dare not\",\n",
    "    \"dasn't\": \"dare not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"didn’t\": \"did not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"don’t\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"e'er\": \"ever\",\n",
    "    \"everyone's\": \"everyone is\",\n",
    "    \"finna\": \"fixing to\",\n",
    "    \"gimme\": \"give me\",\n",
    "    \"gon't\": \"go not\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"gotta\": \"got to\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he've\": \"he have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"how're\": \"how are\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"'tis\": \"it is\",\n",
    "    \"'twas\": \"it was\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"kinda\": \"kind of\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"luv\": \"love\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"may've\": \"may have\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"ne'er\": \"never\",\n",
    "    \"o'\": \"of\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"ol'\": \"old\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"o'er\": \"over\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shalln't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",\n",
    "    \"somebody's\": \"somebody is\",\n",
    "    \"someone's\": \"someone is\",\n",
    "    \"something's\": \"something is\",\n",
    "    \"sux\": \"sucks\",\n",
    "    \"that're\": \"that are\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"that'll\": \"that will\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"em\": \"them\",\n",
    "    \"there're\": \"there are\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"there'll\": \"there will\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"these're\": \"these are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"this's\": \"this is\",\n",
    "    \"those're\": \"those are\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wanna\": \"want to\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what'd\": \"what did\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"where're\": \"where are\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"which's\": \"which is\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'd've\": \"who would have\",\n",
    "    \"why're\": \"why are\",\n",
    "    \"why'd\": \"why did\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you'll've\": \"you shall have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\"\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "733d0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I created a dictionary for emoticons\n",
    "EMOTICONS = {\n",
    "    u\":‑)\":\"Happy\",\n",
    "    u\":-))\":\"Very Happy\",\n",
    "    u\":-)))\":\"Very very Happy\",\n",
    "    u\":)\":\"Happy\",\n",
    "    u\":))\":\"Very Happy\",\n",
    "    u\":)))\":\"Very very Happy\",\n",
    "    u\":-]\":\"Happy\",\n",
    "    u\":]\":\"Happy\",\n",
    "    u\":-3\":\"Happy\",\n",
    "    u\":3\":\"Happy\",\n",
    "    u\":->\":\"Happy\",\n",
    "    u\":>\":\"Happy\",\n",
    "    u\"8-)\":\"Happy\",\n",
    "    u\":o)\":\"Happy\",\n",
    "    u\":-}\":\"Happy\",\n",
    "    u\":}\":\"Happy\",\n",
    "    u\":-)\":\"Happy\",\n",
    "    u\":c)\":\"Happy\",\n",
    "    u\":^)\":\"Happy\",\n",
    "    u\"=]\":\"Happy\",\n",
    "    u\"=)\":\"Happy\",\n",
    "    u\":‑D\":\"Laughing\",\n",
    "    u\":D\":\"Laughing\",\n",
    "    u\"8‑D\":\"Laughing\",\n",
    "    u\"8D\":\"Laughing\",\n",
    "    u\"X‑D\":\"Laughing\",\n",
    "    u\"XD\":\"Laughing\",\n",
    "    u\"=D\":\"Laughing\",\n",
    "    u\"=3\":\"Laughing\",\n",
    "    u\"B^D\":\"Laughing\",\n",
    "    u\":-))\":\"Very happy\",\n",
    "    u\"<3\":\"love\",\n",
    "    u\":-(\":\"sad\",\n",
    "    u\":‑(\":\"sad\",\n",
    "    u\":(\":\"sad\",\n",
    "    u\":‑c\":\"sad\",\n",
    "    u\":c\":\"sad\",\n",
    "    u\":‑<\":\"sad\",\n",
    "    u\":<\":\"sad\",\n",
    "    u\":‑[\":\"sad\",\n",
    "    u\":[\":\"sad\",\n",
    "    u\":-||\":\"sad\",\n",
    "    u\">:[\":\"sad\",\n",
    "    u\":{\":\"sad\",\n",
    "    u\":@\":\"sad\",\n",
    "    u\">:(\":\"sad\",\n",
    "    u\":'‑(\":\"Crying\",\n",
    "    u\":'(\":\"Crying\",\n",
    "    u\":'‑)\":\"Tears of happiness\",\n",
    "    u\":')\":\"Tears of happiness\",\n",
    "    u\"D‑':\":\"sad\",\n",
    "    u\"D:<\":\"sad\",\n",
    "    u\"D:\":\"sad\",\n",
    "    u\"D8\":\"very sad\",\n",
    "    u\"D;\":\"very sad\",\n",
    "    u\"D=\":\"very sad\",\n",
    "    u\"DX\":\"very sad\",\n",
    "    u\":‑O\":\"Surprise\",\n",
    "    u\":O\":\"Surprise\",\n",
    "    u\":‑o\":\"Surprise\",\n",
    "    u\":o\":\"Surprise\",\n",
    "    u\":-0\":\"Sad\",\n",
    "    u\"8‑0\":\"Yawn\",\n",
    "    u\">:O\":\"Yawn\",\n",
    "    u\":-*\":\"Kiss\",\n",
    "    u\":*\":\"Kiss\",\n",
    "    u\":X\":\"Kiss\",\n",
    "    u\";‑)\":\"Wink\",\n",
    "    u\";)\":\"Wink\",\n",
    "    u\"*-)\":\"Wink\",\n",
    "    u\"*)\":\"Wink\",\n",
    "    u\";‑]\":\"Wink\",\n",
    "    u\";]\":\"Wink\",\n",
    "    u\";^)\":\"Wink\",\n",
    "    u\":‑,\":\"Wink\",\n",
    "    u\";D\":\"Wink\",\n",
    "    u\":‑P\":\"fun\",\n",
    "    u\":P\":\"fun\",\n",
    "    u\"X‑P\":\"fun\",\n",
    "    u\"XP\":\"fun\",\n",
    "    u\":‑Þ\":\"fun\",\n",
    "    u\":Þ\":\"fun\",\n",
    "    u\":b\":\"fun\",\n",
    "    u\"d:\":\"fun\",\n",
    "    u\"=p\":\"fun\",\n",
    "    u\">:P\":\"fun\",\n",
    "    u\":‑/\":\"annoyed\",\n",
    "    u\":/\":\"annoyed\",\n",
    "    u\":-[.]\":\"annoyed\",\n",
    "    u\">:[(\\)]\":\"annoyed\",\n",
    "    u\">:/\":\"annoyed\",\n",
    "    u\":[(\\)]\":\"annoyed\",\n",
    "    u\"=/\":\"annoyed\",\n",
    "    u\"=[(\\)]\":\"annoyed\",\n",
    "    u\":L\":\"annoyed\",\n",
    "    u\"=L\":\"annoyed\",\n",
    "    u\":S\":\"annoyed\",\n",
    "    u\":‑|\":\"indecision\",\n",
    "    u\":|\":\"indecision\",\n",
    "    u\":$\":\"Embarrassed\",\n",
    "    u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\"O:‑)\":\"Angel\",\n",
    "    u\"O:)\":\"Angel\",\n",
    "    u\"0:‑3\":\"Angel\",\n",
    "    u\"0:3\":\"Angel\",\n",
    "    u\"0:‑)\":\"Angel\",\n",
    "    u\"0:)\":\"Angel\",\n",
    "    u\":‑b\":\"fun\",\n",
    "    u\"0;^)\":\"Angel\",\n",
    "    u\">:‑)\":\"devilish\",\n",
    "    u\">:)\":\"devilish\",\n",
    "    u\"}:‑)\":\"devilish\",\n",
    "    u\"}:)\":\"devilish\",\n",
    "    u\"3:‑)\":\"devilish\",\n",
    "    u\"3:)\":\"devilish\",\n",
    "    u\">;)\":\"devilish\",\n",
    "    u\"|;‑)\":\"Cool\",\n",
    "    u\"|‑O\":\"Bored\",\n",
    "    u\":‑J\":\"Tongue in cheek\",\n",
    "    u\"#‑)\":\"Party all night\",\n",
    "    u\"%‑)\":\"confused\",\n",
    "    u\"%)\":\"confused\",\n",
    "    u\":-###..\":\"Being sick\",\n",
    "    u\":###..\":\"Being sick\",\n",
    "    u\"<:‑|\":\"silent\",\n",
    "    u\"(>_<)\":\"Troubled\",\n",
    "    u\"(>_<)>\":\"Troubled\",\n",
    "    u\"(';')\":\"Baby\",\n",
    "    u\"(^^>``\":\"Nervous\",\n",
    "    u\"(^_^;)\":\"Troubled\",\n",
    "    u\"(-_-;)\":\"Nervous\",\n",
    "    u\"(~_~;) (・.・;)\":\"Shy\",\n",
    "    u\"(-_-)zzz\":\"Sleeping\",\n",
    "    u\"(^_-)\":\"Wink\",\n",
    "    u\"((+_+))\":\"Confused\",\n",
    "    u\"(+o+)\":\"Confused\",\n",
    "    u\"(o|o)\":\"Ultraman\",\n",
    "    u\"^_^\":\"happy\",\n",
    "    u\"(^_^)/\":\"happy\",\n",
    "    u\"(^O^)／\":\"happy\",\n",
    "    u\"(^o^)／\":\"happy\",\n",
    "    u\"(__)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"_(._.)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<(_ _)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<m(__)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m(__)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m(_ _)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"('_')\":\"Sad\",\n",
    "    u\"(/_;)\":\"Sad\",\n",
    "    u\"(T_T) (;_;)\":\"Sad\",\n",
    "    u\"(;_;\":\"Sad\",\n",
    "    u\"(;_:)\":\"Sad\",\n",
    "    u\"(;O;)\":\"Sad\",\n",
    "    u\"(:_;)\":\"Sad\",\n",
    "    u\"(ToT)\":\"Sad\",\n",
    "    u\";_;\":\"Sad\",\n",
    "    u\";-;\":\"Sad\",\n",
    "    u\";n;\":\"Sad\",\n",
    "    u\";;\":\"Sad\",\n",
    "    u\"Q.Q\":\"Sad\",\n",
    "    u\"T.T\":\"Sad\",\n",
    "    u\"QQ\":\"Sad\",\n",
    "    u\"Q_Q\":\"Sad\",\n",
    "    u\"(-.-)\":\"Shame\",\n",
    "    u\"(-_-)\":\"Shame\",\n",
    "    u\"(一一)\":\"Shame\",\n",
    "    u\"(；一_一)\":\"Shame\",\n",
    "    u\"(=_=)\":\"Tired\",\n",
    "    u\"(=^·^=)\":\"cat\",\n",
    "    u\"(=^··^=)\":\"cat\",\n",
    "    u\"=_^= \":\"cat\",\n",
    "    u\"(..)\":\"Looking down\",\n",
    "    u\"(._.)\":\"Looking down\",\n",
    "    u\"^m^\":\"Giggling\",\n",
    "    u\"(・・?\":\"Confusion\",\n",
    "    u\"(?_?)\":\"Confusion\",\n",
    "    u\">^_^<\":\"Normal Laugh\",\n",
    "    u\"<^!^>\":\"Normal Laugh\",\n",
    "    u\"^/^\":\"Normal Laugh\",\n",
    "    u\"（*^_^*）\" :\"Normal Laugh\",\n",
    "    u\"(^<^) (^.^)\":\"Normal Laugh\",\n",
    "    u\"(^^)\":\"Normal Laugh\",\n",
    "    u\"(^.^)\":\"Normal Laugh\",\n",
    "    u\"(^_^.)\":\"Normal Laugh\",\n",
    "    u\"(^_^)\":\"Normal Laugh\",\n",
    "    u\"(^^)\":\"Normal Laugh\",\n",
    "    u\"(^J^)\":\"Normal Laugh\",\n",
    "    u\"(*^.^*)\":\"Normal Laugh\",\n",
    "    u\"(^—^）\":\"Normal Laugh\",\n",
    "    u\"(#^.^#)\":\"Normal Laugh\",\n",
    "    u\"（^—^）\":\"Waving\",\n",
    "    u\"(;_;)/~~~\":\"Waving\",\n",
    "    u\"(^.^)/~~~\":\"Waving\",\n",
    "    u\"(-_-)/~~~ ($··)/~~~\":\"Waving\",\n",
    "    u\"(T_T)/~~~\":\"Waving\",\n",
    "    u\"(ToT)/~~~\":\"Waving\",\n",
    "    u\"(*^0^*)\":\"Excited\",\n",
    "    u\"(*_*)\":\"Excited\",\n",
    "    u\"(*_*;\":\"Excited\",\n",
    "    u\"(+_+) (@_@)\":\"Excited\",\n",
    "    u\"(*^^)v\":\"Cheerful\",\n",
    "    u\"(^_^)v\":\"Cheerful\",\n",
    "    u\"((d[-_-]b))\":\"Headphones,Listening to music\",\n",
    "    u'(-\"-)':\"Worried\",\n",
    "    u\"(ーー;)\":\"Worried\",\n",
    "    u\"(^0_0^)\":\"Eyeglasses\",\n",
    "    u\"(＾ｖ＾)\":\"Happy\",\n",
    "    u\"(＾ｕ＾)\":\"Happy\",\n",
    "    u\"(^)o(^)\":\"Happy\",\n",
    "    u\"(^O^)\":\"Happy\",\n",
    "    u\"(^o^)\":\"Happy\",\n",
    "    u\")^o^(\":\"Happy\",\n",
    "    u\":O o_O\":\"Surprised\",\n",
    "    u\"o_0\":\"Surprised\",\n",
    "    u\"o.O\":\"Surpised\",\n",
    "    u\"(o.o)\":\"Surprised\",\n",
    "    u\"oO\":\"Surprised\",\n",
    "    u\"(*￣m￣)\":\"Dissatisfied\",\n",
    "    u\"(‘A`)\":\"Deflated\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9e0b3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(tweet):\n",
    "    nan_tweet = 'NaN'  \n",
    "    # this code is to short unnecessary sentence, bec. some rows has unnecessary long repeated characters\n",
    "    # like 'HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH'\n",
    "    # we manually decide len = 10 any with len >10 is discarded\n",
    "        # convert all text lowercase\n",
    "    tweet = tweet.lower() \n",
    "    tweet = tweet.split()\n",
    "    tw = []\n",
    "    for t in tweet:\n",
    "        # removing digits only\n",
    "        try:\n",
    "            int(t)\n",
    "            continue\n",
    "        except:\n",
    "            pass\n",
    "        if len(t)<=20:\n",
    "            if len(set(t))<=1:\n",
    "                continue\n",
    "            tw.append(t)\n",
    "    tweet = ' '.join(tw)   \n",
    "    # remove any urls\n",
    "    tweet = re.sub(r\"www\\S+|http\\S+|\", \"\",tweet, flags = re.MULTILINE)\n",
    "    # remove square bracket including its content if\n",
    "    tweet = re.sub(r'\\[|\\]',\" \",tweet)\n",
    "    # to remove new line character\n",
    "    tweet = re.sub(r'\\n', \" \", tweet)\n",
    "    # remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r\"\\@\\w+|\\#\", \"\",tweet)\n",
    "    # replace emojis with its meaning\n",
    "    tweet = (emoji.demojize(tweet, delimiters=(\" \", \"\"))).replace('_',' ')\n",
    "    # expand contractions\n",
    "    splitted_string = tweet.split()\n",
    "    for index, text in enumerate(splitted_string):\n",
    "        if text in CONTRACTIONS.keys():\n",
    "            splitted_string[index] = CONTRACTIONS[text]\n",
    "    tweet = ' '.join(splitted_string)\n",
    "    # replace emoticons with its meaning\n",
    "    splitted_tweet = tweet.split()\n",
    "    for index, word in enumerate(splitted_tweet):\n",
    "        if word in EMOTICONS.keys():\n",
    "            splitted_tweet[index] = EMOTICONS[word]\n",
    "    tweet = ' '.join(splitted_tweet)\n",
    "    # remove tags\n",
    "    tweet = re.sub(re.compile('<.*?>'), '', tweet)\n",
    "    # remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    # remove stopwords\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [word for word in tweet_tokens if word not in stopwords_list]\n",
    "    # spelling correction\n",
    "    correct_words = []\n",
    "    # initialize Speller object for english language\n",
    "    spell_corrector = Speller(lang='en')\n",
    "    for word in filtered_words:\n",
    "        correct_word = spell_corrector(word)\n",
    "        correct_words.append(correct_word)\n",
    "    # lemmatizing\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemma_words = []\n",
    "    for word, tag in pos_tag(correct_words):      # Part-of-speech constants for ADJ,VERB,ADV = 'a', 'r', 'v'\n",
    "        if tag.startswith('JJ'):      # for adjectives\n",
    "            lemma_word = wnl.lemmatize(word, pos='a')\n",
    "            lemma_words.append(lemma_word)\n",
    "        elif tag.startswith('VB'):   # for verbs\n",
    "            lemma_word = wnl.lemmatize(word, pos='v')\n",
    "            lemma_words.append(lemma_word)\n",
    "        elif tag.startswith('RB'):   # for adverbs\n",
    "            lemma_word = wnl.lemmatize(word, pos='r')\n",
    "            lemma_words.append(lemma_word)\n",
    "        else:\n",
    "            lemma_word = word\n",
    "            lemma_words.append(lemma_word)\n",
    "        \n",
    "    tweet = \" \".join(lemma_words)\n",
    "    if len(tweet) == 0:   # if after pre-processing sent. has no letter\n",
    "        tweet = nan_tweet\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025aae1-4f31-46e8-91f8-d50204d95aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d9bbc2-95d0-45b4-809e-19f55df2293b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ee69e889-046e-4256-a873-a10b4d7ae38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaaaaaaaawesome'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = 'aaaaaaaaawesome'\n",
    "preprocess_text(text)\n",
    "# len('HAHAHAHAHAHgoodaaaaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on train data\n",
    "# applying preprocess_tweet_text() func. on each tweet in Comments and Parent Comments coloumn in dataframe\n",
    "# df_train['Comments'] = df_train['Comments'].apply(preprocess_tweet_text)\n",
    "# df_train['Parent Comments'] = df_train['Parent Comments'].apply(preprocess_tweet_text)\n",
    "# df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "294ff223-35c5-474b-8116-d8d811757a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 64 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# 24000:250000 error somewhere\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True,nb_workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b45ff584-3db6-4f3c-a135-18cfe85f5c77",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045f4766b676427c980f1aaf5144da00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=12636), Label(value='0 / 12636')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2677.4445066452026 sec\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "xx = df_train['Text'].parallel_apply(preprocess_text) \n",
    "df_train.insert(loc = 4,\n",
    "        column = 'Pre Processed Text',\n",
    "        value = xx)\n",
    "# saving\n",
    "df_train.to_csv('processed_train.csv',index=False) # on train data, \n",
    "tt = time.time() - t\n",
    "print(f'{tt} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d6eb80e-37f9-4828-9789-964919857a82",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710c0db3d936402c9f2b328dc2522a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=3159), Label(value='0 / 3159'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725.7949583530426 sec\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "xtt = df_test['Text'].parallel_apply(preprocess_text) # on test data\n",
    "df_test.insert(loc = 4,\n",
    "        column = 'Pre Processed Text',\n",
    "        value = xtt)\n",
    "df_test.to_csv('processed_test.csv',index=False)\n",
    "tt = time.time() - t\n",
    "print(f'{tt} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf1af5-6d72-4b48-970c-be78d5fa5d6e",
   "metadata": {},
   "source": [
    "# Reading the pre-processed csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "187c8f51-cbc9-4e9e-aa0a-2bdbf7351295",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro_train = pd.read_csv('processed_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14f1bd31-af72-4b98-a42c-4bc17027e221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808661, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pre Processed Text</th>\n",
       "      <th>Class Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ocxtitan</td>\n",
       "      <td>Central Illinois</td>\n",
       "      <td>Jesus; where do you live?</td>\n",
       "      <td>Jesus; where do you live? Central Illinois</td>\n",
       "      <td>jesus live central illinois</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LeChuckly</td>\n",
       "      <td>To think - CNN used to be the acronym synonymo...</td>\n",
       "      <td>Even The CNN Staff Is Sick Of The Wall-To-Wall...</td>\n",
       "      <td>Even The CNN Staff Is Sick Of The Wall-To-Wall...</td>\n",
       "      <td>even cnn staff sick walltowall trump coverage ...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>throwitskrub8</td>\n",
       "      <td>But then again; you have to consider that all ...</td>\n",
       "      <td>agree to that part.It can also mean that gujra...</td>\n",
       "      <td>agree to that part.It can also mean that gujra...</td>\n",
       "      <td>agree parti also mean gujarat husbands good su...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresherthanyouuu</td>\n",
       "      <td>ughhhhh</td>\n",
       "      <td>If a guy told you he doesn't use social media ...</td>\n",
       "      <td>If a guy told you he doesn't use social media ...</td>\n",
       "      <td>guy tell use social media go head ughhhhh</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_kushagra</td>\n",
       "      <td>I should've put the</td>\n",
       "      <td>No; it's just a programming bug. After all; th...</td>\n",
       "      <td>No; it's just a programming bug. After all; th...</td>\n",
       "      <td>program bug android app music service still be...</td>\n",
       "      <td>sarcastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                           Comments  \\\n",
       "0          ocxtitan                                   Central Illinois   \n",
       "1         LeChuckly  To think - CNN used to be the acronym synonymo...   \n",
       "2     throwitskrub8  But then again; you have to consider that all ...   \n",
       "3  fresherthanyouuu                                            ughhhhh   \n",
       "4         _kushagra                                I should've put the   \n",
       "\n",
       "                                     Parent Comments  \\\n",
       "0                          Jesus; where do you live?   \n",
       "1  Even The CNN Staff Is Sick Of The Wall-To-Wall...   \n",
       "2  agree to that part.It can also mean that gujra...   \n",
       "3  If a guy told you he doesn't use social media ...   \n",
       "4  No; it's just a programming bug. After all; th...   \n",
       "\n",
       "                                                Text  \\\n",
       "0         Jesus; where do you live? Central Illinois   \n",
       "1  Even The CNN Staff Is Sick Of The Wall-To-Wall...   \n",
       "2  agree to that part.It can also mean that gujra...   \n",
       "3  If a guy told you he doesn't use social media ...   \n",
       "4  No; it's just a programming bug. After all; th...   \n",
       "\n",
       "                                  Pre Processed Text   Class Labels  \n",
       "0                        jesus live central illinois  non-sarcastic  \n",
       "1  even cnn staff sick walltowall trump coverage ...  non-sarcastic  \n",
       "2  agree parti also mean gujarat husbands good su...  non-sarcastic  \n",
       "3          guy tell use social media go head ughhhhh  non-sarcastic  \n",
       "4  program bug android app music service still be...      sarcastic  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_prepro_train.shape)\n",
    "df_prepro_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e410647c-a815-4cd3-9c87-aead0497a215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                     0\n",
       "Comments              38\n",
       "Parent Comments        0\n",
       "Text                   0\n",
       "Pre Processed Text     0\n",
       "Class Labels           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c725b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202166, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pre Processed Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theyoungthaddeus</td>\n",
       "      <td>No one \"needs\" an assault foam dart blaster</td>\n",
       "      <td>Your son has to register those at the county j...</td>\n",
       "      <td>Your son has to register those at the county j...</td>\n",
       "      <td>son register county jungle gym mags hold ounce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just_an_asian_here</td>\n",
       "      <td>Cause all attractive women are uninteresting a...</td>\n",
       "      <td>Likely due to creative and interesting content.</td>\n",
       "      <td>Likely due to creative and interesting content...</td>\n",
       "      <td>likely due creative interesting content cause ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foxprowl</td>\n",
       "      <td>Poser.</td>\n",
       "      <td>Jon Stewart is going to HBO</td>\n",
       "      <td>Jon Stewart is going to HBO Poser.</td>\n",
       "      <td>jon stewart go hbo power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kd7rzv</td>\n",
       "      <td>Won't be long and Anet will start banning peop...</td>\n",
       "      <td>This post looks like bullshit market manipulat...</td>\n",
       "      <td>This post looks like bullshit market manipulat...</td>\n",
       "      <td>post looks like bullshit market manipulation w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ellefied</td>\n",
       "      <td>There goes my hope that Kubo does a Kojima as ...</td>\n",
       "      <td>Plus the Japanese typically do not talk shit w...</td>\n",
       "      <td>Plus the Japanese typically do not talk shit w...</td>\n",
       "      <td>plus japanese typically talk shit come busines...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                           Comments  \\\n",
       "0    theyoungthaddeus        No one \"needs\" an assault foam dart blaster   \n",
       "1  Just_an_asian_here  Cause all attractive women are uninteresting a...   \n",
       "2            Foxprowl                                             Poser.   \n",
       "3              kd7rzv  Won't be long and Anet will start banning peop...   \n",
       "4            Ellefied  There goes my hope that Kubo does a Kojima as ...   \n",
       "\n",
       "                                     Parent Comments  \\\n",
       "0  Your son has to register those at the county j...   \n",
       "1    Likely due to creative and interesting content.   \n",
       "2                        Jon Stewart is going to HBO   \n",
       "3  This post looks like bullshit market manipulat...   \n",
       "4  Plus the Japanese typically do not talk shit w...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Your son has to register those at the county j...   \n",
       "1  Likely due to creative and interesting content...   \n",
       "2                 Jon Stewart is going to HBO Poser.   \n",
       "3  This post looks like bullshit market manipulat...   \n",
       "4  Plus the Japanese typically do not talk shit w...   \n",
       "\n",
       "                                  Pre Processed Text  \n",
       "0  son register county jungle gym mags hold ounce...  \n",
       "1  likely due creative interesting content cause ...  \n",
       "2                           jon stewart go hbo power  \n",
       "3  post looks like bullshit market manipulation w...  \n",
       "4  plus japanese typically talk shit come busines...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "df_prepro_test = pd.read_csv('processed_test.csv')\n",
    "print(df_prepro_test.shape)\n",
    "df_prepro_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e2e255-b4d9-4573-a536-9f7892d3ea98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                     0\n",
       "Comments              15\n",
       "Parent Comments        0\n",
       "Text                   0\n",
       "Pre Processed Text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35269736-9965-405f-9279-49a911a7802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tst.isna().sum() # problem: If the sentence is madeup of only stopwords, numbers etc., the sentence after preprocess becomes empty results in NaN\n",
    "# # fix, after preprocess check sentence length if its 0, return the orig sentence without preprocess. else preprocess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53f12d50-7b40-4b04-9560-4dfb92674f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df[['Comments','Parent Comments']][df['Comments'].isnull()]\n",
    "# X_tst[X_tst['Text'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd6dff77-ac97-4700-b15f-f4b9c92709ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "X = df_prepro_train['Pre Processed Text']\n",
    "y_trn = df_prepro_train['Class Labels']\n",
    "\n",
    "# Test data\n",
    "X_test = df_prepro_test['Pre Processed Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c68e5bf0-e447-494f-ba66-0c9a9f0c4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting Labels coloumn into numerical form of train data\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1bb816b-18c9-481a-937e-5abb848f1373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['non-sarcastic', 'sarcastic'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43446706-769f-4c91-bee1-c7fc710c68a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "156f8075-57cb-47f8-84b7-7e955f20256c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((646928,), (161733,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train validation split of data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=3,stratify=y)\n",
    "X_train.shape,X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c6da2-aeff-4372-bddf-d57d82f06cc7",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35f7c8a4-c795-444d-8f52-823c3fbf677d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H A H A H A H A H A H A '"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "ans = \"\"\n",
    "for a in re.findall('[A-Z][^A-Z]*',\"HAHAHAHAHA HA\"):\n",
    "   ans+=a.strip()+' '\n",
    "\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9f3785f4-6a8f-4ecc-8180-ee6fe794922b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BWAH'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getCounts(strng):\n",
    "    if not strng:\n",
    "        return [], 0\n",
    "    counts = {}\n",
    "    current = strng[0]\n",
    "    for c in strng:\n",
    "        if c in counts.keys():\n",
    "            if current==c:\n",
    "                counts[c] += 1\n",
    "        else:\n",
    "            current = c\n",
    "            counts[c] = 1\n",
    "    return counts.keys(), min(counts.values())\n",
    "\n",
    "result = ''\n",
    "counts=getCounts('BWAHAHAHAHAHAHAHAHAHHHAAHAHAHAHAHHAHAHAHAHAHAHHAHAAHAHAHAHAHAH')\n",
    "for i in range(counts[1]):\n",
    "    result += ''.join(counts[0])\n",
    "result\n",
    "\n",
    "# result = 'aprilaprilapril'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "63733dc1-3a79-4175-ae00-0cafd72da720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gooood'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = nltk.stem.Cistem()\n",
    "s.stem('gooood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5af8beb2-50a3-41f0-ba42-3481c9a12dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "word = 'HAB'\n",
    "text = 'BWAHAHAHAHAHAHAHAHAHHHAAHAHAHAHAHHAHAHAHAHAHAHHAHAAHAHAHAHAHAH'\n",
    "\n",
    "regex = \"\".join(f\"({c}+)\" for c in word)\n",
    "match = re.match(regex, text)\n",
    "# print(match)\n",
    "\n",
    "if match:\n",
    "    # Find the lowest amount of character repeats\n",
    "    lowest_amount = min(len(g) for g in match.groups())\n",
    "    print(word )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3833341d-b0e7-459b-ba18-c5d039e4c7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0017#L@b2Y-b\u0003\u0004@w4\u001a-F)0@c:]\u0004\u0006k\u0014@s7[,R\u0003MXa1Ze\u0001a\u0001Du  [D\u0006\u000b",
      "\u0011le9\u001d",
      "\n"
     ]
    }
   ],
   "source": [
    "def BinaryToDecimal(binary):\n",
    "        \n",
    "    binary1 = binary\n",
    "    decimal, i, n = 0, 0, 0\n",
    "    while(binary != 0):\n",
    "        dec = binary % 10\n",
    "        decimal = decimal + dec * pow(2, i)\n",
    "        binary = binary//10\n",
    "        i += 1\n",
    "    return (decimal)   \n",
    "\n",
    "str_data =' '\n",
    "bin_data ='0100000101101110001000000110000101100100011101100110010101110010011101000110100101110011011001010110110101100101011011100111010000111111001000000010000001010011011011110110111000100000011011110110011000100000011000010010000001100010011010010111010001100011011010000010000000101000010010010010000001110000011100100110111101100010011000010110110001111001001000000110001001110101011101000110001101101000011001010111001001100101011001000010000001110100011010000110000101110100001011000010000001100010011101010111010000100000011010010111010001110011001000000110001001100101011001010110111000100000011000010010000001110111011010000110100101101100011001010010110000100000011000110111010101110100001000000110110101100101001000000111001101101111011011010110010100100000011100110110110001100001011000110110101100101001'\n",
    "# slicing the input and converting it\n",
    "# in decimal and then converting it in string\n",
    "for i in range(0, len(bin_data), 7):\n",
    "     \n",
    "    # slicing the bin_data from index range [0, 6]\n",
    "    # and storing it as integer in temp_data\n",
    "    temp_data = int(bin_data[i:i + 7])\n",
    "      \n",
    "    # passing temp_data in BinarytoDecimal() function\n",
    "    # to get decimal value of corresponding temp_data\n",
    "    decimal_data = BinaryToDecimal(temp_data)\n",
    "      \n",
    "    # Decoding the decimal value returned by\n",
    "    # BinarytoDecimal() function, using chr()\n",
    "    # function which return the string corresponding\n",
    "    # character for given ASCII value, and store it\n",
    "    # in str_data\n",
    "    str_data = str_data + chr(decimal_data)\n",
    "  \n",
    "# printing the result\n",
    "print(\"The Binary value after string conversion is:\",\n",
    "       str_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "22e1126f-9ff3-496a-b990-b62a6828697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 = np.hstack(X.values) # since Count vec takes only list of items\n",
    "# X_tst1 = np.hstack(X_tst.values)\n",
    "CV = CountVectorizer(ngram_range=(1,1),max_features=100000)\n",
    "X_train_bow = sp.csr_matrix(CV.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ccb1022-2886-4194-bdc7-a0870049c865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0000', '00000011', '00001001', '00001011', '0001', '00010001',\n",
       "       '00010101', '00011001', '00011011', '00011101', '00100000',\n",
       "       '00100011', '00101001', '00101100', '00101110', '0011', '00110001',\n",
       "       '00110011', '00110101', '00111001', '0023', '0030', '0100',\n",
       "       '01000001',\n",
       "       '0100000101101110001000000110000101100100011101100110010101110010011101000110100101110011011001010110110101100101011011100111010000111111001000000010000001010011011011110110111000100000011011110110011000100000011000010010000001100010011010010111010001100011011010000010000000101000010010010010000001110000011100100110111101100010011000010110110001111001001000000110001001110101011101000110001101101000011001010111001001100101011001000010000001110100011010000110000101110100001011000010000001100010011101010111010000100000011010010111010001110011001000000110001001100101011001010110111000100000011000010010000001110111011010000110100101101100011001010010110000100000011000110111010101110100001000000110110101100101001000000111001101101111011011010110010100100000011100110110110001100001011000110110101100101001',\n",
       "       '010000100110010100100000011100110111010101110010011001',\n",
       "       '01000100', '01000101', '01001000',\n",
       "       '010010000001110100011011110010000001100100011100100110',\n",
       "       '01001001', '01001101', '0101', '01010001', '01010010', '01010011',\n",
       "       '01010100', '01010101', '01011001', '0110', '01100001', '01100010',\n",
       "       '01100011', '01100100', '01100101', '01100110', '01100111',\n",
       "       '01101000', '01101001', '01101001011011100110010100101110',\n",
       "       '01101011', '01101101', '01101110', '01101111', '0111', '01110001',\n",
       "       '01110010', '01110011', '01110100', '01110101', '01111001',\n",
       "       '01111011', '01111101', '031', '04', '041', '05', '052', '06',\n",
       "       '068', '094', '10', '100', '1000', '10001001', '10001011',\n",
       "       '10010001',\n",
       "       '100101101110011010110010000001111001011011110111010101',\n",
       "       '10011001', '1002301', '1002302', '1010', '10100001', '10100011',\n",
       "       '10101001', '10101111', '1011', '10110001', '10110011', '10110101',\n",
       "       '10111001', '10111001101111e', '1013', '1014', '1031136',\n",
       "       '1052363', '1068126', '1071810', '1071811', '11', '110',\n",
       "       '110010001000000100111101110110011000010110110001110100',\n",
       "       '11001001', '11001011', '11001101', '11010001', '11010011',\n",
       "       '11011001', '11011101', '111', '1110', '11100001', '11101001',\n",
       "       '11101101', '1111', '11110001', '11111011', '11111111', '112',\n",
       "       '1153045', '1153046', '1187464', '1187465', '1192424', '12', '121',\n",
       "       '1238976', '1238977', '125', '1289', '13', '1300188', '1300189',\n",
       "       '1337', '1342525', '1342526', '1359858', '1359859', '137',\n",
       "       '1383309', '1396817', '1396818', '1398922', '1398923', '14',\n",
       "       '14159', '14160', '1440p', '14614', '14615', '1478', '1488510',\n",
       "       '1488511', '15', '1508658', '1508659', '1512797', '1512798',\n",
       "       '1520', '1526104', '1526105', '1555747', '1555748', '157',\n",
       "       '1571048', '1571049', '158', '1584204', '1584205', '159', '16',\n",
       "       '164364', '164365', '17', '1733', '174016', '174017', '1779',\n",
       "       '1790s', '18', '19', '190', '192', '199203', '199204', '20', '200',\n",
       "       '2002', '2010', '2011', '2016', '203', '204', '20999999999999999',\n",
       "       '21', '2119', '216', '22', '2206', '221', '224399', '2244', '2279',\n",
       "       '2311', '2312', '2319', '2334', '2373', '2374', '24', '246',\n",
       "       '2500bc', '255', '256', '261', '271', '28', '281', '282', '288',\n",
       "       '29', '294', '2c3', '2complicated4me', '2deepthroat4me', '2edgy',\n",
       "       '2edgy4gramgram', '2holes4dicks', '2k16', '2man4ji', '2manji4me',\n",
       "       '2spooky', '2spoopy4me', '30', '301971', '301972', '303', '308734',\n",
       "       '308735', '31', '310', '315549', '315550', '31633613', '31644613',\n",
       "       '31k', '326439', '326440', '330', '332628', '332629', '33k', '34',\n",
       "       '340', '3400', '341', '344758', '344759', '348', '350', '36',\n",
       "       '360996', '360997', '362', '37', '375', '379510', '379511', '383',\n",
       "       '396', '397', '3edgy4me', '3edgy5me', '3spook5me', '3spooky5me',\n",
       "       '3verendingfag4', '40', '400', '414', '42', '423', '425', '426585',\n",
       "       '426586', '428281', '428282', '43', '430471', '430472', '431302',\n",
       "       '431303', '439245', '439246', '44', '441899', '441900', '45',\n",
       "       '450', '4500', '477', '48', '49', '496757', '496758', '4u', '50',\n",
       "       '500868', '500869', '505', '506', '506729', '51', '512', '515284',\n",
       "       '515285', '528794', '5386', '5387', '54', '57', '575704', '575705',\n",
       "       '58', '59', '593', '593281', '593413', '5ghz', '62', '620',\n",
       "       '620112', '639', '64', '679', '68', '683', '683680', '685', '69',\n",
       "       '691663', '691664', '691739', '691740', '6969420', '709', '712',\n",
       "       '72', '726', '726042', '729', '73', '730', '735653', '735654',\n",
       "       '750841', '750842', '75k', '76ers', '77', '7777', '78', '785',\n",
       "       '786', '786911', '786912', '79', '7k', '8004', '800594', '800595',\n",
       "       '80085', '803037', '803038', '812203', '812204', '815360',\n",
       "       '815361', '824', '825', '832', '832844', '834910', '834911', '835',\n",
       "       '835710', '835711', '836', '841552', '841553', '843', '871292',\n",
       "       '871293', '88m', '88n', '89', '8th', '91', '92', '93', '933',\n",
       "       '934', '934341', '934342', '9428', '9429', '95', '96', '99', '9k',\n",
       "       '9th', '___', '_______________________________________________',\n",
       "       'aa', 'aaa', 'aaaa', 'aaaaaa', 'aaaaaaaaa', 'aaaaaaaaaa',\n",
       "       'aaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaa',\n",
       "       'aaaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaa',\n",
       "       'aaaaaaaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       "       'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       "       'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaayyye',\n",
       "       'aaaaaaaaaaaaaaaand', 'aaaaaaaaaaaaaaand', 'aaaaaaaaaaaagh',\n",
       "       'aaaaaaaaaaaahhhh', 'aaaaaaaaaaaand', 'aaaaaaaaaaaargh',\n",
       "       'aaaaaaaaaaand', 'aaaaaaaaaah', 'aaaaaaaaaand', 'aaaaaaaaaanndd',\n",
       "       'aaaaaaaaaannnnddd', 'aaaaaaaaah', 'aaaaaaaaahhhhhhhh',\n",
       "       'aaaaaaaaand', 'aaaaaaaaarmmmss', 'aaaaaaaaarrrreeeee',\n",
       "       'aaaaaaaah', 'aaaaaaaahhh', 'aaaaaaaahhhhhhhh', 'aaaaaaaand',\n",
       "       'aaaaaaaany', 'aaaaaaabbeee', 'aaaaaaagh', 'aaaaaaall',\n",
       "       'aaaaaaalll', 'aaaaaaand', 'aaaaaaannnnd', 'aaaaaaany',\n",
       "       'aaaaaaaroooooooo', 'aaaaaahhhhh', 'aaaaaahhhhhhh',\n",
       "       'aaaaaahleluya', 'aaaaaallll', 'aaaaaammmmmm', 'aaaaaanglish',\n",
       "       'aaaaaangry', 'aaaaaannnd', 'aaaaaannnnnnnndddd',\n",
       "       'aaaaagggggghgggghgg', 'aaaaahhh', 'aaaaahhhhh',\n",
       "       'aaaaahhhhhhhahaha', 'aaaaahhhhhhhh', 'aaaaahhhhhhhhhh',\n",
       "       'aaaaahleluya', 'aaaaaint', 'aaaaallll', 'aaaaanglish',\n",
       "       'aaaaannnd', 'aaaaannnnd', 'aaaaannnndd', 'aaaaannnnnd',\n",
       "       'aaaaaoouch', 'aaaaarms', 'aaaaawesome', 'aaaafricaaaa',\n",
       "       'aaaagain', 'aaaaggghhhh', 'aaaahahaha', 'aaaahhh', 'aaaahhhh',\n",
       "       'aaaahhhhhhahahaa', 'aaaalll', 'aaaandim', 'aaaanditsgonejpg',\n",
       "       'aaaandteq', 'aaaanndd', 'aaaannnnnd', 'aaaannnnnnnddddd',\n",
       "       'aaaanythiiiing', 'aaaanyway', 'aaaarrgh', 'aaaarrreee',\n",
       "       'aaaarrrr', 'aaaaugh', 'aaaauuu', 'aaaawww', 'aaabudget', 'aaabuy',\n",
       "       'aaagghhh', 'aaaghhhh', 'aaahahahahahahaha', 'aaahhhh',\n",
       "       'aaahhhhhh', 'aaahhhhhhhhhh', 'aaahthe', 'aaanndd', 'aaannnd',\n",
       "       'aaannndd', 'aaannnddd', 'aaannnnnd', 'aaaooone', 'aaargh',\n",
       "       'aaarghgggh', 'aaasssss', 'aaatitle', 'aaatrox', 'aaayyyy', 'aab',\n",
       "       'aabdelkader', 'aac', 'aacarboner', 'aachen', 'aad', 'aadhaar',\n",
       "       'aadskfjlklk', 'aae', 'aaf', 'aah', 'aahahahaha', 'aahahhaa',\n",
       "       'aahhaha', 'aahhhaa', 'aahhhh', 'aahhhhhhhhhhhhhh', 'aai',\n",
       "       'aairpass', 'aaj', 'aakhir', 'aaliyah', 'aam', 'aamaadmi',\n",
       "       'aampampm', 'aamppairframe', 'aamuja', 'aan', 'aandacht',\n",
       "       'aandelenbeloning', 'aanewale', 'aangehouden', 'aangekomen',\n",
       "       'aangenomen', 'aangepast', 'aangereden', 'aangesproken',\n",
       "       'aangestoken', 'aangevallen', 'aangezien', 'aangggggellll',\n",
       "       'aanholt', 'aanhouden', 'aankaarten', 'aankomen', 'aankunnen',\n",
       "       'aanleiding', 'aannndd', 'aannnnyyyy', 'aanpraten', 'aanschuiven',\n",
       "       'aansluiting', 'aanspreken', 'aanstaan', 'aanstoot', 'aantekenen',\n",
       "       'aantisemite', 'aantonen', 'aanvallen', 'aanvraagformulier',\n",
       "       'aanwezigheid', 'aanwijzingen', 'aanzienlijk', 'aap', 'aapcares',\n",
       "       'aaplogic', 'aaptards', 'aaptroll', 'aar', 'aardappel', 'aardvark',\n",
       "       'aaron', 'aarrrrgghhhhh', 'aas', 'aasmaan', 'aasphew', 'aat',\n",
       "       'aayatallah', 'aayega', 'aayiyo', 'aayyyyyyy', 'aazami', 'ab',\n",
       "       'aba', 'abac', 'abadango', 'abajothat', 'abandon', 'abandoned',\n",
       "       'abandoning', 'abandonment', 'abandons', 'abandonware',\n",
       "       'abastanza', 'abattoir', 'abaxoth', 'abb', 'abbas', 'abbasids',\n",
       "       'abbassare', 'abbastanza', 'abbatoir', 'abbattuto', 'abbedines',\n",
       "       'abbess', 'abbey', 'abbeys', 'abbiamo', 'abbiano', 'abbiocco',\n",
       "       'abbkuhdee', 'abbot', 'abbots', 'abbotsford', 'abbott',\n",
       "       'abbottabad', 'abbottbux', 'abbr', 'abbredris', 'abbrev',\n",
       "       'abbreviate', 'abbreviated', 'abbreviation', 'abbreviations',\n",
       "       'abc', 'abcdef', 'abcdefucku', 'abcgnulinux', 'abcnews',\n",
       "       'abctntespn', 'abd', 'abdelaziz', 'abdelkader', 'abdelkater',\n",
       "       'abdelnasser', 'abdesalam', 'abdeslam', 'abdeslams', 'abdicate',\n",
       "       'abdication', 'abdomen', 'abdomenthe', 'abdomentheres',\n",
       "       'abdominal', 'abduct', 'abducted', 'abduction', 'abdul',\n",
       "       'abdulaziz', 'abduljabaar', 'abduljabbar', 'abdullah',\n",
       "       'abdulqaadir', 'abdulrauf', 'abdulrazzak', 'abe', 'abeardedhaxor',\n",
       "       'abedinclinton', 'abeemination', 'abel', 'abelian', 'abendland',\n",
       "       'abendlandes', 'abendrot', 'abengoa', 'aber', 'aberaberdann',\n",
       "       'abercrombie', 'aberdeen', 'aberdeenshire', 'aberdonians',\n",
       "       'aberfoyle', 'aberlour', 'abernathy', 'abernathys',\n",
       "       'aberrantcheese', 'aberration', 'aberrosexual', 'abeyaar', 'abf',\n",
       "       'abgeben', 'abgerutscht', 'abgeschafft', 'abhinav', 'abhorash',\n",
       "       'abhorrables', 'abhorrence', 'abhorrent', 'abi', 'abiatti',\n",
       "       'abide', 'abiding', 'abigail', 'abilene', 'abilenenot',\n",
       "       'abilities', 'abilitiesautos', 'abilitieslike', 'abilitiessss',\n",
       "       'ability', 'abilitybased', 'abilityheavy', 'abilityhow',\n",
       "       'abilityitem', 'abilityless', 'abilitymight', 'abilitypotential',\n",
       "       'abinbev', 'abiogenesis', 'abiogenisis', 'abitanti', 'abitibi',\n",
       "       'abituati', 'abitudini', 'abjectly', 'abkhazia', 'abknallen',\n",
       "       'abl', 'ablation', 'ablaze', 'able', 'ablebodied',\n",
       "       'ablebodynormative', 'ablegamers', 'ableminded', 'ablesexist',\n",
       "       'abletons', 'abloohooohoooooooo', 'abmahnkanzleien', 'abn',\n",
       "       'abnegada', 'abneigung', 'abnetts', 'abnormal', 'abnormalities',\n",
       "       'abnormality', 'abnormally', 'abo', 'aboard', 'abode', 'abogado',\n",
       "       'abogados', 'abolish', 'abolished', 'abolishedits', 'abolishment',\n",
       "       'abolition', 'abolitionist', 'abolitionists', 'aboltina',\n",
       "       'abomasnow', 'abomibaby', 'abominable', 'abominablement',\n",
       "       'abominably', 'abominevole', 'aboniations', 'aboniert',\n",
       "       'aboobooboo', 'abooooooooaaaarrrd', 'abordarea', 'aboriginal',\n",
       "       'aborigines', 'abort', 'aborted', 'abortifacients',\n",
       "       'abortifactants', 'abortion', 'abortiongiving', 'abortionjoke',\n",
       "       'abortionplex', 'abortionrobots', 'abortions', 'abortionswould',\n",
       "       'abortionthats', 'abortive', 'abortneys', 'abosulotely',\n",
       "       'abosutley', 'abound', 'about', 'aboutaleb', 'aboutall',\n",
       "       'aboutamiibo', 'aboutannounced', 'aboutanything',\n",
       "       'aboutappreciate', 'aboutappreciating', 'aboutavoids',\n",
       "       'aboutblank', 'aboutchamps', 'aboutcomma', 'aboutcompare',\n",
       "       'aboutconfig', 'aboutdeal', 'abouteight', 'aboutface',\n",
       "       'aboutfight', 'aboutflags', 'aboutfor', 'abouthmmm', 'aboutlike',\n",
       "       'aboutmee', 'aboutmeeting', 'aboutmoderators', 'aboutnewtab',\n",
       "       'aboutnotice', 'aboutseems', 'aboutstupid', 'abouttalon',\n",
       "       'aboutteam', 'aboutthe', 'abouttheir', 'aboutthen', 'aboutthey',\n",
       "       'aboutthis', 'abouttobe', 'aboutummwell', 'aboutus', 'aboutwhat',\n",
       "       'aboutwonder', 'aboutyet', 'aboutyou', 'abouzeid', 'above',\n",
       "       'aboveaverage', 'aboveaverageand', 'abovebelow', 'aboveboard',\n",
       "       'abovecaptioned', 'aboveground', 'aboveitall', 'abovepretty',\n",
       "       'abovethelaw', 'abpokeri', 'abr', 'abracadabra', 'abracadavra',\n",
       "       'abraham', 'abramians', 'abramoff', 'abramovic', 'abramovich',\n",
       "       'abramovichs', 'abramovics', 'abrams', 'abrasions', 'abrasive',\n",
       "       'abrasiveness', 'abreast', 'abricot', 'abridge', 'abridged',\n",
       "       'abriform', 'abril', 'abroad', 'abroadfull', 'abroadwealth',\n",
       "       'abrogate', 'abromovich', 'abrs', 'abrupt', 'abruptly',\n",
       "       'abruptness', 'abs', 'absalom', 'abscbnpr', 'abscess',\n",
       "       'abscessesor', 'absence', 'absences', 'absent', 'absentee',\n",
       "       'absenteeism', 'absentia', 'absentminded', 'absentmindedness',\n",
       "       'abshockedbut', 'absicht', 'absinthsoaked', 'absmag', 'absmidriff',\n",
       "       'absofuckinglutely', 'absofuckinlutely', 'absolutamente',\n",
       "       'absolute', 'absolutely', 'absolutelywhat', 'absolutepunk',\n",
       "       'absolutiom', 'absolutism', 'absolutist', 'absolutists', 'absorb',\n",
       "       'absorbable', 'absorbed', 'absorber', 'absorbing', 'absorbs',\n",
       "       'absorption', 'abssinxx', 'abstain', 'abstained', 'abstemious',\n",
       "       'abstention', 'abstentionnistes', 'abstentions', 'abstinceonly',\n",
       "       'abstinence', 'abstinencebased', 'abstinenceonly', 'abstract',\n",
       "       'abstractbullshit', 'abstraction', 'abstractoverview',\n",
       "       'abstractpattern', 'abstracts', 'absurd', 'absurdity',\n",
       "       'absurdpathetic', 'absvalue', 'abt', 'abu', 'abubakr', 'abuchear',\n",
       "       'abudabi', 'abuelas', 'abuelita', 'abuelitos', 'abuhurayrah',\n",
       "       'abuhwhut', 'abukuma', 'abunadh', 'abunai', 'abundance',\n",
       "       'abundant', 'abundantly', 'aburridos', 'abusalih', 'abuse',\n",
       "       'abuseable', 'abused', 'abusedependent', 'abuseexcept',\n",
       "       'abusefriendly', 'abusekill', 'abuseladen', 'abuseneglect',\n",
       "       'abuser', 'abuserape', 'abusers', 'abuses', 'abusevictim',\n",
       "       'abusing', 'abusive', 'abusivecheating', 'abusivenegligent',\n",
       "       'abuzeze', 'abv', 'abwingz', 'aby', 'abysall', 'abysmally',\n",
       "       'abyss', 'abyssals', 'abyssgaios', 'abyssian', 'abyssinia',\n",
       "       'abyssinian', 'abzocksystemen', 'abzuholen', 'abzuschaffen', 'ac',\n",
       "       'aca'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV.get_feature_names_out()[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3c22124-ddf1-40ba-920f-9685d9697268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4fc4a13-5d24-498b-a87b-c408750c1a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     pool\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_func2\u001b[39m(data,cv\u001b[38;5;241m=\u001b[39m\u001b[43mCV\u001b[49m,train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     25\u001b[0m     X_bow \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_bow\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CV' is not defined"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "num_partitions = num_cores-2 # I like to leave some cores for other\n",
    "#processes\n",
    "print(num_partitions)\n",
    "\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    a = np.array_split(df, num_partitions)\n",
    "    del df\n",
    "    pool = Pool(num_cores)\n",
    "    #df = pd.concat(pool.map(func, [a,b,c,d,e]))\n",
    "    # func = func(cv=cv)\n",
    "    df = sp.vstack(pool.map(func,a), format='csr')\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def test_func2(data,cv=CV,train=False):\n",
    "    X_bow = cv.transform(data)\n",
    "    return X_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34f32129-5176-4ed6-b975-30bbba6b1296",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_bow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mX_train_bow\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_bow' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d024fe9-e414-468b-ba5d-b32b11432044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow.__sizeof__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9a9aad0e-4b21-489c-8335-536e5aa117df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5a031a09-8969-42c4-b039-8a6d9accd3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_bow = parallelize_dataframe(X_valid, test_func2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cd6747d-5a63-491f-a55a-660df036ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# joblib.dump(X_valid_bow.tocsr(), 'X_valid_bow.joblib')\n",
    "X_train_bow = joblib.load('X_train_bow.joblib', mmap_mode='c')\n",
    "X_valid_bow = joblib.load('X_valid_bow.joblib', mmap_mode='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "26ae56ef-37ae-4377-98db-f9e441fd01a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X_train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "474f1594-1689-4037-91ec-af143dc1fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_csr = joblib.load('X_train_bow.joblib', mmap_mode='c')\n",
    "x_csr = X_train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5f2b77e6-0981-4f92-b40d-dea93f7ad576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "logistic= SGDClassifier(loss='log_loss') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b94ed-a6cd-4f7f-846f-be9a249bb244",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.fit(x_csr,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ae193b36-e91d-4803-8237-a85575112309",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred=logistic.predict(X_valid_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "282dfd4b-1662-4731-b815-2a73fe2f9311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.74      0.68     80866\n",
      "           1       0.69      0.57      0.62     80867\n",
      "\n",
      "    accuracy                           0.65    161733\n",
      "   macro avg       0.66      0.65      0.65    161733\n",
      "weighted avg       0.66      0.65      0.65    161733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid,y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18467613-b9a7-4c2b-9ab1-b17d098a681a",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5f63eff9-b58d-41a1-b4f8-9f1e90bf0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "043fff58-d984-4fc9-95af-d037ed7f60e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=1000)\n",
    "trun = TruncatedSVD(n_components=3000,n_iter=1)\n",
    "X_train_trunb = trun.fit_transform(X_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cd2057e3-4811-4de1-a423-394a2e74fcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.70      0.66     80866\n",
      "           1       0.66      0.58      0.62     80867\n",
      "\n",
      "    accuracy                           0.64    161733\n",
      "   macro avg       0.64      0.64      0.64    161733\n",
      "weighted avg       0.64      0.64      0.64    161733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_valid_trunb = trun.transform(X_valid_bow)\n",
    "logistic2= SGDClassifier(loss='log_loss') \n",
    "logistic2.fit(X_train_trunb,y_train)\n",
    "y_val_pred=logistic2.predict(X_valid_trunb)\n",
    "print(classification_report(y_valid,y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "664ab426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(ngram_range=(1,3),max_features=60000) # (1,3) means Unigrams, Bigrams and Trigrams\n",
    "# X_train_bow = cv.fit_transform(X_train).toarray()\n",
    "# X_valid_bow = cv.transform(X_valid).toarray()\n",
    "# X_test_bow = cv.transform(X_test).toarray()\n",
    "\n",
    "# X_train_bow.shape,X_valid_bow.shape,X_test_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6055698-823d-477d-8ab6-d00b4e5665e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5f7147c-5ae5-4504-8eea-6056ceec4b51",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dcbdbe1-85dd-4cf5-b884-8eb4fd2454e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((646928, 60000), (161733, 60000), (202166, 60000))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3),max_features=60000,use_idf=True,smooth_idf=True)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "X_valid_tfidf = tfidf.transform(X_valid).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test).toarray()\n",
    "\n",
    "X_train_tfidf.shape,X_valid_tfidf.shape,X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837e48e-069a-434b-920f-76971ae28692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert array into dataframe\n",
    "# dbbtr = pd.DataFrame(X_train_tfidf)\n",
    "# dbbvd = pd.DataFrame(X_valid_tfidf)\n",
    "# dbbtt = pd.DataFrame(X_test_tfidf)\n",
    "\n",
    "# # save the dataframe as a csv file\n",
    "# dbbtr.to_csv(\"x_train_tfidf.csv\",index=False)\n",
    "# dbbvd.to_csv(\"x_valid_tfidf.csv\",index=False)\n",
    "# dbbtt.to_csv(\"x_test_tfidf.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac4669-0e68-409e-80f2-529a93445719",
   "metadata": {},
   "source": [
    "# word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d15a3-77f0-4c81-899a-d8d69ff23b8b",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb008abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1eab65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import en_core_web_lg\n",
    "# nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9182846f-f417-4856-b076-a2f242b3e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences \n",
    "    def __iter__(self):\n",
    "        for line in self.sentences:\n",
    "            yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04c3a97b-1ca0-4660-9625-a5e246061c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_arr = []\n",
    "# for item in X_train.values:\n",
    "#     doc = nlp(item)\n",
    "#     input_arr.append(doc.vector)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36fc9b4f-cb63-4354-9665-c810c37451e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_arr\n",
    "# X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "812d85f6-146e-44e2-945a-cc95f0e24d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c7d63c6-f1c3-464d-b6dd-85b9e08fc1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MySentences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c6e70544-29a8-4f1b-9758-7823aa5a68ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drake', 'nba', 'millions', 'benjamin', 'franklin', 'canadian', 'dollars']\n"
     ]
    }
   ],
   "source": [
    "for s in sentences:\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "27d6d774-41b8-489f-bd48-bd53267f66d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Word2Vec(sentences=sentences,vector_size=100,sg= 0,window=5,min_count=1,workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0ee4975-fc48-4998-b242-3dc7bea3a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f09a41-2579-4a7c-9efd-1d62be1bdf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a bigram detector.\n",
    "bigram_transformer = Phrases(sentences=sentences,connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "\n",
    "# Apply the trained MWE detector to a corpus, using the result to train a Word2vec model.\n",
    "model = Word2Vec(bigram_transformer[sentences],vector_size=300, min_count=1,workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14a12c1e-0026-4396-aa64-4dc9e718ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('word2vec_b300.model')#bigram included w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e68a7976-e25e-497f-8641-297aec565ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['good'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e97ba97-bf4e-4630-87fc-d9530a683c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10873901"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca82524a-f7b8-432c-9839-148f18485b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5139933"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ENGLISH_CONNECTOR_WORDS\n",
    "len(bigram_transformer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88197d4-5758-4ebd-bb75-5a7e2d1bd29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0834f56c-b50c-40f2-8f22-a4d2e145144c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyedVectors<vector_size=200, 1193517 keys>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c0a7e139-93a4-456f-88df-5383bb504a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv['computer','laptop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a6a26fb4-4d8f-405d-9b33-82e9c47f398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word2vec(x):\n",
    "#     doc = nlp(x)\n",
    "#     vec = doc.vector\n",
    "#     return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8aa86411-9ff7-48c8-a944-80c05b7cc615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['vec'] = df['review'].apply(lambda x: word2vec(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc66cd02-f6e9-4322-8aaf-f1f1c3f96b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77b3b74c-28df-4169-85a8-7d5791aa91cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5be31915-df66-47b8-869c-02793f4bb3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v=dict(zip(model.wv.index_to_key, model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "623db553-097f-46c7-917a-671a38cd4b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e1cfff1-fa6b-4a1b-94b4-79631fffb099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec,dim):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = dim\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec.keys()] or [np.zeros(self.dim)], axis=0) for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38871eac-97fa-4e9e-a5a7-07f85d4b80f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean([w2v[w] for w in ['word','computer'] if w in w2v.keys()],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36aecb10-660b-4f64-acd7-dda6023d1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d5afd2c-b706-4136-b7e3-7fde4cf05cf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedShuffleSplit\n\u001b[1;32m      8\u001b[0m etree_w2v \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m----> 9\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword2vec vectorizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, MeanEmbeddingVectorizer(\u001b[43mw2v\u001b[49m,\u001b[38;5;241m100\u001b[39m)),\n\u001b[1;32m     10\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra trees\u001b[39m\u001b[38;5;124m\"\u001b[39m, ExtraTreesClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m))])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w2v' is not defined"
     ]
    }
   ],
   "source": [
    "etree_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v,100)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200,n_jobs=32))])\n",
    "# etree_w2v_tfidf = Pipeline([\n",
    "#     (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "#     (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef4dd6-ebfa-4188-b66b-7cea0c2f21c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3ad0784f-195a-4003-92f6-d6eda0744423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5270200072863425"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(etree_w2v,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "11d7e72c-0507-474f-928f-87a519da775e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0824815 ,  0.71639675,  0.21218523, -0.34808567,  0.77492297,\n",
       "         1.4794843 ,  0.6010719 ,  0.22179958,  1.6036116 ,  0.9052475 ,\n",
       "         0.87729186, -0.38980472, -2.2052183 , -0.5118392 , -0.55851465,\n",
       "         1.9769785 , -0.7538709 ,  0.6616144 , -0.48489776, -1.002662  ,\n",
       "         1.525203  , -0.16999859, -0.4207438 ,  1.139093  , -1.515057  ,\n",
       "         0.30103773,  0.20051995,  0.23532754,  0.20731485, -1.3388798 ,\n",
       "        -0.90691745,  1.703035  ,  2.0329447 , -0.80370677,  0.09067011,\n",
       "        -0.32836422, -0.44403347,  0.64649737, -1.2978325 ,  1.3318552 ,\n",
       "         1.2680016 , -0.3032618 ,  0.18964666,  1.4722872 , -1.2167127 ,\n",
       "         0.18335038,  0.19206388, -0.9325362 , -0.28691283,  0.9847708 ,\n",
       "         1.3986429 , -0.6484471 ,  0.3222345 ,  0.6789633 , -0.4748214 ,\n",
       "         0.70946497, -2.216866  , -0.75549024, -1.1875656 , -0.5204948 ,\n",
       "        -0.17813279, -0.7590742 ,  1.5861192 ,  0.05278382,  0.38371533,\n",
       "        -0.05811653, -1.7596617 ,  0.3337974 , -0.8002107 ,  0.35835427,\n",
       "        -0.60430896,  1.7696618 , -0.32513386, -0.49064678,  1.4002503 ,\n",
       "        -1.7897534 , -0.59278315, -0.20181972,  1.0709932 , -0.5111989 ,\n",
       "        -2.3518243 , -0.89271474, -0.10539907, -0.47897545, -0.21617419,\n",
       "         0.50755274,  1.1471093 ,  1.1263436 ,  0.47332776, -0.9347746 ,\n",
       "        -0.7791906 ,  0.23648573, -1.3413388 , -0.06077297, -0.59889746,\n",
       "         1.2687843 ,  0.15113397,  0.33563057,  0.84049916,  0.88338804]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MeanEmbeddingVectorizer(w2v).transform([['computer','word']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040b9d4-50fa-4c81-9399-9d112abb4436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "368ebf33-2fe7-4fc6-80b5-5a234ddda735",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d72ad81b-0fc7-4569-b020-e5d4eb2235d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: Extracting glove.twitter.27B.zip ...\n",
      "patool: ... glove.twitter.27B.zip extracted to `glove'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'glove'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import patoolib\n",
    "# patoolib.extract_archive('glove.twitter.27B.zip',outdir='glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "698109da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# glove_input_file = 'glove/glove.twitter.27B.200d.txt'\n",
    "# word2vec_output_file = 'glove/glove.twitter.27B.200d.txt.word2vec'\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "494d5d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.6820898056030273)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/khadga_19024/khadga_19024/lib/python3.10/site-packages/gensim/models/keyedvectors.py:850: RuntimeWarning: invalid value encountered in divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "glove_input_file = 'glove/glove.twitter.27B.200d.txt'\n",
    "model = KeyedVectors.load_word2vec_format(glove_input_file, binary=False,no_header=True)\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d935232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v=dict(zip(model.index_to_key, model.vectors))\n",
    "etree_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v,200)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200,n_jobs=32))])\n",
    "Gau_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v,200)),\n",
    "    (\"extra trees\", GaussianNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5581d908-1cec-4537-88a7-a200805c5b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeanEmbeddingVectorizer(w2v,200).transform([['computer','word']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0433c5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5307530378462768"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(Gau_w2v,X_train,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1490c780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef1460a4-1e3c-4d56-8ae6-52d72a81582e",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "54193f53-76ef-48d5-9ef0-922e1605d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_gui,tqdm,tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "55f04e5b-985d-437e-b202-c340a28afeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MySentences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "afaf392d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1434201/286049523.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tqdm_notebook(sentences))]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba484c6e48c7451aa40b03541aa13b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tqdm_notebook(sentences))]\n",
    "D2V = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "32b3ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c624659d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877953e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70191e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "# Load the spacy model.\n",
    "nlp = en_core_web_sm.load()\n",
    "# Process a sentence using the model\n",
    "doc = nlp(X_train.values[0])\n",
    "print(doc.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa45e10",
   "metadata": {},
   "source": [
    "[ 0.15583833  0.22911808  0.13835862 -0.2090293  -0.00363565 -0.13118638\n",
    " -0.15676734  0.23040245 -0.24502586 -0.10665981  0.23412983 -0.21705897\n",
    "  0.13545002  0.13861737  0.12415285  0.21369664 -0.19687523 -0.10246313\n",
    " -0.03955808  0.27656794 -0.11084636 -0.28008527  0.3439968   0.0482447\n",
    " -0.06924975 -0.03105986 -0.54718924 -0.3975329   0.1704354  -0.16617411\n",
    "  0.09593288 -0.31931752  0.42519966  0.05527496  0.06130749 -0.3888249\n",
    " -0.14431256  0.07848645  0.07683445  0.3278561  -0.3507611  -0.1357377\n",
    "  0.28880984  0.09166662  0.0593424  -0.04693977  0.1484201   0.01081875\n",
    "  0.25666746  0.21095097  0.1744978   0.04549917  0.19118404 -0.25923675\n",
    " -0.06680956  0.25583443 -0.06317639 -0.2850735  -0.03264944  0.0042502\n",
    " -0.07356709 -0.09716224  0.08560921 -0.09941689 -0.12068396 -0.21283795\n",
    " -0.03129313 -0.32813746  0.42847297 -0.37240502  0.26251498 -0.11578684\n",
    " -0.03221569 -0.05079779  0.17090753 -0.12238634 -0.07934     0.22784583\n",
    "  0.20331696 -0.5104732   0.07128731 -0.03987676 -0.05812799  0.19840325\n",
    " -0.10885198 -0.09540041  0.2161586  -0.17952771  0.04469092  0.06091264\n",
    "  0.05722181 -0.02751416  0.19805025  0.32649586 -0.06467736  0.10155083]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c7771c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32193344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ebc3b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "class GpuInfo(object):\n",
    "    def __init__(self, index, memory_total, memory_used, gpu_load):\n",
    "        \"\"\"\n",
    "        :param index: GPU index\n",
    "        :param memory_total: total GPU memory, Mb\n",
    "        :param memory_used: GPU memory already in use, Mb\n",
    "        :param gpu_load: gpu utilization load, percents\n",
    "        \"\"\"\n",
    "        self.index = int(index)\n",
    "        self.memory_total = int(memory_total)\n",
    "        self.memory_used = int(memory_used)\n",
    "        try:\n",
    "            self.gpu_load = int(gpu_load) / 100.\n",
    "        except ValueError:\n",
    "            # gpu utilization load is not supported in current driver\n",
    "            self.gpu_load = 0.\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"GPU #{}: memory total={} Mb, used={} Mb ({:.1f} %), gpu.load={}\".format(\n",
    "            self.index, self.memory_total, self.memory_used, 100. * self.memory_used / self.memory_total, self.gpu_load)\n",
    "\n",
    "    def get_available_memory_portion(self):\n",
    "        return (self.memory_total - self.memory_used) / self.memory_total\n",
    "\n",
    "\n",
    "class NvidiaSmi(object):\n",
    "    def __init__(self):\n",
    "        command = \"nvidia-smi --query-gpu=index,memory.total,memory.used,utilization.gpu --format=csv,noheader,nounits\".split()\n",
    "        self.gpus = []\n",
    "        try:\n",
    "            process = subprocess.Popen(command,\n",
    "                                       universal_newlines=True,\n",
    "                                       stdout=subprocess.PIPE)\n",
    "            stdout, stderr_ignored = process.communicate()\n",
    "            for line in stdout.splitlines():\n",
    "                index, memory_total, memory_used, gpu_load = line.split(', ')\n",
    "                gpu = GpuInfo(index, memory_total, memory_used, gpu_load)\n",
    "                self.gpus.append(gpu)\n",
    "        except FileNotFoundError:\n",
    "            # No GPU is detected. Try running `nvidia-smi` in a terminal.\"\n",
    "            pass\n",
    "\n",
    "    def get_gpus(self, min_free_memory=0., max_load=1.):\n",
    "        \"\"\"\n",
    "        :param min_free_memory: filter GPUs with free memory no less than specified, between 0 and 1\n",
    "        :param max_load: max gpu utilization load, between 0 and 1\n",
    "        :return: list of available GpuInfo's\n",
    "        \"\"\"\n",
    "        gpus = [gpu for gpu in self.gpus if gpu.get_available_memory_portion() >= min_free_memory and\n",
    "                gpu.gpu_load <= max_load]\n",
    "        return gpus\n",
    "\n",
    "\n",
    "def set_cuda_visible_devices(limit_devices=int(1e9), min_free_memory=0.4, max_load=0.6) -> list:\n",
    "    \"\"\"\n",
    "    Automatically sets CUDA_VISIBLE_DEVICES env to first `limit_devices` available GPUs with least used memory.\n",
    "    :param limit_devices: limit available GPU devices to use\n",
    "    :param min_free_memory: filter GPUs with free memory no less than specified, between 0 and 1\n",
    "    :param max_load: max gpu utilization load, between 0 and 1\n",
    "    \"\"\"\n",
    "    gpus = NvidiaSmi().get_gpus(min_free_memory, max_load)\n",
    "    gpus.sort(key=lambda gpu: gpu.get_available_memory_portion(), reverse=True)\n",
    "    limit_devices = min(limit_devices, len(gpus))\n",
    "    gpus = gpus[:limit_devices]\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(str(gpu.index) for gpu in gpus)\n",
    "    print(\"'CUDA_VISIBLE_DEVICES' is set to '{}'\".format(os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
    "    return gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579098f-ef31-4e11-ac99-aef4d8669257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_cuda_visible_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd951d98-30dc-4bf7-ad0f-ceda41c8a424",
   "metadata": {},
   "source": [
    "## My model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ac72a46-667f-4cc9-b670-806bf663293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ray.tune.sklearn import TuneGridSearchCV\n",
    "# from ray.tune.sklearn import TuneSearchCV\n",
    "# from ray import tune\n",
    "# from sklearn.svm import NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d612b31-24ef-4862-8be7-1370e9131fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ray.tune.search import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6bf999-8e75-4d6b-a490-1cf2dc61d878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b742251-b9f6-492d-83bf-43d8dafd28a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classification(X_train, X_test, y_train, y_test,data,name:str,Gridsearch = False,fs:str,model:str:'all'):\n",
    "    # try:\n",
    "    #     y_train = y_train.values.ravel()\n",
    "    #     y_test = y_test.values.ravel()\n",
    "    # except:\n",
    "    #     pass\n",
    "    assert fs in ['pca','chi2','mi',None], f'feature selection should be {\"pca\",\"chi2\",\"mi\",None}. got:{fs}'\n",
    "    assert model in ['all','lr','svm','knn','dtree','rf','mnb'], f\"model should be {'all','lr','svm','knn','dtree','rf','mnb'} but got : {model}\"\n",
    "    \n",
    "    \n",
    "    n_comp = 'fs__K'\n",
    "    if fs == 'mi' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "        fslk =  [500,5000,10000]\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "        fslk = [500,5000,10000]\n",
    "    elif fs == 'pca':\n",
    "        n_comp = 'fs__n_components'\n",
    "        if isinstance(X_train,sp.csr_matrix):\n",
    "            fsl = TruncatedSVD()\n",
    "            fslk = [100,5000,10000]\n",
    "            \n",
    "        else:\n",
    "            fsl =  PCA(svd_solver = 'full')\n",
    "            fslk = [0.95]\n",
    "            \n",
    "    elif fs==None:\n",
    "        fsl = None\n",
    "    \n",
    "    \n",
    "    classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    SGDClassifier(n_jobs=-1)\n",
    "    ]\n",
    "    if Gridsearch ==True:\n",
    "        clf_parameters = [\n",
    "            {\n",
    "                \"clf__n_neighbors\": [1000]#np.arange(2,25 ,10),\n",
    "                \"clf__metric\": [\"euclidean\",\"l1\",\"l2\",\"manhattan\",\"cosine\"],\n",
    "                \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "         },\n",
    "        {\n",
    "               'clf__penalty':['l1', 'l2']\n",
    "        }]\n",
    "    else:\n",
    "        clf_parameters = [{},{}]\n",
    "    # data[name] = {'Model':[],'Accuracy' :[],'f1_micro' :[],'f1_macro' :[]}\n",
    "    # dataint = {'Model':[],'Accuracy' :[],'f1_micro' :[],'f1_macro' :[]}\n",
    "    \n",
    "    i=1\n",
    "    for classifier,clf_params in zip(classifiers,clf_parameters):\n",
    "        if fs!=None:\n",
    "            clf_params[n_comp] = fslk                \n",
    "            pipe = Pipeline(steps = [('fs', fsl),('clf', classifier)])\n",
    "        else:\n",
    "            pipe = Pipeline(steps=[('clf', classifier)])\n",
    "        grid = GridSearchCV(pipe,clf_params,scoring='f1_macro',cv=5,n_jobs=-1,verbose=1) #early_stopping=False,use_gpu=True)  \n",
    "        try:\n",
    "            grid.fit(X_train, y_train)   \n",
    "            pred = grid.predict(X_test)\n",
    "            print(\"_\"*32)\n",
    "            print(f'{i}.',classifier)\n",
    "            print(\"_\"*32)\n",
    "            print(grid.best_params_)\n",
    "            print(classification_report(y_test, pred))\n",
    "            i+=1\n",
    "            i1 = classifier.__class__.__name__\n",
    "            i2 = sklearn.metrics.accuracy_score(y_test,pred)\n",
    "            i3 = sklearn.metrics.f1_score(y_test,pred,average='micro')\n",
    "            i4 = sklearn.metrics.f1_score(y_test,pred,average='macro')\n",
    "            dataint['Model'].append(i1)\n",
    "            dataint['Accuracy'].append(i2)\n",
    "            dataint['f1_micro'].append(i3)\n",
    "            dataint['f1_macro'].append(i4)\n",
    "            print(\"-\"*80)\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "        except Exception as e: print(e)\n",
    "    classifiers = [\n",
    "        DecisionTreeClassifier(),     \n",
    "        RandomForestClassifier(),\n",
    "#         AdaBoostClassifier(),\n",
    "#         GradientBoostingClassifier(),\n",
    "        SGDClassifier(n_jobs=-1,early_stopping=True)\n",
    "        ]\n",
    "    base_estimators = classifiers\n",
    "    if Gridsearch==True:\n",
    "        clf_parameters = [\n",
    "                          {\n",
    "                'clf__criterion' : [\"gini\", \"entropy\"], \n",
    "            'clf__max_features':['sqrt', 'log2',None],\n",
    "        #             'max_depth':np.linspace(140,190,10),\n",
    "                    #'clf__ccp_alpha':np.logspace(-3,-2,20),#np.logspace(-2.32,-2.3,20),\n",
    "            \"clf__max_leaf_nodes\" : [None]+np.arange(30,40,5).tolist(),\n",
    "\n",
    "            \"clf__splitter\" : [\"best\", \"random\"],\n",
    "            #\"clf__min_samples_split\":np.arange(2,50,10)\n",
    "        },\n",
    "            {\n",
    "                 'clf__n_estimators': [150,200,250],\n",
    "            'clf__max_features': ['sqrt', 'log2',None],\n",
    "            'clf__max_depth' : [None],#np.arange(4,15,2).tolist(),\n",
    "            'clf__criterion' :['gini', 'entropy']   ,\n",
    "                'clf__bootstrap' :[True],\n",
    "        #         'clf__ccp_alpha':np.logspace(-2,1,10)\n",
    "            },\n",
    "#             {\n",
    "#                 'clf__base_estimator':[RandomForestClassifier(),DecisionTreeClassifier(criterion='entropy'),SVC(),LogisticRegression(**{'C': 0.017433288221999882, 'penalty': 'l2', 'solver': 'newton-cg'})],\n",
    "#                 'clf__algorithm' : ['SAMME', 'SAMME.R'],\n",
    "#                 'clf__n_estimators': [50,100]\n",
    "#             },\n",
    "#             {\n",
    "#               'clf__loss' :['deviance', 'exponential'],\n",
    "#                 'clf__criterion' : ['friedman_mse', 'squared_error'],\n",
    "#                 'clf__max_features' : [ 'sqrt', 'log2'],\n",
    "#                 'clf__learning_rate': np.logspace(-1,1,5),\n",
    "#                 'clf__n_estimators':np.arange(100,1000,200)\n",
    "\n",
    "#             },\n",
    "            {\n",
    "                'clf__loss' :['hinge'], #'modified_huber','squared_hinge', 'perceptron'],\n",
    "                #'clf__early_stopping':[True],\n",
    "                \n",
    "            }\n",
    "            ]\n",
    "    else:\n",
    "        clf_parameters = [{}]*len(classifiers)\n",
    "    for classifier,clf_params in zip(classifiers,clf_parameters):\n",
    "        if fs!=None:\n",
    "            clf_params[n_comp] = fslk                \n",
    "            pipe = Pipeline(steps = [('fs', fsl),('clf', classifier)])\n",
    "        else:\n",
    "            pipe = Pipeline(steps=[('clf', classifier)])\n",
    "        grid = GridSearchCV(pipe,clf_params,scoring='f1_macro',cv=5,n_jobs=-1, verbose=1)#use_gpu=True,  \n",
    "        try:\n",
    "            grid.fit(X_train, y_train)   \n",
    "            pred = grid.predict(X_test)\n",
    "            print(\"_\"*32)\n",
    "            print(f'{i}.',classifier)\n",
    "            print(\"_\"*32)\n",
    "            print(grid.best_params_)\n",
    "            print(classification_report(y_test, pred))\n",
    "            i=i+1\n",
    "            i1 = classifier.__class__.__name__\n",
    "            i2 = sklearn.metrics.accuracy_score(y_test,pred)\n",
    "            i3 = sklearn.metrics.f1_score(y_test,pred,average='micro')\n",
    "            i4 = sklearn.metrics.f1_score(y_test,pred,average='macro')\n",
    "            dataint['Model'].append(i1)\n",
    "            dataint['Accuracy'].append(i2)\n",
    "            dataint['f1_micro'].append(i3)\n",
    "            dataint['f1_macro'].append(i4)\n",
    "            print(\"-\"*80)\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "        except Exception as e: print(e)\n",
    "    \n",
    "    \n",
    "    classifiers = [\n",
    "    MultinomialNB()\n",
    "    ]\n",
    "    if Gridsearch==True:\n",
    "        clf_parameters = [\n",
    "         \n",
    "            {\n",
    "                'clf__alpha':[0] + np.logspace(-2,5,5).tolist(),\n",
    "                'clf__fit_prior':[True,False]\n",
    "            }\n",
    "             ]\n",
    "    else:\n",
    "        clf_parameters = [{}]*len(classifiers)\n",
    "\n",
    "    for classifier,clf_params in zip(classifiers,clf_parameters):\n",
    "        if fs!=None:\n",
    "            clf_params[n_comp] = fslk                \n",
    "            pipe = Pipeline(steps = [('fs', fsl),('clf', classifier)])\n",
    "        else:\n",
    "            pipe = Pipeline(steps=[('clf', classifier)])\n",
    "        grid = GridSearchCV(pipe,clf_params,scoring='f1_macro',cv=5,n_jobs=-1,verbose=1)  \n",
    "        try:\n",
    "            grid.fit(X_train, y_train)   \n",
    "            pred = grid.predict(X_test)\n",
    "            print(\"_\"*32)\n",
    "            print(f'{i}.',classifier)\n",
    "            print(\"_\"*32)\n",
    "            print(grid.best_params_)\n",
    "            print(classification_report(y_test, pred))\n",
    "            i=i+1\n",
    "            i1 = classifier.__class__.__name__\n",
    "            i2 = sklearn.metrics.accuracy_score(y_test,pred)\n",
    "            i3 = sklearn.metrics.f1_score(y_test,pred,average='micro')\n",
    "            i4 = sklearn.metrics.f1_score(y_test,pred,average='macro')\n",
    "            dataint['Model'].append(i1)\n",
    "            dataint['Accuracy'].append(i2)\n",
    "            dataint['f1_micro'].append(i3)\n",
    "            dataint['f1_macro'].append(i4)\n",
    "            print(\"-\"*80)\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "        except Exception as e: print(e)\n",
    "            \n",
    "    # data[name] = dataint\n",
    "    # print(dataint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87869a5-0eba-4aed-9a0d-b8f6e554a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classification(X_train, X_test, y_train, y_test,data,name:str,Gridsearch = False,fs:str=None,model:str='all',cv=5):\n",
    "    # try:\n",
    "    #     y_train = y_train.values.ravel()\n",
    "    #     y_test = y_test.values.ravel()\n",
    "    # except:\n",
    "    #     pass\n",
    "    assert fs in ['pca','chi2','mi',None], f'feature selection should be {\"pca\",\"chi2\",\"mi\",None}. got:{fs}'\n",
    "    mdls = ['knn','lr','dtree','rf','svm','mnb','all']\n",
    "    assert model in mdls, f\"model should be {'all','lr','svm','knn','dtree','rf','mnb'} but got : {model}\"\n",
    "    \n",
    "    \n",
    "    n_comp = 'fs__K'\n",
    "    if fs == 'mi' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "        fslk =  [500,5000,10000]\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "        fslk = [500,5000,10000]\n",
    "    elif fs == 'pca':\n",
    "        n_comp = 'fs__n_components'\n",
    "        if isinstance(X_train,sp.csr_matrix):\n",
    "            fsl = TruncatedSVD()\n",
    "            fslk = [100,5000,10000]\n",
    "            \n",
    "        else:\n",
    "            fsl =  PCA(svd_solver = 'full')\n",
    "            fslk = [0.95]\n",
    "            \n",
    "    elif fs==None:\n",
    "        fsl = None\n",
    "    \n",
    "    \n",
    "    classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    SGDClassifier(n_jobs=-1,loss='log'),\n",
    "    DecisionTreeClassifier(),     \n",
    "    RandomForestClassifier(),\n",
    "#         AdaBoostClassifier(),\n",
    "#         GradientBoostingClassifier(),\n",
    "    SGDClassifier(n_jobs=-1,early_stopping=True),\n",
    "    MultinomialNB()\n",
    "        \n",
    "    ]\n",
    "    if Gridsearch ==True:\n",
    "        clf_parameters = [\n",
    "            {\n",
    "                \"clf__n_neighbors\": [1000],#np.arange(2,25 ,10),\n",
    "                \"clf__metric\": [\"euclidean\",\"l1\",\"l2\",\"manhattan\",\"cosine\"],\n",
    "                \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "         },\n",
    "        {\n",
    "               'clf__penalty':['l1', 'l2']\n",
    "        },\n",
    "        {\n",
    "                'clf__criterion' : [\"gini\", \"entropy\"], \n",
    "            'clf__max_features':['sqrt', 'log2',None],\n",
    "        #             'max_depth':np.linspace(140,190,10),\n",
    "                    #'clf__ccp_alpha':np.logspace(-3,-2,20),#np.logspace(-2.32,-2.3,20),\n",
    "            \"clf__max_leaf_nodes\" : [None]+np.arange(30,40,5).tolist(),\n",
    "\n",
    "            \"clf__splitter\" : [\"best\", \"random\"],\n",
    "            #\"clf__min_samples_split\":np.arange(2,50,10)\n",
    "        },\n",
    "            {\n",
    "                 'clf__n_estimators': [150,200,250],\n",
    "            'clf__max_features': ['sqrt', 'log2',None],\n",
    "            'clf__max_depth' : [None],#np.arange(4,15,2).tolist(),\n",
    "            'clf__criterion' :['gini', 'entropy']   ,\n",
    "                'clf__bootstrap' :[True],\n",
    "        #         'clf__ccp_alpha':np.logspace(-2,1,10)\n",
    "            },\n",
    "#             {\n",
    "#                 'clf__base_estimator':[RandomForestClassifier(),DecisionTreeClassifier(criterion='entropy'),SVC(),LogisticRegression(**{'C': 0.017433288221999882, 'penalty': 'l2', 'solver': 'newton-cg'})],\n",
    "#                 'clf__algorithm' : ['SAMME', 'SAMME.R'],\n",
    "#                 'clf__n_estimators': [50,100]\n",
    "#             },\n",
    "#             {\n",
    "#               'clf__loss' :['deviance', 'exponential'],\n",
    "#                 'clf__criterion' : ['friedman_mse', 'squared_error'],\n",
    "#                 'clf__max_features' : [ 'sqrt', 'log2'],\n",
    "#                 'clf__learning_rate': np.logspace(-1,1,5),\n",
    "#                 'clf__n_estimators':np.arange(100,1000,200)\n",
    "\n",
    "#             },\n",
    "            {\n",
    "                'clf__loss' :['hinge'], #'modified_huber','squared_hinge', 'perceptron'],\n",
    "                #'clf__early_stopping':[True],\n",
    "                \n",
    "            },\n",
    "            {\n",
    "                'clf__alpha':[0] + np.logspace(-2,5,5).tolist(),\n",
    "                'clf__fit_prior':[True,False]\n",
    "            }]\n",
    "    else:\n",
    "        clf_parameters = [{},{}]\n",
    "    # data[name] = {'Model':[],'Accuracy' :[],'f1_micro' :[],'f1_macro' :[]}\n",
    "    # dataint = {'Model':[],'Accuracy' :[],'f1_micro' :[],'f1_macro' :[]}\n",
    "    \n",
    "    def fit():\n",
    "        if fs!=None:\n",
    "                clf_params[n_comp] = fslk                \n",
    "                pipe = Pipeline(steps = [('fs', fsl),('clf', classifier)])\n",
    "        else:\n",
    "            pipe = Pipeline(steps=[('clf', classifier)])\n",
    "        grid = GridSearchCV(pipe,clf_params,scoring='f1_macro',cv=cv,n_jobs=-1,verbose=1) #early_stopping=False,use_gpu=True)  \n",
    "        try:\n",
    "            grid.fit(X_train, y_train)   \n",
    "            pred = grid.predict(X_test)\n",
    "            print(\"_\"*32)\n",
    "            print(f'{i}.',classifier)\n",
    "            print(\"_\"*32)\n",
    "            print(grid.best_params_)\n",
    "            print(classification_report(y_test, pred))\n",
    "            i+=1\n",
    "            i1 = classifier.__class__.__name__\n",
    "            i2 = sklearn.metrics.accuracy_score(y_test,pred)\n",
    "            i3 = sklearn.metrics.f1_score(y_test,pred,average='micro')\n",
    "            i4 = sklearn.metrics.f1_score(y_test,pred,average='macro')\n",
    "            dataint['Model'].append(i1)\n",
    "            dataint['Accuracy'].append(i2)\n",
    "            dataint['f1_micro'].append(i3)\n",
    "            dataint['f1_macro'].append(i4)\n",
    "            print(\"-\"*80)\n",
    "            print(\"-\"*80)\n",
    "\n",
    "        except Exception as e: print(e)\n",
    "    i=1\n",
    "    if model =='all':        \n",
    "        for classifier,clf_params in zip(classifiers,clf_parameters):\n",
    "            fit()\n",
    "    else:\n",
    "        _ = mdls.index(model)\n",
    "        classifier,clf_params = classifiers[_],clf_parameters[_]\n",
    "        fit()\n",
    "#     classifiers = [\n",
    "#         DecisionTreeClassifier(),     \n",
    "#         RandomForestClassifier(),\n",
    "# #         AdaBoostClassifier(),\n",
    "# #         GradientBoostingClassifier(),\n",
    "#         SGDClassifier(n_jobs=-1,early_stopping=True)\n",
    "#         ]\n",
    "#     base_estimators = classifiers\n",
    "#     if Gridsearch==True:\n",
    "#         clf_parameters = [\n",
    "#                           {\n",
    "#                 'clf__criterion' : [\"gini\", \"entropy\"], \n",
    "#             'clf__max_features':['sqrt', 'log2',None],\n",
    "#         #             'max_depth':np.linspace(140,190,10),\n",
    "#                     #'clf__ccp_alpha':np.logspace(-3,-2,20),#np.logspace(-2.32,-2.3,20),\n",
    "#             \"clf__max_leaf_nodes\" : [None]+np.arange(30,40,5).tolist(),\n",
    "\n",
    "#             \"clf__splitter\" : [\"best\", \"random\"],\n",
    "#             #\"clf__min_samples_split\":np.arange(2,50,10)\n",
    "#         },\n",
    "#             {\n",
    "#                  'clf__n_estimators': [150,200,250],\n",
    "#             'clf__max_features': ['sqrt', 'log2',None],\n",
    "#             'clf__max_depth' : [None],#np.arange(4,15,2).tolist(),\n",
    "#             'clf__criterion' :['gini', 'entropy']   ,\n",
    "#                 'clf__bootstrap' :[True],\n",
    "#         #         'clf__ccp_alpha':np.logspace(-2,1,10)\n",
    "#             },\n",
    "# #             {\n",
    "# #                 'clf__base_estimator':[RandomForestClassifier(),DecisionTreeClassifier(criterion='entropy'),SVC(),LogisticRegression(**{'C': 0.017433288221999882, 'penalty': 'l2', 'solver': 'newton-cg'})],\n",
    "# #                 'clf__algorithm' : ['SAMME', 'SAMME.R'],\n",
    "# #                 'clf__n_estimators': [50,100]\n",
    "# #             },\n",
    "# #             {\n",
    "# #               'clf__loss' :['deviance', 'exponential'],\n",
    "# #                 'clf__criterion' : ['friedman_mse', 'squared_error'],\n",
    "# #                 'clf__max_features' : [ 'sqrt', 'log2'],\n",
    "# #                 'clf__learning_rate': np.logspace(-1,1,5),\n",
    "# #                 'clf__n_estimators':np.arange(100,1000,200)\n",
    "\n",
    "# #             },\n",
    "#             {\n",
    "#                 'clf__loss' :['hinge'], #'modified_huber','squared_hinge', 'perceptron'],\n",
    "#                 #'clf__early_stopping':[True],\n",
    "                \n",
    "#             }\n",
    "#             ]\n",
    "#     else:\n",
    "#         clf_parameters = [{}]*len(classifiers)\n",
    "#     for classifier,clf_params in zip(classifiers,clf_parameters):\n",
    "#         if fs!=None:\n",
    "#             clf_params[n_comp] = fslk                \n",
    "#             pipe = Pipeline(steps = [('fs', fsl),('clf', classifier)])\n",
    "#         else:\n",
    "#             pipe = Pipeline(steps=[('clf', classifier)])\n",
    "#         grid = GridSearchCV(pipe,clf_params,scoring='f1_macro',cv=5,n_jobs=-1, verbose=1)#use_gpu=True,  \n",
    "#         try:\n",
    "#             grid.fit(X_train, y_train)   \n",
    "#             pred = grid.predict(X_test)\n",
    "#             print(\"_\"*32)\n",
    "#             print(f'{i}.',classifier)\n",
    "#             print(\"_\"*32)\n",
    "#             print(grid.best_params_)\n",
    "#             print(classification_report(y_test, pred))\n",
    "#             i=i+1\n",
    "#             i1 = classifier.__class__.__name__\n",
    "#             i2 = sklearn.metrics.accuracy_score(y_test,pred)\n",
    "#             i3 = sklearn.metrics.f1_score(y_test,pred,average='micro')\n",
    "#             i4 = sklearn.metrics.f1_score(y_test,pred,average='macro')\n",
    "#             dataint['Model'].append(i1)\n",
    "#             dataint['Accuracy'].append(i2)\n",
    "#             dataint['f1_micro'].append(i3)\n",
    "#             dataint['f1_macro'].append(i4)\n",
    "#             print(\"-\"*80)\n",
    "#             print(\"-\"*80)\n",
    "            \n",
    "#         except Exception as e: print(e)\n",
    "    \n",
    "    \n",
    "#     classifiers = [\n",
    "#     MultinomialNB()\n",
    "#     ]\n",
    "#     if Gridsearch==True:\n",
    "#         clf_parameters = [\n",
    "         \n",
    "#             {\n",
    "#                 'clf__alpha':[0] + np.logspace(-2,5,5).tolist(),\n",
    "#                 'clf__fit_prior':[True,False]\n",
    "#             }\n",
    "#              ]\n",
    "#     else:\n",
    "#         clf_parameters = [{}]*len(classifiers)\n",
    "\n",
    "#     for classifier,clf_params in zip(classifiers,clf_parameters):\n",
    "#         if fs!=None:\n",
    "#             clf_params[n_comp] = fslk                \n",
    "#             pipe = Pipeline(steps = [('fs', fsl),('clf', classifier)])\n",
    "#         else:\n",
    "#             pipe = Pipeline(steps=[('clf', classifier)])\n",
    "#         grid = GridSearchCV(pipe,clf_params,scoring='f1_macro',cv=5,n_jobs=-1,verbose=1)  \n",
    "#         try:\n",
    "#             grid.fit(X_train, y_train)   \n",
    "#             pred = grid.predict(X_test)\n",
    "#             print(\"_\"*32)\n",
    "#             print(f'{i}.',classifier)\n",
    "#             print(\"_\"*32)\n",
    "#             print(grid.best_params_)\n",
    "#             print(classification_report(y_test, pred))\n",
    "#             i=i+1\n",
    "#             i1 = classifier.__class__.__name__\n",
    "#             i2 = sklearn.metrics.accuracy_score(y_test,pred)\n",
    "#             i3 = sklearn.metrics.f1_score(y_test,pred,average='micro')\n",
    "#             i4 = sklearn.metrics.f1_score(y_test,pred,average='macro')\n",
    "#             dataint['Model'].append(i1)\n",
    "#             dataint['Accuracy'].append(i2)\n",
    "#             dataint['f1_micro'].append(i3)\n",
    "#             dataint['f1_macro'].append(i4)\n",
    "#             print(\"-\"*80)\n",
    "#             print(\"-\"*80)\n",
    "            \n",
    "#         except Exception as e: print(e)\n",
    "            \n",
    "    # data[name] = dataint\n",
    "#     print(dataint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9ad522f-84a7-4970-b376-65ad1cb99777",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data_GS = {}\n",
    "# If you have 8 GPUs, this will run 8 trials at once.\n",
    "# tuner = tune.Tuner(tune.with_resources(trainable, {\"gpu\": 0}, tune_config=tune.TuneConfig(num_samples=10)))\n",
    "# results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44cd7dc9-2db5-4e0b-8bf7-1520d0e67191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<646928x60000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12694957 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5108068-757a-46d4-9096-aa1908395385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB(alpha=0.5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(alpha=0.5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB(alpha=0.5)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Knn = MultinomialNB(alpha=0.5)\n",
    "Knn.fit(X_train_bow,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22b17074-95ca-4ddd-b8cc-839f75909292",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Knn.predict(X_valid_bow[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "034bdcf2-a164-40da-a881-6335263c7a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.69      0.66       498\n",
      "           1       0.67      0.62      0.64       502\n",
      "\n",
      "    accuracy                           0.65      1000\n",
      "   macro avg       0.65      0.65      0.65      1000\n",
      "weighted avg       0.65      0.65      0.65      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid[0:1000],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c125fa2f-ac34-427a-934e-c92ea6b8804a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/khadga_19024/khadga_19024/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/khadga_19024/khadga_19024/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mClassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_bow\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_valid_bow\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBoW\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mGridsearch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [20], line 36\u001b[0m, in \u001b[0;36mClassification\u001b[0;34m(X_train, X_test, y_train, y_test, data, name, Gridsearch)\u001b[0m\n\u001b[1;32m     34\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(pipe,clf_params,scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m,cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#early_stopping=False,use_gpu=True)  \u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     37\u001b[0m     pred \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[0;32m~/khadga_19024/khadga_19024/lib/python3.10/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/khadga_19024/khadga_19024/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/khadga_19024/khadga_19024/lib/python3.10/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/khadga_19024/khadga_19024/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/khadga_19024/khadga_19024/lib/python3.10/site-packages/joblib/parallel.py:997\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;28mhasattr\u001b[39m(backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabort_everything\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;66;03m# If the backend is managed externally we need to make sure\u001b[39;00m\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;66;03m# to leave it in a working state to allow for future jobs\u001b[39;00m\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;66;03m# scheduling.\u001b[39;00m\n\u001b[1;32m    996\u001b[0m     ensure_ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_managed_backend\n\u001b[0;32m--> 997\u001b[0m     \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabort_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensure_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ready\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/khadga_19024/khadga_19024/lib/python3.10/site-packages/joblib/_parallel_backends.py:586\u001b[0m, in \u001b[0;36mLokyBackend.abort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabort_everything\u001b[39m(\u001b[38;5;28mself\u001b[39m, ensure_ready\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124;03m\"\"\"Shutdown the workers and restart a new one with the same parameters\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ensure_ready:\n",
      "File \u001b[0;32m~/khadga_19024/khadga_19024/lib/python3.10/site-packages/joblib/executor.py:74\u001b[0m, in \u001b[0;36mMemmappingExecutor.terminate\u001b[0;34m(self, kill_workers)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mterminate\u001b[39m(\u001b[38;5;28mself\u001b[39m, kill_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkill_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kill_workers:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;66;03m# When workers are killed in such a brutal manner, they cannot\u001b[39;00m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;66;03m# execute the finalizer of their shared memmaps. The refcount of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m         \u001b[38;5;66;03m# unregister temporary resources from all contexts\u001b[39;00m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit_resize_lock:\n",
      "File \u001b[0;32m~/khadga_19024/khadga_19024/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:1199\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread_wakeup\u001b[38;5;241m.\u001b[39mwakeup()\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_manager_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m wait:\n\u001b[0;32m-> 1199\u001b[0m     \u001b[43mexecutor_manager_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;66;03m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;66;03m# objects that use file descriptors.\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1089\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1089\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1109\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1110\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1111\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Classification(X_train_bow,X_valid_bow,y_train,y_valid,data,'BoW',Gridsearch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c18c489d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan', 'precomputed'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.neighbors.VALID_METRICS_SPARSE['brute']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a94c6-8cff-4816-ba5d-18445b89a398",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3429407-3182-4ebb-898c-78fe52e512fe",
   "metadata": {},
   "source": [
    "### With Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24777d36-0cc2-4dc3-a65e-de6a7ecf16cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "371380e6-1731-43ba-9d85-b70db854a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    fsl = 0\n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=100))\n",
    "    \n",
    "    print('\\n\\t ---------- Training Logistic Regression Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline1 = Pipeline([('fs',fsl),\n",
    "                          ('clf1', LogisticRegression(class_weight='balanced'))])\n",
    "    clf1_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf1__penalty':['none', 'l1', 'l2', 'elasticnet',],\n",
    "        'clf1__C':[10,0.01,0.001,0.003,0.004],     \n",
    "        'clf1__solver':['newton-cg','liblinear','sag',]\n",
    "        }\n",
    "    grid_search1 = GridSearchCV(estimator=pipeline1, param_grid=clf1_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search1.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search1.best_score_}\")\n",
    "    clf1 = grid_search1.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf1) \n",
    "    predicted_class_labels1 = clf1.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels1))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a7a75407-270c-4276-926b-59fe2b613fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    print('\\n\\t ---------- Training Decision Tree Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline2 = Pipeline([('fs', fsl),\n",
    "                          ('clf2', DecisionTreeClassifier(random_state=40))])\n",
    "    clf2_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf2__criterion':['entropy','gini',], \n",
    "        'clf2__max_features':['sqrt', 'log2',None],\n",
    "        'clf2__max_depth':[10,25,40,100],\n",
    "        'clf2__ccp_alpha':[0.002,0.004,0.009,0.01,0.1,]\n",
    "        }\n",
    "    grid_search2 = GridSearchCV(estimator=pipeline2, param_grid=clf2_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search2.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search2.best_score_}\")\n",
    "    clf2 = grid_search2.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf2)\n",
    "    predicted_class_labels2 = clf2.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels2))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4062c346-0457-46a8-ad89-6a3293925b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    print('\\n\\t ---------- Training KNN Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline3 = Pipeline([('fs', fsl),\n",
    "                          ('clf3', KNeighborsClassifier())])\n",
    "    clf3_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf3__n_neighbors': [1,2,3,5,10,20,25,35,36,38,],      \n",
    "        'clf3__weights':['uniform', 'distance',],\n",
    "        'clf3__p':[1,2,],\n",
    "        'clf3__metric':['euclidean', 'manhattan',] \n",
    "        }\n",
    "    grid_search3 = GridSearchCV(estimator=pipeline3, param_grid=clf3_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search3.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search3.best_score_}\")\n",
    "    clf3 = grid_search3.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf3) \n",
    "    predicted_class_labels3 = clf3.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels3))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "312880c9-7032-464e-bb93-cccb13d9e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianNB(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    # Gaussian Naive Bayes\n",
    "    print('\\n\\t ---------- Training Gaussian Naive Bayes Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline4 = Pipeline([('fs', fsl),\n",
    "                          ('clf4', GaussianNB())])\n",
    "    clf4_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf4__var_smoothing': np.logspace(0,-9, num=60)\n",
    "        }\n",
    "    grid_search4 = GridSearchCV(estimator=pipeline4, param_grid=clf4_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search4.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search4.best_score_}\")\n",
    "    clf4 = grid_search4.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf4) \n",
    "    predicted_class_labels4 = clf4.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels4))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "62a9a628-b55f-4c29-b899-b760508ed1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultinomialNB(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    print('\\n\\t ---------- Training Multinomial Naive Bayes Classifier ---------- \\n')  \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline5 = Pipeline([('fs', fsl),\n",
    "                          ('clf5', MultinomialNB(fit_prior=True, class_prior=None))])\n",
    "    clf5_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf5__alpha':[0,1,]\n",
    "        }\n",
    "    grid_search5 = GridSearchCV(estimator=pipeline5, param_grid=clf5_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search5.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search5.best_score_}\")\n",
    "    clf5 = grid_search5.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf5) \n",
    "    predicted_class_labels5 = clf5.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels5))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e90e3426-3da7-47a0-a243-9a101c1ca0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearSVC(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    print('\\n\\t ---------- Training Linear SVC Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline8 = Pipeline([('fs', fsl),\n",
    "                          ('clf8', LinearSVC(class_weight='balanced'))])\n",
    "    clf8_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf8__C':[0.001,0.01,1,100,],\n",
    "        }  \n",
    "    grid_search8 = GridSearchCV(estimator=pipeline8, param_grid=clf8_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search8.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search8.best_score_}\")\n",
    "    clf8 = grid_search8.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf8) \n",
    "    predicted_class_labels8 = clf8.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels8))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "866e17ab-6d5f-4078-945b-ba45ad831a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVC(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    print('\\n\\t ---------- Training SVM Classifier ---------- \\n') \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline9 = Pipeline([('fs', fsl),\n",
    "                          ('clf9', SVC(probability=True))])\n",
    "    clf9_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf9__C':[0.1,1,50,60,70,80,100,150],\n",
    "        'clf9__kernel':['poly','linear','sigmoid','rbf',],\n",
    "        }\n",
    "    grid_search9 = GridSearchCV(estimator=pipeline9, param_grid=clf9_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search9.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search9.best_score_}\")\n",
    "    clf9 = grid_search9.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf9) \n",
    "    predicted_class_labels9 = clf9.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels9))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "872f36c9-9237-41c0-9e97-8263e899f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000)) \n",
    "    \n",
    "    print('\\n\\t ---------- Training Random Forest Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline10 = Pipeline([('fs', fsl),\n",
    "                           ('clf10', RandomForestClassifier(class_weight='balanced'))])\n",
    "    clf10_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf10__criterion':['entropy','gini',],\n",
    "        'clf10__max_depth':[10,25,30,45,50,80,100,200,],\n",
    "        'clf10__n_estimators':[30,50,100,],\n",
    "        'clf10__max_features':['sqrt','log2',None,] \n",
    "        } \n",
    "    grid_search10 = GridSearchCV(estimator=pipeline10, param_grid=clf10_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search10.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search10.best_score_}\")\n",
    "    clf10 = grid_search10.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf10) \n",
    "    predicted_class_labels10 = clf10.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels10))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e8c02-03ee-42e1-8f2d-8267fa02c7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b4f0f4-259d-4b42-b827-f530f19a1595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950da33-a787-4ded-bb50-6b13cff5d15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b1dce-9865-445f-94a9-9658fbef42fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc979d3a-36c8-4495-a8d2-4735565b8066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f1650f-ec6a-4887-a949-96631f063832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cea766d5-66ae-4952-a244-245540af722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    print('\\n\\t ---------- Training LDA Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline12 = Pipeline([('fs', fsl),\n",
    "                           ('clf12', LinearDiscriminantAnalysis())])\n",
    "    clf12_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf12__solver':['svd','lsqr', 'eigen',],\n",
    "        'clf12__shrinkage':['auto',0.0005,0.0008,0.001,0.005,0.01,0.1,None,]\n",
    "        } \n",
    "    grid_search12 = GridSearchCV(estimator=pipeline12, param_grid=clf12_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search12.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search12.best_score_}\")\n",
    "    clf12 = grid_search12.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf12) \n",
    "    predicted_class_labels12 = clf12.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels12)) \n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "62e7390d-0248-4cf5-b78d-d9a6f96ccd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(SelectKBest(chi2, k=1000))\n",
    "    \n",
    "    print('\\n\\t ---------- Training XGBoost Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline13 = Pipeline([('fs', fsl),\n",
    "                           ('clf13', XGBClassifier(booster = 'gbtree',objective='multi:softmax',num_class = 4,eval_metric = 'merror'))])\n",
    "    clf13_parameters = {\n",
    "        'fs__k' : [i+1 for i in range(x_train.shape[1])],\n",
    "        'clf13__n_estimators':[20,50,80,100,200,], \n",
    "        'clf13__max_depth':[5,10,15,30,50,100,], \n",
    "        'clf13__learning_rate':[0.15, 0.2, 0.3,],\n",
    "        'clf13__gamma':[0,0.1,0.2,0.3,],\n",
    "        'clf13__min_child_weight':[1,3,5,7,30,], \n",
    "        'clf13__colsample_bytree':[0.3,0.6,0.9,1,2], \n",
    "        'clf13__reg_lambda':[0,0.1,0.2,0.4,0.8,],      \n",
    "        }\n",
    "    grid_search13 = GridSearchCV(estimator=pipeline13, param_grid=clf13_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search13.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search13.best_score_}\")\n",
    "    clf13 = grid_search13.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf13) \n",
    "    predicted_class_labels13 = clf13.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels13))   \n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80daa4b6-f264-48ef-a19a-422133eadea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b763af64-0c3d-48bf-be69-455d87093cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = DecisionTree(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300aeb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = KNN(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c25d54-04e5-4196-8ee3-c6619eec9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = GaussianNB(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c76d8-7ac1-4157-bd3f-b30c292adbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5 = MultinomialNB(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf0c31e-e853-4f83-b390-3460b3ae6dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8 = LinearSVC(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1572afa-c610-4892-b49b-1ed3b9e999ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9 = SVC(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89385cd7-cd92-4f24-bf87-c2d194755270",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10 = RandomForest(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1a41e-aa45-4af9-b9e8-1a9fcfcb81b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf11 = AdaBoost(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2a53d-f0f5-4062-9c99-e2672077ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf12 = LDA(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d14c5-2c26-422b-bab4-f0386382b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf13 = XGBoost(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4089ef-5845-4634-a28a-3eda60472af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0670d7d-d8e4-4952-ae28-96faf8d384a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a32b0f-4d08-4934-bfaa-bbeb9a9656c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4686ea4-8d38-48a8-95f8-8ea63d5588d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bfbaa3-62a0-4858-bef8-4730df7f473d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad5cf3-1807-416a-8af3-0bc60ff9a0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb3da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "print('\\n Total documents in the training set: '+str(len(trn_data))+'\\n')    \n",
    "print('\\n Total documents in the test set: '+str(len(tst_data))+'\\n') \n",
    "\n",
    "pr=precision_score(tst_cat, predicted, average='binary') \n",
    "print ('\\n Precision:'+str(pr)) \n",
    "\n",
    "rl=recall_score(tst_cat, predicted, average='binary') \n",
    "print ('\\n Recall:'+str(rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a669e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab5936",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1_mis = logistic_fs(x_trains,x_valids,y_train, y_valid,'MI') # by standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7d6e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1_anovas = logistic_fs(x_trains,x_valids,y_train, y_valid,'ANOVA') # Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f6eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895003f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c4eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df5918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4635ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get name of the 5 best features in decresing order i.e 1st feature is high import. then 2nd fet then 3rd so on\n",
    "X_train.columns[sel_five_cols.get_support()] # x_train is the same as my x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffl = logistic_fsf(x_trains,x_valids,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56275aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predl = ffl.predict(x_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving predicted class labels as txt file\n",
    "np.savetxt('Akash_Singh_test_class_labels.txt',predl,fmt='%d',delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c84942b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
