{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631578ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk    \n",
    "# !pip install emoji        \n",
    "# !pip install autocorrec\\t    \n",
    "# !pip install xgboost\n",
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e5a27d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nltk.download('all-corpora')\n",
    "# nltk.download('punkt')  # for using word_tokenizer\n",
    "# nltk.download('wordnet')  # for using Lemmatizer\n",
    "# nltk.download('averaged_perceptron_tagger') # for language processing i.e tagging words with their parts of speech (POS)\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f454ef6d-b13b-495d-9af8-8b098ae96f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e54a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import spacy\n",
    "import nltk\n",
    "import re   # regular expression\n",
    "import string\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import scipy.sparse as sp\n",
    "import joblib\n",
    "import emoji\n",
    "from autocorrect import Speller   # for correcting spelling\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize    # for tokenizing string into words\n",
    "from nltk.stem import WordNetLemmatizer    # for lemmatizing words\n",
    "from nltk.tag import pos_tag # for tagging words with their parts of speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82ce6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e22a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d02e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.feature_selection import SelectKBest,chi2, mutual_info_classif\n",
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204dfd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# import xgboost\n",
    "# from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a38aca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b53798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55fae504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/project_training_data_with_class_labels.csv',dtype=str,delimiter=',',quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aac43a2-648e-452d-b522-a6ee121d573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = ps.DataFrame(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2705797d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(808661, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73aae4ff-3b29-4156-9a7e-d6a07f824017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 808661 entries, 0 to 808660\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   ID               808661 non-null  object\n",
      " 1   Comments         808623 non-null  object\n",
      " 2   Parent Comments  808661 non-null  object\n",
      " 3    Class Labels    808661 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 24.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06e4d84d-5562-4392-9796-16d3048bbfd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                  0\n",
       "Comments           38\n",
       "Parent Comments     0\n",
       " Class Labels       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for null value\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b97a58a0-dc97-4726-84f5-24d24a8e4191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Comments', 'Parent Comments', ' Class Labels '], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "455b7529-d811-4836-9276-79dd25994b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Comments', 'Parent Comments', 'Class Labels'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.rename(columns={' Class Labels ' : 'Class Labels'},inplace=True) # changing col name\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7705e14-dab0-4d2c-87a2-69d2e674df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove data points whose class labels are not present\n",
    "# df_train.dropna(subset=['Comments'], inplace=True)      # then dropped that rows with no value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bde0f58-8aa9-4d9a-a2bc-b991db3c0aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= df_train.replace(to_replace = np.nan, value = '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1747ae66-dffd-4370-a36a-60834afb8c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                 0\n",
       "Comments           0\n",
       "Parent Comments    0\n",
       "Class Labels       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for null value\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad5b66c8-adf1-4b61-aaf0-e8409cfabac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "      <th>Text</th>\n",
       "      <th>Class Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ocxtitan</td>\n",
       "      <td>Central Illinois</td>\n",
       "      <td>Jesus; where do you live?</td>\n",
       "      <td>Jesus; where do you live? Central Illinois</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LeChuckly</td>\n",
       "      <td>To think - CNN used to be the acronym synonymo...</td>\n",
       "      <td>Even The CNN Staff Is Sick Of The Wall-To-Wall...</td>\n",
       "      <td>Even The CNN Staff Is Sick Of The Wall-To-Wall...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>throwitskrub8</td>\n",
       "      <td>But then again; you have to consider that all ...</td>\n",
       "      <td>agree to that part.It can also mean that gujra...</td>\n",
       "      <td>agree to that part.It can also mean that gujra...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresherthanyouuu</td>\n",
       "      <td>ughhhhh</td>\n",
       "      <td>If a guy told you he doesn't use social media ...</td>\n",
       "      <td>If a guy told you he doesn't use social media ...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_kushagra</td>\n",
       "      <td>I should've put the</td>\n",
       "      <td>No; it's just a programming bug. After all; th...</td>\n",
       "      <td>No; it's just a programming bug. After all; th...</td>\n",
       "      <td>sarcastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                           Comments  \\\n",
       "0          ocxtitan                                   Central Illinois   \n",
       "1         LeChuckly  To think - CNN used to be the acronym synonymo...   \n",
       "2     throwitskrub8  But then again; you have to consider that all ...   \n",
       "3  fresherthanyouuu                                            ughhhhh   \n",
       "4         _kushagra                                I should've put the   \n",
       "\n",
       "                                     Parent Comments  \\\n",
       "0                          Jesus; where do you live?   \n",
       "1  Even The CNN Staff Is Sick Of The Wall-To-Wall...   \n",
       "2  agree to that part.It can also mean that gujra...   \n",
       "3  If a guy told you he doesn't use social media ...   \n",
       "4  No; it's just a programming bug. After all; th...   \n",
       "\n",
       "                                                Text   Class Labels  \n",
       "0         Jesus; where do you live? Central Illinois  non-sarcastic  \n",
       "1  Even The CNN Staff Is Sick Of The Wall-To-Wall...  non-sarcastic  \n",
       "2  agree to that part.It can also mean that gujra...  non-sarcastic  \n",
       "3  If a guy told you he doesn't use social media ...  non-sarcastic  \n",
       "4  No; it's just a programming bug. After all; th...      sarcastic  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserting the column before Class Labels col.\n",
    "df_train.insert(loc = 3,\n",
    "        column = 'Text',\n",
    "        value = df_train['Parent Comments'] + \" \" +df_train['Comments'])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bda1c5a8-d7b1-4975-bcbe-1b59dfa42ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(808661, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "600c0908-55f1-4260-8a67-607cd1eed83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we are combining Parent Comment and Comment cols. into one cols.\n",
    "# df_train['Text'] = df_train['Parent Comments'] + \" \" +df_train['Comments']\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe69be2f-6031-4e86-8da7-6c1131466b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                 0\n",
       "Comments           0\n",
       "Parent Comments    0\n",
       "Text               0\n",
       "Class Labels       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for null value\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f06c0da3-a92c-4636-9ff0-d761a162e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we know that adding any thing with nan value gives nan. so in Text col having nan value we replace \n",
    "# # one time with comment col. value where value of parent comment col is nan and other with parent comment where\n",
    "# # comment col. is empty.\n",
    "\n",
    "# # fill the rows of Col. Text having no values with that of col. Parent Comments where Comment col. is empty\n",
    "# df_train['Text'][df_train['Comments'].isnull()] = df_train['Parent Comments'][df_train['Comments'].isnull()]\n",
    "\n",
    "# # fill the rows of Col. Text having no values with that of col. Comments where Parent Comment col. is empty\n",
    "# df_train['Text'][df_train['Parent Comments'].isnull()] = df_train['Comments'][df_train['Parent Comments'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f25f346-4d50-44c2-a230-f57df4745ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to check for null value\n",
    "# df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40700151-3c17-46dd-8544-fba8d3b2e821",
   "metadata": {},
   "source": [
    "Since here there is issue with the format of some rows. So we are manually correcting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b9149b2-3de9-45bc-9c92-e38474afafca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theyoungthaddeus</td>\n",
       "      <td>No one \"needs\" an assault foam dart blaster</td>\n",
       "      <td>Your son has to register those at the county j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just_an_asian_here</td>\n",
       "      <td>Cause all attractive women are uninteresting a...</td>\n",
       "      <td>Likely due to creative and interesting content.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foxprowl</td>\n",
       "      <td>Poser.</td>\n",
       "      <td>Jon Stewart is going to HBO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kd7rzv</td>\n",
       "      <td>Won't be long and Anet will start banning peop...</td>\n",
       "      <td>This post looks like bullshit market manipulat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ellefied</td>\n",
       "      <td>There goes my hope that Kubo does a Kojima as ...</td>\n",
       "      <td>Plus the Japanese typically do not talk shit w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                           Comments  \\\n",
       "0    theyoungthaddeus        No one \"needs\" an assault foam dart blaster   \n",
       "1  Just_an_asian_here  Cause all attractive women are uninteresting a...   \n",
       "2            Foxprowl                                             Poser.   \n",
       "3              kd7rzv  Won't be long and Anet will start banning peop...   \n",
       "4            Ellefied  There goes my hope that Kubo does a Kojima as ...   \n",
       "\n",
       "                                    Parent Comments   \n",
       "0  Your son has to register those at the county j...  \n",
       "1    Likely due to creative and interesting content.  \n",
       "2                        Jon Stewart is going to HBO  \n",
       "3  This post looks like bullshit market manipulat...  \n",
       "4  Plus the Japanese typically do not talk shit w...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/project_test_data.csv',dtype=str,delimiter=',',quoting=3)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b051283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202166, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44d9bd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                   0\n",
       "Comments            15\n",
       "Parent Comments      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for null value\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "540c0fd4-2122-431d-a1e9-60e063c92f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test= df_test.replace(to_replace = np.nan, value = '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "183955a2-8175-474d-857e-b818155eccd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                  0\n",
       "Comments            0\n",
       "Parent Comments     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check for null value\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80f774ce-01d9-497f-960d-617046eeb630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Comments', 'Parent Comments '], dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d3a61bd-9873-4723-9791-b0e31ef3f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.rename(columns={'Parent Comments ': 'Parent Comments'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3306398f-bbb9-4ad9-bddc-9388ebaf06b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Comments', 'Parent Comments'], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97354327-3255-4122-8785-bed9ae434166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theyoungthaddeus</td>\n",
       "      <td>No one \"needs\" an assault foam dart blaster</td>\n",
       "      <td>Your son has to register those at the county j...</td>\n",
       "      <td>Your son has to register those at the county j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just_an_asian_here</td>\n",
       "      <td>Cause all attractive women are uninteresting a...</td>\n",
       "      <td>Likely due to creative and interesting content.</td>\n",
       "      <td>Likely due to creative and interesting content...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foxprowl</td>\n",
       "      <td>Poser.</td>\n",
       "      <td>Jon Stewart is going to HBO</td>\n",
       "      <td>Jon Stewart is going to HBO Poser.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kd7rzv</td>\n",
       "      <td>Won't be long and Anet will start banning peop...</td>\n",
       "      <td>This post looks like bullshit market manipulat...</td>\n",
       "      <td>This post looks like bullshit market manipulat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ellefied</td>\n",
       "      <td>There goes my hope that Kubo does a Kojima as ...</td>\n",
       "      <td>Plus the Japanese typically do not talk shit w...</td>\n",
       "      <td>Plus the Japanese typically do not talk shit w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                           Comments  \\\n",
       "0    theyoungthaddeus        No one \"needs\" an assault foam dart blaster   \n",
       "1  Just_an_asian_here  Cause all attractive women are uninteresting a...   \n",
       "2            Foxprowl                                             Poser.   \n",
       "3              kd7rzv  Won't be long and Anet will start banning peop...   \n",
       "4            Ellefied  There goes my hope that Kubo does a Kojima as ...   \n",
       "\n",
       "                                     Parent Comments  \\\n",
       "0  Your son has to register those at the county j...   \n",
       "1    Likely due to creative and interesting content.   \n",
       "2                        Jon Stewart is going to HBO   \n",
       "3  This post looks like bullshit market manipulat...   \n",
       "4  Plus the Japanese typically do not talk shit w...   \n",
       "\n",
       "                                                Text  \n",
       "0  Your son has to register those at the county j...  \n",
       "1  Likely due to creative and interesting content...  \n",
       "2                 Jon Stewart is going to HBO Poser.  \n",
       "3  This post looks like bullshit market manipulat...  \n",
       "4  Plus the Japanese typically do not talk shit w...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are combining Parent Comment and Comment cols. into one cols.\n",
    "df_test['Text'] = df_test['Parent Comments'] + \" \" +df_test['Comments']\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "310e11f6-981b-4262-badc-e772918f5fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['Class Labels'].value_counts().plot(kind = 'bar',rot = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47d4c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# english Stopwords\n",
    "# stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c3aef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # used this dictionary for expanding conracted words. this is taken from Github\n",
    "# CONTRACTIONS = {\n",
    "#     \"I'm\": \"I am\",\n",
    "#     \"I'm'a\": \"I am about to\",\n",
    "#     \"I'm'o\": \"I am going to\",\n",
    "#     \"I've\": \"I have\",\n",
    "#     \"I'll\": \"I will\",\n",
    "#     \"I'll've\": \"I will have\",\n",
    "#     \"I'd\": \"I would\",\n",
    "#     \"I'd've\": \"I would have\",\n",
    "#     \"Whatcha\": \"What are you\",\n",
    "#     \"amn't\": \"am not\",\n",
    "#     \"ain't\": \"are not\",\n",
    "#     \"aren't\": \"are not\",\n",
    "#     \"'cause\": \"because\",\n",
    "#     \"can't\": \"cannot\",\n",
    "#     \"can't've\": \"cannot have\",\n",
    "#     \"could've\": \"could have\",\n",
    "#     \"couldn't\": \"could not\",\n",
    "#     \"couldn't've\": \"could not have\",\n",
    "#     \"daren't\": \"dare not\",\n",
    "#     \"daresn't\": \"dare not\",\n",
    "#     \"dasn't\": \"dare not\",\n",
    "#     \"didn't\": \"did not\",\n",
    "#     \"didn’t\": \"did not\",\n",
    "#     \"don't\": \"do not\",\n",
    "#     \"don’t\": \"do not\",\n",
    "#     \"doesn't\": \"does not\",\n",
    "#     \"e'er\": \"ever\",\n",
    "#     \"everyone's\": \"everyone is\",\n",
    "#     \"finna\": \"fixing to\",\n",
    "#     \"gimme\": \"give me\",\n",
    "#     \"gon't\": \"go not\",\n",
    "#     \"gonna\": \"going to\",\n",
    "#     \"gotta\": \"got to\",\n",
    "#     \"hadn't\": \"had not\",\n",
    "#     \"hadn't've\": \"had not have\",\n",
    "#     \"hasn't\": \"has not\",\n",
    "#     \"haven't\": \"have not\",\n",
    "#     \"he've\": \"he have\",\n",
    "#     \"he's\": \"he is\",\n",
    "#     \"he'll\": \"he will\",\n",
    "#     \"he'll've\": \"he will have\",\n",
    "#     \"he'd\": \"he would\",\n",
    "#     \"he'd've\": \"he would have\",\n",
    "#     \"here's\": \"here is\",\n",
    "#     \"how're\": \"how are\",\n",
    "#     \"how'd\": \"how did\",\n",
    "#     \"how'd'y\": \"how do you\",\n",
    "#     \"how's\": \"how is\",\n",
    "#     \"how'll\": \"how will\",\n",
    "#     \"isn't\": \"is not\",\n",
    "#     \"it's\": \"it is\",\n",
    "#     \"'tis\": \"it is\",\n",
    "#     \"'twas\": \"it was\",\n",
    "#     \"it'll\": \"it will\",\n",
    "#     \"it'll've\": \"it will have\",\n",
    "#     \"it'd\": \"it would\",\n",
    "#     \"it'd've\": \"it would have\",\n",
    "#     \"kinda\": \"kind of\",\n",
    "#     \"let's\": \"let us\",\n",
    "#     \"luv\": \"love\",\n",
    "#     \"ma'am\": \"madam\",\n",
    "#     \"may've\": \"may have\",\n",
    "#     \"mayn't\": \"may not\",\n",
    "#     \"might've\": \"might have\",\n",
    "#     \"mightn't\": \"might not\",\n",
    "#     \"mightn't've\": \"might not have\",\n",
    "#     \"must've\": \"must have\",\n",
    "#     \"mustn't\": \"must not\",\n",
    "#     \"mustn't've\": \"must not have\",\n",
    "#     \"needn't\": \"need not\",\n",
    "#     \"needn't've\": \"need not have\",\n",
    "#     \"ne'er\": \"never\",\n",
    "#     \"o'\": \"of\",\n",
    "#     \"o'clock\": \"of the clock\",\n",
    "#     \"ol'\": \"old\",\n",
    "#     \"oughtn't\": \"ought not\",\n",
    "#     \"oughtn't've\": \"ought not have\",\n",
    "#     \"o'er\": \"over\",\n",
    "#     \"shan't\": \"shall not\",\n",
    "#     \"sha'n't\": \"shall not\",\n",
    "#     \"shalln't\": \"shall not\",\n",
    "#     \"shan't've\": \"shall not have\",\n",
    "#     \"she's\": \"she is\",\n",
    "#     \"she'll\": \"she will\",\n",
    "#     \"she'd\": \"she would\",\n",
    "#     \"she'd've\": \"she would have\",\n",
    "#     \"should've\": \"should have\",\n",
    "#     \"shouldn't\": \"should not\",\n",
    "#     \"shouldn't've\": \"should not have\",\n",
    "#     \"so've\": \"so have\",\n",
    "#     \"so's\": \"so is\",\n",
    "#     \"somebody's\": \"somebody is\",\n",
    "#     \"someone's\": \"someone is\",\n",
    "#     \"something's\": \"something is\",\n",
    "#     \"sux\": \"sucks\",\n",
    "#     \"that're\": \"that are\",\n",
    "#     \"that's\": \"that is\",\n",
    "#     \"that'll\": \"that will\",\n",
    "#     \"that'd\": \"that would\",\n",
    "#     \"that'd've\": \"that would have\",\n",
    "#     \"em\": \"them\",\n",
    "#     \"there're\": \"there are\",\n",
    "#     \"there's\": \"there is\",\n",
    "#     \"there'll\": \"there will\",\n",
    "#     \"there'd\": \"there would\",\n",
    "#     \"there'd've\": \"there would have\",\n",
    "#     \"these're\": \"these are\",\n",
    "#     \"they're\": \"they are\",\n",
    "#     \"they've\": \"they have\",\n",
    "#     \"they'll\": \"they will\",\n",
    "#     \"they'll've\": \"they will have\",\n",
    "#     \"they'd\": \"they would\",\n",
    "#     \"they'd've\": \"they would have\",\n",
    "#     \"this's\": \"this is\",\n",
    "#     \"those're\": \"those are\",\n",
    "#     \"to've\": \"to have\",\n",
    "#     \"wanna\": \"want to\",\n",
    "#     \"wasn't\": \"was not\",\n",
    "#     \"we're\": \"we are\",\n",
    "#     \"we've\": \"we have\",\n",
    "#     \"we'll\": \"we will\",\n",
    "#     \"we'll've\": \"we will have\",\n",
    "#     \"we'd\": \"we would\",\n",
    "#     \"we'd've\": \"we would have\",\n",
    "#     \"weren't\": \"were not\",\n",
    "#     \"what're\": \"what are\",\n",
    "#     \"what'd\": \"what did\",\n",
    "#     \"what've\": \"what have\",\n",
    "#     \"what's\": \"what is\",\n",
    "#     \"what'll\": \"what will\",\n",
    "#     \"what'll've\": \"what will have\",\n",
    "#     \"when've\": \"when have\",\n",
    "#     \"when's\": \"when is\",\n",
    "#     \"where're\": \"where are\",\n",
    "#     \"where'd\": \"where did\",\n",
    "#     \"where've\": \"where have\",\n",
    "#     \"where's\": \"where is\",\n",
    "#     \"which's\": \"which is\",\n",
    "#     \"who're\": \"who are\",\n",
    "#     \"who've\": \"who have\",\n",
    "#     \"who's\": \"who is\",\n",
    "#     \"who'll\": \"who will\",\n",
    "#     \"who'll've\": \"who will have\",\n",
    "#     \"who'd\": \"who would\",\n",
    "#     \"who'd've\": \"who would have\",\n",
    "#     \"why're\": \"why are\",\n",
    "#     \"why'd\": \"why did\",\n",
    "#     \"why've\": \"why have\",\n",
    "#     \"why's\": \"why is\",\n",
    "#     \"will've\": \"will have\",\n",
    "#     \"won't\": \"will not\",\n",
    "#     \"won't've\": \"will not have\",\n",
    "#     \"would've\": \"would have\",\n",
    "#     \"wouldn't\": \"would not\",\n",
    "#     \"wouldn't've\": \"would not have\",\n",
    "#     \"y'all\": \"you all\",\n",
    "#     \"y'all're\": \"you all are\",\n",
    "#     \"y'all've\": \"you all have\",\n",
    "#     \"y'all'd\": \"you all would\",\n",
    "#     \"y'all'd've\": \"you all would have\",\n",
    "#     \"you're\": \"you are\",\n",
    "#     \"you've\": \"you have\",\n",
    "#     \"you'll've\": \"you shall have\",\n",
    "#     \"you'll\": \"you will\",\n",
    "#     \"you'd\": \"you would\",\n",
    "#     \"you'd've\": \"you would have\"\n",
    "#  }\n",
    "# with open('CONTRACTIONS.pkl', 'wb') as f:\n",
    "#     pickle.dump(CONTRACTIONS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "733d0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #I created a dictionary for emoticons\n",
    "# EMOTICONS = {\n",
    "#     u\":‑)\":\"Happy\",\n",
    "#     u\":-))\":\"Very Happy\",\n",
    "#     u\":-)))\":\"Very very Happy\",\n",
    "#     u\":)\":\"Happy\",\n",
    "#     u\":))\":\"Very Happy\",\n",
    "#     u\":)))\":\"Very very Happy\",\n",
    "#     u\":-]\":\"Happy\",\n",
    "#     u\":]\":\"Happy\",\n",
    "#     u\":-3\":\"Happy\",\n",
    "#     u\":3\":\"Happy\",\n",
    "#     u\":->\":\"Happy\",\n",
    "#     u\":>\":\"Happy\",\n",
    "#     u\"8-)\":\"Happy\",\n",
    "#     u\":o)\":\"Happy\",\n",
    "#     u\":-}\":\"Happy\",\n",
    "#     u\":}\":\"Happy\",\n",
    "#     u\":-)\":\"Happy\",\n",
    "#     u\":c)\":\"Happy\",\n",
    "#     u\":^)\":\"Happy\",\n",
    "#     u\"=]\":\"Happy\",\n",
    "#     u\"=)\":\"Happy\",\n",
    "#     u\":‑D\":\"Laughing\",\n",
    "#     u\":D\":\"Laughing\",\n",
    "#     u\"8‑D\":\"Laughing\",\n",
    "#     u\"8D\":\"Laughing\",\n",
    "#     u\"X‑D\":\"Laughing\",\n",
    "#     u\"XD\":\"Laughing\",\n",
    "#     u\"=D\":\"Laughing\",\n",
    "#     u\"=3\":\"Laughing\",\n",
    "#     u\"B^D\":\"Laughing\",\n",
    "#     u\":-))\":\"Very happy\",\n",
    "#     u\"<3\":\"love\",\n",
    "#     u\":-(\":\"sad\",\n",
    "#     u\":‑(\":\"sad\",\n",
    "#     u\":(\":\"sad\",\n",
    "#     u\":‑c\":\"sad\",\n",
    "#     u\":c\":\"sad\",\n",
    "#     u\":‑<\":\"sad\",\n",
    "#     u\":<\":\"sad\",\n",
    "#     u\":‑[\":\"sad\",\n",
    "#     u\":[\":\"sad\",\n",
    "#     u\":-||\":\"sad\",\n",
    "#     u\">:[\":\"sad\",\n",
    "#     u\":{\":\"sad\",\n",
    "#     u\":@\":\"sad\",\n",
    "#     u\">:(\":\"sad\",\n",
    "#     u\":'‑(\":\"Crying\",\n",
    "#     u\":'(\":\"Crying\",\n",
    "#     u\":'‑)\":\"Tears of happiness\",\n",
    "#     u\":')\":\"Tears of happiness\",\n",
    "#     u\"D‑':\":\"sad\",\n",
    "#     u\"D:<\":\"sad\",\n",
    "#     u\"D:\":\"sad\",\n",
    "#     u\"D8\":\"very sad\",\n",
    "#     u\"D;\":\"very sad\",\n",
    "#     u\"D=\":\"very sad\",\n",
    "#     u\"DX\":\"very sad\",\n",
    "#     u\":‑O\":\"Surprise\",\n",
    "#     u\":O\":\"Surprise\",\n",
    "#     u\":‑o\":\"Surprise\",\n",
    "#     u\":o\":\"Surprise\",\n",
    "#     u\":-0\":\"Sad\",\n",
    "#     u\"8‑0\":\"Yawn\",\n",
    "#     u\">:O\":\"Yawn\",\n",
    "#     u\":-*\":\"Kiss\",\n",
    "#     u\":*\":\"Kiss\",\n",
    "#     u\":X\":\"Kiss\",\n",
    "#     u\";‑)\":\"Wink\",\n",
    "#     u\";)\":\"Wink\",\n",
    "#     u\"*-)\":\"Wink\",\n",
    "#     u\"*)\":\"Wink\",\n",
    "#     u\";‑]\":\"Wink\",\n",
    "#     u\";]\":\"Wink\",\n",
    "#     u\";^)\":\"Wink\",\n",
    "#     u\":‑,\":\"Wink\",\n",
    "#     u\";D\":\"Wink\",\n",
    "#     u\":‑P\":\"fun\",\n",
    "#     u\":P\":\"fun\",\n",
    "#     u\"X‑P\":\"fun\",\n",
    "#     u\"XP\":\"fun\",\n",
    "#     u\":‑Þ\":\"fun\",\n",
    "#     u\":Þ\":\"fun\",\n",
    "#     u\":b\":\"fun\",\n",
    "#     u\"d:\":\"fun\",\n",
    "#     u\"=p\":\"fun\",\n",
    "#     u\">:P\":\"fun\",\n",
    "#     u\":‑/\":\"annoyed\",\n",
    "#     u\":/\":\"annoyed\",\n",
    "#     u\":-[.]\":\"annoyed\",\n",
    "#     u\">:[(\\)]\":\"annoyed\",\n",
    "#     u\">:/\":\"annoyed\",\n",
    "#     u\":[(\\)]\":\"annoyed\",\n",
    "#     u\"=/\":\"annoyed\",\n",
    "#     u\"=[(\\)]\":\"annoyed\",\n",
    "#     u\":L\":\"annoyed\",\n",
    "#     u\"=L\":\"annoyed\",\n",
    "#     u\":S\":\"annoyed\",\n",
    "#     u\":‑|\":\"indecision\",\n",
    "#     u\":|\":\"indecision\",\n",
    "#     u\":$\":\"Embarrassed\",\n",
    "#     u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "#     u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "#     u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "#     u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "#     u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "#     u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "#     u\"O:‑)\":\"Angel\",\n",
    "#     u\"O:)\":\"Angel\",\n",
    "#     u\"0:‑3\":\"Angel\",\n",
    "#     u\"0:3\":\"Angel\",\n",
    "#     u\"0:‑)\":\"Angel\",\n",
    "#     u\"0:)\":\"Angel\",\n",
    "#     u\":‑b\":\"fun\",\n",
    "#     u\"0;^)\":\"Angel\",\n",
    "#     u\">:‑)\":\"devilish\",\n",
    "#     u\">:)\":\"devilish\",\n",
    "#     u\"}:‑)\":\"devilish\",\n",
    "#     u\"}:)\":\"devilish\",\n",
    "#     u\"3:‑)\":\"devilish\",\n",
    "#     u\"3:)\":\"devilish\",\n",
    "#     u\">;)\":\"devilish\",\n",
    "#     u\"|;‑)\":\"Cool\",\n",
    "#     u\"|‑O\":\"Bored\",\n",
    "#     u\":‑J\":\"Tongue in cheek\",\n",
    "#     u\"#‑)\":\"Party all night\",\n",
    "#     u\"%‑)\":\"confused\",\n",
    "#     u\"%)\":\"confused\",\n",
    "#     u\":-###..\":\"Being sick\",\n",
    "#     u\":###..\":\"Being sick\",\n",
    "#     u\"<:‑|\":\"silent\",\n",
    "#     u\"(>_<)\":\"Troubled\",\n",
    "#     u\"(>_<)>\":\"Troubled\",\n",
    "#     u\"(';')\":\"Baby\",\n",
    "#     u\"(^^>``\":\"Nervous\",\n",
    "#     u\"(^_^;)\":\"Troubled\",\n",
    "#     u\"(-_-;)\":\"Nervous\",\n",
    "#     u\"(~_~;) (・.・;)\":\"Shy\",\n",
    "#     u\"(-_-)zzz\":\"Sleeping\",\n",
    "#     u\"(^_-)\":\"Wink\",\n",
    "#     u\"((+_+))\":\"Confused\",\n",
    "#     u\"(+o+)\":\"Confused\",\n",
    "#     u\"(o|o)\":\"Ultraman\",\n",
    "#     u\"^_^\":\"happy\",\n",
    "#     u\"(^_^)/\":\"happy\",\n",
    "#     u\"(^O^)／\":\"happy\",\n",
    "#     u\"(^o^)／\":\"happy\",\n",
    "#     u\"(__)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "#     u\"_(._.)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "#     u\"<(_ _)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "#     u\"<m(__)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "#     u\"m(__)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "#     u\"m(_ _)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "#     u\"('_')\":\"Sad\",\n",
    "#     u\"(/_;)\":\"Sad\",\n",
    "#     u\"(T_T) (;_;)\":\"Sad\",\n",
    "#     u\"(;_;\":\"Sad\",\n",
    "#     u\"(;_:)\":\"Sad\",\n",
    "#     u\"(;O;)\":\"Sad\",\n",
    "#     u\"(:_;)\":\"Sad\",\n",
    "#     u\"(ToT)\":\"Sad\",\n",
    "#     u\";_;\":\"Sad\",\n",
    "#     u\";-;\":\"Sad\",\n",
    "#     u\";n;\":\"Sad\",\n",
    "#     u\";;\":\"Sad\",\n",
    "#     u\"Q.Q\":\"Sad\",\n",
    "#     u\"T.T\":\"Sad\",\n",
    "#     u\"QQ\":\"Sad\",\n",
    "#     u\"Q_Q\":\"Sad\",\n",
    "#     u\"(-.-)\":\"Shame\",\n",
    "#     u\"(-_-)\":\"Shame\",\n",
    "#     u\"(一一)\":\"Shame\",\n",
    "#     u\"(；一_一)\":\"Shame\",\n",
    "#     u\"(=_=)\":\"Tired\",\n",
    "#     u\"(=^·^=)\":\"cat\",\n",
    "#     u\"(=^··^=)\":\"cat\",\n",
    "#     u\"=_^= \":\"cat\",\n",
    "#     u\"(..)\":\"Looking down\",\n",
    "#     u\"(._.)\":\"Looking down\",\n",
    "#     u\"^m^\":\"Giggling\",\n",
    "#     u\"(・・?\":\"Confusion\",\n",
    "#     u\"(?_?)\":\"Confusion\",\n",
    "#     u\">^_^<\":\"Normal Laugh\",\n",
    "#     u\"<^!^>\":\"Normal Laugh\",\n",
    "#     u\"^/^\":\"Normal Laugh\",\n",
    "#     u\"（*^_^*）\" :\"Normal Laugh\",\n",
    "#     u\"(^<^) (^.^)\":\"Normal Laugh\",\n",
    "#     u\"(^^)\":\"Normal Laugh\",\n",
    "#     u\"(^.^)\":\"Normal Laugh\",\n",
    "#     u\"(^_^.)\":\"Normal Laugh\",\n",
    "#     u\"(^_^)\":\"Normal Laugh\",\n",
    "#     u\"(^^)\":\"Normal Laugh\",\n",
    "#     u\"(^J^)\":\"Normal Laugh\",\n",
    "#     u\"(*^.^*)\":\"Normal Laugh\",\n",
    "#     u\"(^—^）\":\"Normal Laugh\",\n",
    "#     u\"(#^.^#)\":\"Normal Laugh\",\n",
    "#     u\"（^—^）\":\"Waving\",\n",
    "#     u\"(;_;)/~~~\":\"Waving\",\n",
    "#     u\"(^.^)/~~~\":\"Waving\",\n",
    "#     u\"(-_-)/~~~ ($··)/~~~\":\"Waving\",\n",
    "#     u\"(T_T)/~~~\":\"Waving\",\n",
    "#     u\"(ToT)/~~~\":\"Waving\",\n",
    "#     u\"(*^0^*)\":\"Excited\",\n",
    "#     u\"(*_*)\":\"Excited\",\n",
    "#     u\"(*_*;\":\"Excited\",\n",
    "#     u\"(+_+) (@_@)\":\"Excited\",\n",
    "#     u\"(*^^)v\":\"Cheerful\",\n",
    "#     u\"(^_^)v\":\"Cheerful\",\n",
    "#     u\"((d[-_-]b))\":\"Headphones,Listening to music\",\n",
    "#     u'(-\"-)':\"Worried\",\n",
    "#     u\"(ーー;)\":\"Worried\",\n",
    "#     u\"(^0_0^)\":\"Eyeglasses\",\n",
    "#     u\"(＾ｖ＾)\":\"Happy\",\n",
    "#     u\"(＾ｕ＾)\":\"Happy\",\n",
    "#     u\"(^)o(^)\":\"Happy\",\n",
    "#     u\"(^O^)\":\"Happy\",\n",
    "#     u\"(^o^)\":\"Happy\",\n",
    "#     u\")^o^(\":\"Happy\",\n",
    "#     u\":O o_O\":\"Surprised\",\n",
    "#     u\"o_0\":\"Surprised\",\n",
    "#     u\"o.O\":\"Surpised\",\n",
    "#     u\"(o.o)\":\"Surprised\",\n",
    "#     u\"oO\":\"Surprised\",\n",
    "#     u\"(*￣m￣)\":\"Dissatisfied\",\n",
    "#     u\"(‘A`)\":\"Deflated\"\n",
    "\n",
    "# }\n",
    "# with open('EMOTICONS.pkl', 'wb') as f:\n",
    "#     pickle.dump(EMOTICONS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "953b1a10-4146-410f-bff3-274559c8b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # english Stopwords\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    with open('EMOTICONS.pkl', 'rb') as f1:\n",
    "        EMOTICONS = pickle.load(f1)\n",
    "    with open('CONTRACTIONS.pkl','rb') as f2:\n",
    "        CONTRACTIONS = pickle.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e0b3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(tweet):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pickle\n",
    "    import time\n",
    "    import spacy\n",
    "    import nltk\n",
    "    import re   # regular expression\n",
    "    import string\n",
    "    import multiprocessing\n",
    "    from multiprocessing import Pool\n",
    "    import scipy.sparse as sp\n",
    "    import joblib\n",
    "    import emoji\n",
    "    from autocorrect import Speller   # for correcting spelling\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize    # for tokenizing string into words\n",
    "    from nltk.stem import WordNetLemmatizer    # for lemmatizing words\n",
    "    from nltk.tag import pos_tag # for tagging words with their parts of speech (POS)\n",
    "    \n",
    "    \n",
    "    # english Stopwords\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    with open('EMOTICONS.pkl', 'rb') as f1:\n",
    "        EMOTICONS = pickle.load(f1)\n",
    "    with open('CONTRACTIONS.pkl','rb') as f2:\n",
    "        CONTRACTIONS = pickle.load(f2)\n",
    "    \n",
    "    nan_tweet = 'NaN'  \n",
    "    # this code is to short unnecessary sentence, bec. some rows has unnecessary long repeated characters\n",
    "    # like 'HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH'\n",
    "    # we manually decide len = 10 any with len >10 is discarded\n",
    "        # convert all text lowercase\n",
    "    tweet = tweet.lower() \n",
    "    tweet = tweet.split()\n",
    "    tw = []\n",
    "    for t in tweet:\n",
    "        # removing digits only\n",
    "        if t.isnumeric():\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "        if len(t)<=20:\n",
    "            if len(set(t))<=1:\n",
    "                continue\n",
    "            if sum(c.isdigit() for c in t) > sum(c.isalpha() for c in t):\n",
    "                continue\n",
    "            tw.append(t)\n",
    "    tweet = ' '.join(tw)   \n",
    "    # remove any urls\n",
    "    tweet = re.sub(r\"www\\S+|http\\S+|\", \"\",tweet, flags = re.MULTILINE)\n",
    "    # remove square bracket including its content if\n",
    "    tweet = re.sub(r'\\[|\\]',\" \",tweet)\n",
    "    # to remove new line character\n",
    "    tweet = re.sub(r'\\n', \" \", tweet)\n",
    "    # remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r\"\\@\\w+|\\#\", \"\",tweet)\n",
    "    # replace emojis with its meaning\n",
    "    tweet = (emoji.demojize(tweet, delimiters=(\" \", \"\"))).replace('_',' ')\n",
    "    # expand contractions\n",
    "    splitted_string = tweet.split()\n",
    "    for index, text in enumerate(splitted_string):\n",
    "        if text in CONTRACTIONS.keys():\n",
    "            splitted_string[index] = CONTRACTIONS[text]\n",
    "    tweet = ' '.join(splitted_string)\n",
    "    # replace emoticons with its meaning\n",
    "    splitted_tweet = tweet.split()\n",
    "    for index, word in enumerate(splitted_tweet):\n",
    "        if word in EMOTICONS.keys():\n",
    "            splitted_tweet[index] = EMOTICONS[word]\n",
    "    tweet = ' '.join(splitted_tweet)\n",
    "    # remove tags\n",
    "    tweet = re.sub(re.compile('<.*?>'), '', tweet)\n",
    "    # remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    # remove stopwords\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [word for word in tweet_tokens if word not in stopwords_list]\n",
    "    # spelling correction\n",
    "    correct_words = []\n",
    "    # initialize Speller object for english language\n",
    "    spell_corrector = Speller(lang='en')\n",
    "    for word in filtered_words:\n",
    "        correct_word = spell_corrector(word)\n",
    "        correct_words.append(correct_word)\n",
    "    # lemmatizing\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemma_words = []\n",
    "    for word, tag in pos_tag(correct_words):      # Part-of-speech constants for ADJ,VERB,ADV = 'a', 'r', 'v'\n",
    "        if tag.startswith('JJ'):      # for adjectives\n",
    "            lemma_word = wnl.lemmatize(word, pos='a')\n",
    "            lemma_words.append(lemma_word)\n",
    "        elif tag.startswith('VB'):   # for verbs\n",
    "            lemma_word = wnl.lemmatize(word, pos='v')\n",
    "            lemma_words.append(lemma_word)\n",
    "        elif tag.startswith('RB'):   # for adverbs\n",
    "            lemma_word = wnl.lemmatize(word, pos='r')\n",
    "            lemma_words.append(lemma_word)\n",
    "        else:\n",
    "            lemma_word = word\n",
    "            lemma_words.append(lemma_word)\n",
    "        \n",
    "    tweet = \" \".join(lemma_words)\n",
    "    if len(tweet) == 0:   # if after pre-processing sent. has no letter\n",
    "        tweet = nan_tweet\n",
    "    # f1.close(),f2.close()\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e907f43f-0068-454f-93eb-598387bf5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_parallel_apply import DataFrameParallel, apply_on_df_col_parallel, apply_on_series_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f532bd-0659-4cee-8e25-43afe9208339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35da4c1c-eab1-47ff-83d9-8d5f059f6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocesspandas import applyparallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2362936a-0e68-4735-a5db-fab88779f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['Text'][0:100].apply_parallel(preprocess_text, num_processes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da41a61d-1607-4781-b0a1-f40a4c29a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DataFrameParallel(df_train, n_cores=4 , pbar=True)['Text'].apply(preprocess_text)\n",
    "# r = apply_on_df_col_parallel(df_train[0:10], 'Text', preprocess_text, 4, pbar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67408d83-8d1f-4f03-8dda-dbc963fb4b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "294ff223-35c5-474b-8116-d8d811757a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "# 24000:250000 error somewhere\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True,nb_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be66bb51-1077-492d-b3c1-33f83e40e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_cores = multiprocessing.cpu_count()\n",
    "# num_partitions = num_cores-2 # leave some cores for other processes\n",
    "# print(num_partitions)\n",
    "\n",
    "# def parallelize_dataframe(df, func):\n",
    "#     a = np.array_split(df, num_partitions)\n",
    "#     del df\n",
    "#     pool = Pool(num_cores)\n",
    "#     df = sp.vstack(pool.map(func,a), format='csr')\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "#     return df\n",
    "\n",
    "# # def test_func2(data,cv=CV,train=False):\n",
    "# #     X_bow = cv.transform(data)\n",
    "# #     return X_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d7907df6-49b0-41a0-8ef4-62c396989d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize_dataframe(df_train['Text'][0:100],preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b45ff584-3db6-4f3c-a135-18cfe85f5c77",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328226dc016f49ca9fa8bf4a7410dfb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=134777), Label(value='0 / 134777')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27843.700672388077 sec\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "xx = df_train['Text'].parallel_apply(preprocess_text) \n",
    "df_train.insert(loc = 4,\n",
    "        column = 'Pre Processed Text',\n",
    "        value = xx)\n",
    "# saving  \n",
    "df_train.to_csv('processed_train.csv',index=False) # on train data, \n",
    "tt = time.time() - t\n",
    "print(f'{tt} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6eb80e-37f9-4828-9789-964919857a82",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5266970bf866421c909676a19ee831d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=101083), Label(value='0 / 101083')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = time.time()\n",
    "xtt = df_test['Text'].parallel_apply(preprocess_text) # on test data\n",
    "df_test.insert(loc = 4,\n",
    "        column = 'Pre Processed Text',\n",
    "        value = xtt)\n",
    "df_test.to_csv('processed_test.csv',index=False)\n",
    "tt = time.time() - t\n",
    "print(f'{tt} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68dae800-7f7e-4528-afeb-a51c7e107f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train, df_test, xx, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf1af5-6d72-4b48-970c-be78d5fa5d6e",
   "metadata": {},
   "source": [
    "### Reading the pre-processed csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "187c8f51-cbc9-4e9e-aa0a-2bdbf7351295",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro_train = pd.read_csv('processed_train.csv',na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14f1bd31-af72-4b98-a42c-4bc17027e221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808661, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pre Processed Text</th>\n",
       "      <th>Class Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ocxtitan</td>\n",
       "      <td>Central Illinois</td>\n",
       "      <td>Jesus; where do you live?</td>\n",
       "      <td>Jesus; where do you live? Central Illinois</td>\n",
       "      <td>jesus live central illinois</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LeChuckly</td>\n",
       "      <td>To think - CNN used to be the acronym synonymo...</td>\n",
       "      <td>Even The CNN Staff Is Sick Of The Wall-To-Wall...</td>\n",
       "      <td>Even The CNN Staff Is Sick Of The Wall-To-Wall...</td>\n",
       "      <td>even cnn staff sick walltowall trump coverage ...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>throwitskrub8</td>\n",
       "      <td>But then again; you have to consider that all ...</td>\n",
       "      <td>agree to that part.It can also mean that gujra...</td>\n",
       "      <td>agree to that part.It can also mean that gujra...</td>\n",
       "      <td>agree parti also mean gujarat husbands good su...</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresherthanyouuu</td>\n",
       "      <td>ughhhhh</td>\n",
       "      <td>If a guy told you he doesn't use social media ...</td>\n",
       "      <td>If a guy told you he doesn't use social media ...</td>\n",
       "      <td>guy tell use social media go head ughhhhh</td>\n",
       "      <td>non-sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_kushagra</td>\n",
       "      <td>I should've put the</td>\n",
       "      <td>No; it's just a programming bug. After all; th...</td>\n",
       "      <td>No; it's just a programming bug. After all; th...</td>\n",
       "      <td>program bug android app music service still be...</td>\n",
       "      <td>sarcastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                           Comments  \\\n",
       "0          ocxtitan                                   Central Illinois   \n",
       "1         LeChuckly  To think - CNN used to be the acronym synonymo...   \n",
       "2     throwitskrub8  But then again; you have to consider that all ...   \n",
       "3  fresherthanyouuu                                            ughhhhh   \n",
       "4         _kushagra                                I should've put the   \n",
       "\n",
       "                                     Parent Comments  \\\n",
       "0                          Jesus; where do you live?   \n",
       "1  Even The CNN Staff Is Sick Of The Wall-To-Wall...   \n",
       "2  agree to that part.It can also mean that gujra...   \n",
       "3  If a guy told you he doesn't use social media ...   \n",
       "4  No; it's just a programming bug. After all; th...   \n",
       "\n",
       "                                                Text  \\\n",
       "0         Jesus; where do you live? Central Illinois   \n",
       "1  Even The CNN Staff Is Sick Of The Wall-To-Wall...   \n",
       "2  agree to that part.It can also mean that gujra...   \n",
       "3  If a guy told you he doesn't use social media ...   \n",
       "4  No; it's just a programming bug. After all; th...   \n",
       "\n",
       "                                  Pre Processed Text   Class Labels  \n",
       "0                        jesus live central illinois  non-sarcastic  \n",
       "1  even cnn staff sick walltowall trump coverage ...  non-sarcastic  \n",
       "2  agree parti also mean gujarat husbands good su...  non-sarcastic  \n",
       "3          guy tell use social media go head ughhhhh  non-sarcastic  \n",
       "4  program bug android app music service still be...      sarcastic  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_prepro_train.shape)\n",
    "df_prepro_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e410647c-a815-4cd3-9c87-aead0497a215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                    0\n",
       "Comments              0\n",
       "Parent Comments       0\n",
       "Text                  0\n",
       "Pre Processed Text    0\n",
       "Class Labels          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c725b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202166, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Parent Comments</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pre Processed Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theyoungthaddeus</td>\n",
       "      <td>No one \"needs\" an assault foam dart blaster</td>\n",
       "      <td>Your son has to register those at the county j...</td>\n",
       "      <td>Your son has to register those at the county j...</td>\n",
       "      <td>son register county jungle gym mags hold ounce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just_an_asian_here</td>\n",
       "      <td>Cause all attractive women are uninteresting a...</td>\n",
       "      <td>Likely due to creative and interesting content.</td>\n",
       "      <td>Likely due to creative and interesting content...</td>\n",
       "      <td>likely due creative interesting content cause ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foxprowl</td>\n",
       "      <td>Poser.</td>\n",
       "      <td>Jon Stewart is going to HBO</td>\n",
       "      <td>Jon Stewart is going to HBO Poser.</td>\n",
       "      <td>jon stewart go hbo power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kd7rzv</td>\n",
       "      <td>Won't be long and Anet will start banning peop...</td>\n",
       "      <td>This post looks like bullshit market manipulat...</td>\n",
       "      <td>This post looks like bullshit market manipulat...</td>\n",
       "      <td>post looks like bullshit market manipulation w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ellefied</td>\n",
       "      <td>There goes my hope that Kubo does a Kojima as ...</td>\n",
       "      <td>Plus the Japanese typically do not talk shit w...</td>\n",
       "      <td>Plus the Japanese typically do not talk shit w...</td>\n",
       "      <td>plus japanese typically talk shit come busines...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                           Comments  \\\n",
       "0    theyoungthaddeus        No one \"needs\" an assault foam dart blaster   \n",
       "1  Just_an_asian_here  Cause all attractive women are uninteresting a...   \n",
       "2            Foxprowl                                             Poser.   \n",
       "3              kd7rzv  Won't be long and Anet will start banning peop...   \n",
       "4            Ellefied  There goes my hope that Kubo does a Kojima as ...   \n",
       "\n",
       "                                     Parent Comments  \\\n",
       "0  Your son has to register those at the county j...   \n",
       "1    Likely due to creative and interesting content.   \n",
       "2                        Jon Stewart is going to HBO   \n",
       "3  This post looks like bullshit market manipulat...   \n",
       "4  Plus the Japanese typically do not talk shit w...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Your son has to register those at the county j...   \n",
       "1  Likely due to creative and interesting content...   \n",
       "2                 Jon Stewart is going to HBO Poser.   \n",
       "3  This post looks like bullshit market manipulat...   \n",
       "4  Plus the Japanese typically do not talk shit w...   \n",
       "\n",
       "                                  Pre Processed Text  \n",
       "0  son register county jungle gym mags hold ounce...  \n",
       "1  likely due creative interesting content cause ...  \n",
       "2                           jon stewart go hbo power  \n",
       "3  post looks like bullshit market manipulation w...  \n",
       "4  plus japanese typically talk shit come busines...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "df_prepro_test = pd.read_csv('processed_test.csv')\n",
    "print(df_prepro_test.shape)\n",
    "df_prepro_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e2e255-b4d9-4573-a536-9f7892d3ea98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                     0\n",
       "Comments              15\n",
       "Parent Comments        0\n",
       "Text                   0\n",
       "Pre Processed Text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd6dff77-ac97-4700-b15f-f4b9c92709ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "X = df_prepro_train['Pre Processed Text']\n",
    "y_trn = df_prepro_train['Class Labels']\n",
    "\n",
    "# Test data\n",
    "X_test = df_prepro_test['Pre Processed Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c68e5bf0-e447-494f-ba66-0c9a9f0c4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting Labels coloumn into numerical form of train data\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1bb816b-18c9-481a-937e-5abb848f1373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['non-sarcastic', 'sarcastic'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "671149f0-dbce-4461-b86a-ddab18ec0214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['non-sarcastic'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.inverse_transform([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43446706-769f-4c91-bee1-c7fc710c68a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "156f8075-57cb-47f8-84b7-7e955f20256c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((646928,), (161733,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train validation split of data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=3,stratify=y)\n",
    "X_train.shape,X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8c1a2b2-3400-402c-b0e7-a87dd934108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X_train, X_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c6da2-aeff-4372-bddf-d57d82f06cc7",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664ab426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(ngram_range=(1,3),max_features=60000) \n",
    "# X_train_bow = cv.fit_transform(X_train).toarray()\n",
    "# X_valid_bow = cv.transform(X_valid).toarray()\n",
    "# X_test_bow = cv.transform(X_test).toarray()\n",
    "\n",
    "# X_train_bow.shape,X_valid_bow.shape,X_test_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa98cf60-5a93-4193-973e-57a1b72ff6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = CountVectorizer(ngram_range=(2,2),max_features=100000) # (1,3) means Unigrams, Bigrams and Trigrams\n",
    "X_train_bow = sp.csr_matrix(CV.fit_transform(X_train))  # compressing the data to use less memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7933e5c7-a170-45d7-97f7-ff9972ba2f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(646928, 100000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a849c323-66f7-42fa-9878-601e6037eabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_train_bow_22l.joblib']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X_train_bow.tocsr(), 'X_train_bow_22l.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b5e1b66-844b-4c1d-99dd-13a1c47a546c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10th grade', '12th man', '14th amendment', '15th century',\n",
       "       '16gb ram', '16th century', '18th birthday', '18th century',\n",
       "       '19th century', '1b grichuk', '1b move', '1st 2nd', '1st 3rd',\n",
       "       '1st amendment', '1st day', '1st gen', '1st half', '1st line',\n",
       "       '1st overall', '1st person', '1st place', '1st quarter',\n",
       "       '1st round', '1st team', '1st time', '1st world', '1st year',\n",
       "       '20th anniversary', '20th century', '21st birthday',\n",
       "       '21st century', '27th world', '2b carpenter', '2b cruz', '2nd 3rd',\n",
       "       '2nd amendment', '2nd best', '2nd game', '2nd gen', '2nd goal',\n",
       "       '2nd grade', '2nd half', '2nd high', '2nd large', '2nd last',\n",
       "       '2nd one', '2nd place', '2nd quarter', '2nd round', '2nd rounder',\n",
       "       '2nd season', '2nd time', '2nd year', '30th anniversary',\n",
       "       '32gb ram', '3b holiday', '3b piscotty', '3d model', '3d print',\n",
       "       '3d printed', '3d printer', '3d printers', '3d printing',\n",
       "       '3d touch', '3d world', '3ds game', '3ds games', '3ds go',\n",
       "       '3ds version', '3ds wii', '3ds xl', '3rd 4th', '3rd best',\n",
       "       '3rd floor', '3rd game', '3rd gen', '3rd generation', '3rd grade',\n",
       "       '3rd line', '3rd one', '3rd parties', '3rd party', '3rd period',\n",
       "       '3rd person', '3rd place', '3rd quarter', '3rd round',\n",
       "       '3rd string', '3rd time', '3rd world', '3rd year', '4d chess',\n",
       "       '4gb ram', '4k 60fps', '4k fps', '4k game', '4k monitor',\n",
       "       '4k resolution', '4k screen', '4k tv'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV.get_feature_names_out()[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19fc0675-6861-4674-9af6-3fe3cc97e015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "num_cores = multiprocessing.cpu_count()\n",
    "num_partitions = num_cores-2 # leave some cores for other processes\n",
    "print(num_partitions)\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    a = np.array_split(df, num_partitions)\n",
    "    del df\n",
    "    pool = Pool(num_cores)\n",
    "    df = sp.vstack(pool.map(func,a), format='csr')\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def test_func2(data,cv=CV,train=False):\n",
    "    X_bow = cv.transform(data)\n",
    "    return X_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e4d4f36a-9065-46d5-a447-9d117fa96000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_valid_bow_22l.joblib']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_valid_bow = parallelize_dataframe(X_valid, test_func2)\n",
    "X_valid_bow = test_func2(X_valid)\n",
    "X_valid_bow.shape\n",
    "joblib.dump(X_valid_bow.tocsr(), 'X_valid_bow_22l.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7423f50-9309-419a-b46b-eca5923bbf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_bow = parallelize_dataframe(X_test, test_func2)\n",
    "# X_test_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737dccc-ba37-401e-af6f-624e4a1deea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(X_train_bow.tocsr(), 'X_train_bow_13m.joblib')\n",
    "\n",
    "joblib.dump(X_test_bow.tocsr(), 'X_test_bow_13m.joblib')\n",
    "del X_train_bow,X_valid_bow,X_test_bow # deleting the variable for freeing memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0521d4-78a4-4620-992d-c6c7b8baff4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5f7147c-5ae5-4504-8eea-6056ceec4b51",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcbdbe1-85dd-4cf5-b884-8eb4fd2454e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer(ngram_range=(1,3),max_features=60000,use_idf=True,smooth_idf=True)\n",
    "# X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "# X_valid_tfidf = tfidf.transform(X_valid).toarray()\n",
    "# X_test_tfidf = tfidf.transform(X_test).toarray()\n",
    "\n",
    "# X_train_tfidf.shape,X_valid_tfidf.shape,X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48714281-3791-42c1-be09-eacee071ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3),max_features=100000,use_idf=True,smooth_idf=True)\n",
    "X_train_tfidf = sp.csr_matrix(tfidf.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b1056-d45d-48f0-826b-6f32b93342f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d7ab8-264b-470a-a388-32b047a45d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = multiprocessing.cpu_count()\n",
    "num_partitions = num_cores-2 # leave some cores for other processes\n",
    "print(num_partitions)\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    a = np.array_split(df, num_partitions)\n",
    "    del df\n",
    "    pool = Pool(num_cores)\n",
    "    df = sp.vstack(pool.map(func,a), format='csr')\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def test_func2(data,cv=tfidf,train=False):\n",
    "    X_bow = cv.transform(data)\n",
    "    return X_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e6fe5a-9518-4fa4-bf25-5e0ef8b69d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_tfidf = parallelize_dataframe(X_valid, test_func2)\n",
    "X_valid_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ce764-53a9-4eaa-82e0-254178f4156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = parallelize_dataframe(X_test, test_func2)\n",
    "X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90268f-2f78-49a3-8150-6e34e25e8a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(X_train_tfidf.tocsr(), 'X_train_tfidf.joblib')\n",
    "joblib.dump(X_valid_tfidf.tocsr(), 'X_valid_tfidf.joblib')\n",
    "joblib.dump(X_test_tfidf.tocsr(), 'X_test_tfidf.joblib')\n",
    "del X_train_tfidf,X_valid_tfidf,X_test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac4669-0e68-409e-80f2-529a93445719",
   "metadata": {},
   "source": [
    "# word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d15a3-77f0-4c81-899a-d8d69ff23b8b",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3a97b-1ca0-4660-9625-a5e246061c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_spacy(X):\n",
    "    w2v_spacy = []\n",
    "    for item in X.values: # .values returns dataframe rows as list, eg. 1st sentence as ['he is good']\n",
    "        doc = nlp(item)\n",
    "        w2v_spacy.append(doc.vector)\n",
    "    w2v_spacy = np.array(w2v_spacy)\n",
    "    return w2v_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a12c1e-0026-4396-aa64-4dc9e718ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on train data\n",
    "w2v_spacy_train = word2vec_spacy(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601fb92-d2b5-4967-9d3d-63817d6ce137",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_spacy_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65ad18-1b41-4076-87f9-a3204240599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_spacy_train = sp.csr_matrix(w2v_spacy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b3b74c-28df-4169-85a8-7d5791aa91cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on validation data\n",
    "w2v_spacy_valid = word2vec_spacy(X_valid)\n",
    "w2v_spacy_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d0203-0a1b-4fc7-917c-dfd067286546",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_spacy_valid = sp.csr_matrix(w2v_spacy_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1cfff1-fa6b-4a1b-94b4-79631fffb099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on test data\n",
    "w2v_spacy_test = word2vec_spacy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447121b2-f156-4d8d-bcbb-4f22f04e825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_spacy_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423f7fa-2383-4341-bda1-dde44b2586f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_spacy_test = sp.csr_matrix(w2v_spacy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45644891-6887-4a2d-801d-a3983dbd154b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19162293-da87-4c7e-a885-fd090faa7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(w2v_spacy_train.tocsr(), 'w2v_spacy_train.joblib')\n",
    "joblib.dump(w2v_spacy_valid.tocsr(), 'w2v_spacy_valid.joblib')\n",
    "joblib.dump(w2v_spacy_test.tocsr(), 'w2v_spacy_test.joblib')\n",
    "del w2v_spacy_train,w2v_spacy_valid,w2v_spacy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ebf88-5351-44d5-bad1-930a76a87aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fdb3943-5f35-466c-a634-6a86b7eec9ea",
   "metadata": {},
   "source": [
    "### Custom Training - W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d88ed45-4696-4339-8b88-cbb14b9f8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00dd4644-6a5c-434f-94a5-28c21894ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences \n",
    "    def __iter__(self):\n",
    "        for line in self.sentences:\n",
    "            yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e36bd77e-0151-4eee-810c-575ede01cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MySentences(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46bcfd9-3fb2-4162-9c6d-380c7649bbbe",
   "metadata": {},
   "source": [
    "## uni+bi grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "839f2352-1b8e-4006-8db2-a33a64b0450e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20687929, 21807120)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(sentences=sentences,vector_size=300,sg= 0,window=5,min_count=1,workers=12)\n",
    "\n",
    "# Train a bigram detector.\n",
    "bigram_transformer = Phrases(sentences=sentences,connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "\n",
    "model.build_vocab(bigram_transformer[sentences],update=True)\n",
    "\n",
    "model.train(bigram_transformer[sentences],total_examples=model.corpus_count, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fbb1ecf1-5925-48b8-a9b1-2bc66847369e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sector_jobs'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index_to_key[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "68a02e4f-f861-41ee-afc2-74580e51fc63",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187433"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c117266-0b35-4016-ba9f-814457007b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the trained MWE detector to a corpus, using the result to train a Word2vec model.\n",
    "# model = Word2Vec(bigram_transformer[sentences],vector_size=300, min_count=1,workers = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "383f6427-b938-47e1-b480-fe1053147fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('word2vec_b300.model')#bigram included w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc107854-5ef9-4d56-bcff-4510aef2949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load(\"word2vec_b300.model\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8d776e48-3867-4458-b1c9-67379c745470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.wv['sector_jobs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a8253-ecaf-4116-a16d-e6cf6dd6f05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0c594833-a3cc-48ec-8b97-e2d2dee380ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec,dim):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = dim\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean(\n",
    "                [self.word2vec[w] for w in words if w in self.word2vec.keys()] or\n",
    "                    [np.zeros(self.dim)], axis=0) for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a0ed7a72-6598-4e8a-8298-809df9fba362",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_b300_dict=dict(zip(wv.wv.index_to_key, wv.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "426d90f8-0bcb-4bf1-820c-3d48965de0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_b300_train = sp.csr_matrix(MeanEmbeddingVectorizer(w2v_b300_dict,300).transform(X_train))\n",
    "joblib.dump(w2v_b300_train, 'w2v_b300_train.joblib')\n",
    "\n",
    "w2v_b300_valid = sp.csr_matrix(MeanEmbeddingVectorizer(w2v_b300_dict,300).transform(X_valid))\n",
    "joblib.dump(w2v_b300_valid, 'w2v_b300_valid.joblib')\n",
    "\n",
    "w2v_b300_test = sp.csr_matrix(MeanEmbeddingVectorizer(w2v_b300_dict,300).transform(X_test))\n",
    "joblib.dump(w2v_b300_test, 'w2v_b300_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a557c7af-7c88-4165-a0bb-a9a1b8b0a909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "0 6\n",
      "here\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "________________________________\n",
      "1. KNeighborsClassifier()\n",
      "________________________________\n",
      "{}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.48      0.49      4985\n",
      "           1       0.50      0.51      0.51      5015\n",
      "\n",
      "    accuracy                           0.50     10000\n",
      "   macro avg       0.50      0.50      0.50     10000\n",
      "weighted avg       0.50      0.50      0.50     10000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "g = Classification(w2v_b300_train[0:10000],w2v_b300_valid[0:10000],y_train[0:10000],y_valid[0:10000],data,'BoW',Gridsearch=0,model='knn',cv=2,fs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df7ae99-ed0e-4cb2-b5f7-763bd5771237",
   "metadata": {},
   "source": [
    "## Unigram only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bde976eb-c635-4ed8-ac87-b8f1fe14f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=sentences,vector_size=300,sg= 0,window=5,min_count=1,workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c800f19-e616-412d-8984-8dac9328495e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ira401k'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index_to_key[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf2be17e-6bc2-40ef-b0ca-68cf805fc86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159709"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c3ccd9d7-a92f-4af6-babf-02a007c76b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('word2vec_u300.model')#bigram included w2v\n",
    "del model\n",
    "wv = KeyedVectors.load(\"word2vec_u300.model\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "41747970-bb31-4489-a754-8b90f9b3de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_u300_dict=dict(zip(wv.wv.index_to_key, wv.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b6c58966-adce-462a-ab72-be09e5ccb11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w2v_u300_valid.joblib']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_u300_train = sp.csr_matrix(MeanEmbeddingVectorizer(w2v_u300_dict,300).transform(X_train))\n",
    "joblib.dump(w2v_u300_train, 'w2v_u300_train.joblib')\n",
    "\n",
    "w2v_u300_valid = sp.csr_matrix(MeanEmbeddingVectorizer(w2v_u300_dict,300).transform(X_valid))\n",
    "joblib.dump(w2v_u300_valid, 'w2v_u300_valid.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0c6f638f-308d-40bb-8a79-1f8a4a830e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5937704 ,  0.27558184,  0.24063347,  0.32158598, -0.85547984,\n",
       "         0.3118926 ,  0.72292525, -0.05425905,  0.82208353,  0.97658396,\n",
       "         0.21900721, -0.38159436,  0.7642419 ,  0.83633316, -0.94005495,\n",
       "         0.66408646, -0.1297711 ,  0.7680552 , -1.5277641 , -1.4388092 ,\n",
       "         0.31362134, -1.261821  , -0.2727935 ,  0.3542199 , -1.4014866 ,\n",
       "        -0.54228425,  0.08667997,  0.38010943,  0.1862712 , -0.10839657,\n",
       "         0.03373314,  0.44717583,  1.0492045 , -0.71907884, -0.08599456,\n",
       "         1.1627269 ,  0.70185465, -0.56828076, -0.07758637,  1.1084275 ,\n",
       "        -0.40251008,  0.17311534,  1.9498093 , -0.61532366,  0.04940276,\n",
       "        -0.5446284 , -0.01688252, -1.0019437 ,  0.36000264,  0.3026221 ,\n",
       "         1.6763095 ,  0.9329868 ,  0.6187248 ,  0.12736283,  0.69449955,\n",
       "         0.08917353, -1.3212767 ,  0.80669856,  0.45861074,  0.37602043,\n",
       "        -0.013489  , -0.35456342,  0.4146715 ,  0.89444816,  0.40953887,\n",
       "         0.82650536, -0.23190902, -0.04139166, -0.08281112, -1.0187852 ,\n",
       "        -0.5740357 , -0.17266262,  0.55184937,  1.1479685 , -0.43765876,\n",
       "        -1.0588027 ,  1.2090994 ,  0.05317143,  0.40071228,  0.85083944,\n",
       "        -0.2535904 , -0.6552047 , -0.14062852, -0.34230033, -0.976548  ,\n",
       "        -0.9562891 ,  0.54407686, -0.37111428,  0.56827676, -1.2357916 ,\n",
       "        -0.6499386 ,  0.9593414 ,  1.4800719 , -0.7613821 ,  1.2362518 ,\n",
       "         0.6621665 ,  0.9437458 ,  1.6062136 , -0.5003286 , -1.1060684 ,\n",
       "        -0.5152172 , -0.13041608, -0.06231114, -0.7971625 , -1.1319693 ,\n",
       "        -0.07340837, -0.07259294,  0.9052302 ,  1.009561  , -0.22352502,\n",
       "        -0.0267363 , -0.36015892, -0.3238275 , -0.01846557,  0.06050394,\n",
       "        -0.23634823,  1.0645058 ,  1.0503539 , -1.3318768 ,  1.1831938 ,\n",
       "        -0.6245317 , -0.9309794 ,  0.80325764, -0.23310058,  0.5766215 ,\n",
       "         1.4950037 , -0.9981609 ,  0.4425121 ,  0.23498759,  1.1027273 ,\n",
       "         0.3824819 ,  0.12849684,  1.180789  ,  0.3868746 ,  0.21110255,\n",
       "         0.58829814, -0.68939346, -0.0067964 ,  0.41821763,  0.68298244,\n",
       "        -0.2336221 , -0.02553652,  0.5675242 ,  0.9855221 , -0.09831384,\n",
       "         0.4806965 ,  0.54708415,  0.02397589, -0.859835  ,  0.52621347,\n",
       "         0.11503464, -0.38899666, -0.6769812 ,  0.5776981 , -0.9123042 ,\n",
       "        -0.57513475, -1.0281849 ,  0.24602862, -1.0243446 , -0.34378013,\n",
       "        -0.19449972,  0.3262398 , -0.186845  ,  0.11826883,  0.5302437 ,\n",
       "         1.0885853 , -0.19601546,  0.18676148, -0.38264185, -0.8085211 ,\n",
       "        -1.5016865 , -0.4313323 , -0.27506518,  0.39983448,  0.9169717 ,\n",
       "         0.17235261, -0.54293185, -0.43286628,  0.4524036 ,  0.01517068,\n",
       "        -0.5730048 ,  0.8500363 , -0.2321258 ,  0.13497378, -0.40977016,\n",
       "        -1.0042697 ,  0.8088151 , -0.17910543,  0.12336014, -0.39655107,\n",
       "        -0.15676494,  0.5024944 , -0.40406165, -0.27173302,  0.5692354 ,\n",
       "         0.8715052 ,  0.99133   ,  0.6064242 , -0.97357744,  0.27894473,\n",
       "        -0.14354773, -2.1294887 ,  0.60396   ,  0.22121048, -0.6122429 ,\n",
       "         1.078049  , -0.16354112, -0.34025955, -0.4213347 ,  1.4388924 ,\n",
       "         0.20006189,  0.15279701, -1.1681377 , -1.4737592 , -0.5506107 ,\n",
       "         0.36733642, -0.621469  ,  0.6930208 ,  1.3758776 ,  0.9376094 ,\n",
       "         0.38163334, -0.05356754,  1.5487165 , -0.7494406 , -1.1324434 ,\n",
       "         0.789432  ,  1.276249  , -0.612191  ,  2.1584003 , -0.55664563,\n",
       "        -0.7938441 ,  0.675108  ,  0.2768611 ,  0.06809457, -0.42478254,\n",
       "         0.54982597,  0.8152462 , -0.06941804,  0.363703  , -0.46354085,\n",
       "         0.1487795 ,  0.952479  ,  0.28855467, -0.89708257, -0.07034382,\n",
       "        -1.2391818 ,  0.22077402,  0.3296721 ,  1.3679254 , -0.90544456,\n",
       "         0.19446847, -0.76262355,  2.091361  , -0.3796625 , -0.09543493,\n",
       "         0.3610717 , -0.8282572 ,  0.68935484, -0.5378823 , -0.87007666,\n",
       "        -0.5047997 ,  0.08195537,  1.7837926 , -0.40694812,  0.59605336,\n",
       "        -1.1266078 , -0.5129543 , -0.7962019 , -1.9065652 ,  0.20369966,\n",
       "         1.3267835 ,  0.2160981 , -0.2510294 , -0.44488528, -0.23912576,\n",
       "        -0.21245064, -0.18612395, -0.30783084,  0.12854402,  0.6811688 ,\n",
       "        -0.15152772, -0.9787078 , -0.6806251 , -1.2895017 , -0.03522707,\n",
       "         0.02297472,  0.25866708, -2.2969186 ,  0.3211071 , -0.3455054 ,\n",
       "         0.62349427, -0.45718464,  0.4521279 ,  1.0553578 ,  0.32483053,\n",
       "        -0.3618319 , -1.4607558 , -0.73754084, -0.07104734,  1.0896091 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MeanEmbeddingVectorizer(w2v_u300_dict,300).transform([['aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa','good']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "38824837-c39c-452e-9a69-482197ec21bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m w2v_u300_test \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(\u001b[43mMeanEmbeddingVectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw2v_u300_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(w2v_u300_test, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2v_u300_test.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36mMeanEmbeddingVectorizer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[0;32m     13\u001b[0m         np\u001b[38;5;241m.\u001b[39mmean(\n\u001b[0;32m     14\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword2vec[w] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword2vec\u001b[38;5;241m.\u001b[39mkeys()] \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m     15\u001b[0m                 [np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m words \u001b[38;5;129;01min\u001b[39;00m X\n\u001b[0;32m     16\u001b[0m     ])\n",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[0;32m     13\u001b[0m         np\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m---> 14\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword2vec[w] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword2vec\u001b[38;5;241m.\u001b[39mkeys()] \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m     15\u001b[0m                 [np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m words \u001b[38;5;129;01min\u001b[39;00m X\n\u001b[0;32m     16\u001b[0m     ])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "w2v_u300_test = sp.csr_matrix(MeanEmbeddingVectorizer(w2v_u300_dict,300).transform(X_test))\n",
    "joblib.dump(w2v_u300_test, 'w2v_u300_test.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ebf33-2fe7-4fc6-80b5-5a234ddda735",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "698109da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.6820898056030273)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KHADGA JYOTH ALLI\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:783: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dists = dot(self.vectors[clip_start:clip_end], mean) / self.norms[clip_start:clip_end]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "glove_input_file = 'glove/glove.twitter.27B.200d.txt'\n",
    "model_glove = KeyedVectors.load_word2vec_format(glove_input_file, binary=False,no_header=True)\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model_glove.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "494d5d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w2v_glv_valid.joblib']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dict=dict(zip(model_glove.index_to_key, model_glove.vectors))\n",
    "w2v_glv_train = sp.csr_matrix(MeanEmbeddingVectorizer(glove_dict,200).transform(X_train))\n",
    "joblib.dump(w2v_glv_train, 'w2v_glv_train.joblib')\n",
    "\n",
    "w2v_glv_valid = sp.csr_matrix(MeanEmbeddingVectorizer(glove_dict,200).transform(X_valid))\n",
    "joblib.dump(w2v_glv_valid, 'w2v_glv_valid.joblib')\n",
    "\n",
    "# w2v_glv_test = sp.csr_matrix(MeanEmbeddingVectorizer(glove_dict,200).transform(X_test))\n",
    "# joblib.dump(w2v_glv_test, 'w2v_glv_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d935232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "0 6\n",
      "here\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "________________________________\n",
      "1. KNeighborsClassifier()\n",
      "________________________________\n",
      "{}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.51      0.51      4985\n",
      "           1       0.52      0.52      0.52      5015\n",
      "\n",
      "    accuracy                           0.52     10000\n",
      "   macro avg       0.52      0.52      0.52     10000\n",
      "weighted avg       0.52      0.52      0.52     10000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "g = Classification(w2v_b300_train[0:10000],w2v_glv_valid[0:10000],y_train[0:10000],y_valid[0:10000],data,'BoW',Gridsearch=0,model='knn',cv=2,fs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f9aac-e27c-4d84-bfdb-36f93659b13b",
   "metadata": {},
   "source": [
    "## DoC2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bb671dfe-3acc-443c-bac5-f21a04911121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c2e3cad88d45be9a4037656374adc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [117]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m sentences \u001b[38;5;241m=\u001b[39m MySentences(X_train)\n\u001b[0;32m      5\u001b[0m documents \u001b[38;5;241m=\u001b[39m [TaggedDocument(doc, [i]) \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(notebook\u001b[38;5;241m.\u001b[39mtqdm(sentences))]\n\u001b[1;32m----> 6\u001b[0m D2V \u001b[38;5;241m=\u001b[39m \u001b[43mDoc2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:294\u001b[0m, in \u001b[0;36mDoc2Vec.__init__\u001b[1;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, shrink_windows, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# EXPERIMENTAL lockf feature; create minimal no-op lockf arrays (1 element of 1.0)\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;66;03m# advanced users should directly resize/adjust as desired after any vocab growth\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdv\u001b[38;5;241m.\u001b[39mvectors_lockf \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mREAL)  \u001b[38;5;66;03m# 0.0 values suppress word-backprop-updates; 1.0 allows\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28msuper\u001b[39m(Doc2Vec, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    295\u001b[0m     sentences\u001b[38;5;241m=\u001b[39mcorpus_iterable,\n\u001b[0;32m    296\u001b[0m     corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file,\n\u001b[0;32m    297\u001b[0m     vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_size,\n\u001b[0;32m    298\u001b[0m     sg\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m dm) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    299\u001b[0m     null_word\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdm_concat,\n\u001b[0;32m    300\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    301\u001b[0m     window\u001b[38;5;241m=\u001b[39mwindow,\n\u001b[0;32m    302\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m    303\u001b[0m     shrink_windows\u001b[38;5;241m=\u001b[39mshrink_windows,\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    305\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:426\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_total_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trim_rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:514\u001b[0m, in \u001b[0;36mDoc2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    511\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffsets\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m offsets\n\u001b[0;32m    512\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_doctags\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m start_doctags\n\u001b[1;32m--> 514\u001b[0m \u001b[38;5;28msuper\u001b[39m(Doc2Vec, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m    515\u001b[0m     corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file,\n\u001b[0;32m    516\u001b[0m     total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words,\n\u001b[0;32m    517\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs, start_alpha\u001b[38;5;241m=\u001b[39mstart_alpha, end_alpha\u001b[38;5;241m=\u001b[39mend_alpha, word_count\u001b[38;5;241m=\u001b[39mword_count,\n\u001b[0;32m    518\u001b[0m     queue_factor\u001b[38;5;241m=\u001b[39mqueue_factor, report_delay\u001b[38;5;241m=\u001b[39mreport_delay, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:1069\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1069\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch(\n\u001b[0;32m   1070\u001b[0m         corpus_iterable, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples,\n\u001b[0;32m   1071\u001b[0m         total_words\u001b[38;5;241m=\u001b[39mtotal_words, queue_factor\u001b[38;5;241m=\u001b[39mqueue_factor, report_delay\u001b[38;5;241m=\u001b[39mreport_delay,\n\u001b[0;32m   1072\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1074\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch_corpusfile(\n\u001b[0;32m   1075\u001b[0m         corpus_file, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words,\n\u001b[0;32m   1076\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:1430\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m-> 1430\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_epoch_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_corpus_file_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py:1285\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1282\u001b[0m unfinished_worker_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unfinished_worker_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1285\u001b[0m     report \u001b[38;5;241m=\u001b[39m \u001b[43mprogress_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# blocks if workers too slow\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# a thread reporting that it finished\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m         unfinished_worker_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[1;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import notebook\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "sentences = MySentences(X_train)\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(notebook.tqdm(sentences))]\n",
    "D2V = Doc2Vec(documents, vector_size=100, window=5, min_count=1, workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edecf975-48ca-46bd-a273-10bd18fb64e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9605a726-987a-4bda-8373-a488b11c28c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06ee87-ea1e-443b-bde7-c61f2a3501d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b06a94c6-8cff-4816-ba5d-18445b89a398",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3429407-3182-4ebb-898c-78fe52e512fe",
   "metadata": {},
   "source": [
    "### With Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e658243-d356-4b43-948e-9b7c719e3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def logistic(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    fsl = 0\n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    print('\\n\\t ---------- Training Logistic Regression Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, random_state=1)\n",
    "    pipeline1 = Pipeline([#('fs',fsl),\n",
    "                          ('clf1', SGDClassifier(loss='log',alpha=0.0001))])\n",
    "    clf1_parameters = {\n",
    "        # 'fs__k' : ['all'],#100000\n",
    "        'clf1__penalty':['l1', 'l2',],\n",
    "        }\n",
    "    grid_search1 = GridSearchCV(estimator=pipeline1, param_grid=clf1_parameters, n_jobs=-1, cv=2, scoring='f1_macro',verbose=1)\n",
    "    grid_search1.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search1.best_score_}\")\n",
    "    clf1 = grid_search1.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf1) \n",
    "    predicted_class_labels1 = clf1.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels1))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c51fb54-723e-4658-ac6b-19e069810dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t ---------- Training Logistic Regression Classifier ---------- \n",
      "\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "Best score on Training set :  0.6474901494342165\n",
      "\n",
      "\n",
      " The best set of parameters of the pipeline in Training Phase are: \n",
      "Pipeline(steps=[('fs',\n",
      "                 SelectKBest(k=100000,\n",
      "                             score_func=<function chi2 at 0x0000028D7A7671F0>)),\n",
      "                ('clf1', SGDClassifier(loss='log'))])\n",
      "\n",
      " *******  Scores on Validation Data  ******* \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.71      0.67     80866\n",
      "           1       0.67      0.60      0.63     80867\n",
      "\n",
      "    accuracy                           0.66    161733\n",
      "   macro avg       0.66      0.66      0.65    161733\n",
      "weighted avg       0.66      0.66      0.65    161733\n",
      "\n",
      "====================================================================\n",
      "Process Completed and time taken is : 0.28 minutes\n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "clf1bn = logistic(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3284ac8a-ffda-4c79-ad0c-f4c8975f1313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t ---------- Training Logistic Regression Classifier ---------- \n",
      "\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "Best score on Training set :  0.6398512775985705\n",
      "\n",
      "\n",
      " The best set of parameters of the pipeline in Training Phase are: \n",
      "Pipeline(steps=[('fs',\n",
      "                 SelectKBest(k=100000,\n",
      "                             score_func=<function chi2 at 0x0000028D7A7671F0>)),\n",
      "                ('clf1', SGDClassifier(loss='log'))])\n",
      "\n",
      " *******  Scores on Validation Data  ******* \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.91      0.65     80866\n",
      "           1       0.50      0.09      0.15     80867\n",
      "\n",
      "    accuracy                           0.50    161733\n",
      "   macro avg       0.50      0.50      0.40    161733\n",
      "weighted avg       0.50      0.50      0.40    161733\n",
      "\n",
      "====================================================================\n",
      "Process Completed and time taken is : 0.21 minutes\n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "clf1bu = logistic(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ac21344-c54a-442c-a9fd-adf60f854707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t ---------- Training Logistic Regression Classifier ---------- \n",
      "\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "Best score on Training set :  0.4866029400012552\n",
      "\n",
      "\n",
      " The best set of parameters of the pipeline in Training Phase are: \n",
      "Pipeline(steps=[('fs',\n",
      "                 SelectKBest(k=100000,\n",
      "                             score_func=<function chi2 at 0x0000028D7A7671F0>)),\n",
      "                ('clf1', SGDClassifier(loss='log'))])\n",
      "\n",
      " *******  Scores on Validation Data  ******* \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.49      0.50     80866\n",
      "           1       0.50      0.51      0.51     80867\n",
      "\n",
      "    accuracy                           0.50    161733\n",
      "   macro avg       0.50      0.50      0.50    161733\n",
      "weighted avg       0.50      0.50      0.50    161733\n",
      "\n",
      "====================================================================\n",
      "Process Completed and time taken is : 0.06 minutes\n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "clf1bt = logistic(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6dc47fbb-8038-4ea1-a2ee-8014db701b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t ---------- Training Logistic Regression Classifier ---------- \n",
      "\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "Best score on Training set :  0.5960829572972676\n",
      "\n",
      "\n",
      " The best set of parameters of the pipeline in Training Phase are: \n",
      "Pipeline(steps=[('fs',\n",
      "                 SelectKBest(k=100000,\n",
      "                             score_func=<function chi2 at 0x0000028D7A7671F0>)),\n",
      "                ('clf1', SGDClassifier(loss='log'))])\n",
      "\n",
      " *******  Scores on Validation Data  ******* \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.54      0.53     80866\n",
      "           1       0.52      0.50      0.51     80867\n",
      "\n",
      "    accuracy                           0.52    161733\n",
      "   macro avg       0.52      0.52      0.52    161733\n",
      "weighted avg       0.52      0.52      0.52    161733\n",
      "\n",
      "====================================================================\n",
      "Process Completed and time taken is : 0.08 minutes\n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "clf1bb = logistic(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab38491f-708d-462d-8a71-c1a32567896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import Nystroem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6bba4871-dba9-4837-a4f0-433f2a9929c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_map_nystroem = Nystroem(gamma=.2,\n",
    "#                                  random_state=1,\n",
    "#                                 n_components=300)\n",
    "# data_transformed_tr = feature_map_nystroem.fit_transform(X_train_bow)\n",
    "# data_transformed_val = feature_map_nystroem.transform(X_valid_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade24fa8-3666-494e-b050-4876d0f85f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "70d3b968-e205-40f7-a6ac-f29b90eed188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t ---------- Training Logistic Regression Classifier ---------- \n",
      "\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "Best score on Training set :  0.5203800817426183\n",
      "\n",
      "\n",
      " The best set of parameters of the pipeline in Training Phase are: \n",
      "Pipeline(steps=[('clf1', SGDClassifier(loss='log', penalty='l1'))])\n",
      "\n",
      " *******  Scores on Validation Data  ******* \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.09      0.16     80866\n",
      "           1       0.51      0.95      0.66     80867\n",
      "\n",
      "    accuracy                           0.52    161733\n",
      "   macro avg       0.58      0.52      0.41    161733\n",
      "weighted avg       0.58      0.52      0.41    161733\n",
      "\n",
      "====================================================================\n",
      "Process Completed and time taken is : 0.3 minutes\n",
      "====================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('clf1', SGDClassifier(loss='log', penalty='l1'))])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic(data_transformed_tr, data_transformed_val, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beca215b-53a6-4bda-a89f-bbbfe3d39ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classification(X_train, X_test, y_train, y_test,file_name:str,method:str,Gridsearch = False,fs:str=None,model:str='all',cv=5):\n",
    "    # try:\n",
    "    #     y_train = y_train.values.ravel()\n",
    "    #     y_test = y_test.values.ravel()\n",
    "    # except:\n",
    "    #     pass\n",
    "    with open(file_name+'.txt','a') as f:     \n",
    "        file_size = os.stat(f'{file_name}.txt').st_size\n",
    "        if file_size == 0:\n",
    "            print(f\"New File Created: {file_name}.txt\")\n",
    "            f.write(\"method, Model, Accuracy, f1_micro, f1_macro,other_info \\n\")\n",
    "        else:\n",
    "            print(f\"Existing file found: Appending to File: {file_name}.txt\")       \n",
    "        f.close()\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    assert fs in ['pca','chi2','mi',None], f'feature selection should be {{\"pca\",\"chi2\",\"mi\",None}}. got:{fs}'\n",
    "    mdls = ['knn','lr','dtree','rf','svm','mnb','all']\n",
    "    assert model in mdls, f\"model should be {{'all','lr','svm','knn','dtree','rf','mnb'}} but got : {model}\"\n",
    "    \n",
    "    \n",
    "    n_comp = 'fsp__k'\n",
    "    if fs == 'mi' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "        fslk =  [10000]\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "        fslk = [10000]\n",
    "    elif fs == 'pca':\n",
    "        n_comp = 'fsp__n_components'\n",
    "        if isinstance(X_train,sp.csr_matrix):\n",
    "            print('using TruncatedSVD as input is sparse')\n",
    "            fsl = TruncatedSVD()\n",
    "            fslk = [5000,10000]\n",
    "            \n",
    "        else:\n",
    "            fsl =  PCA(svd_solver = 'full')\n",
    "            fslk = [0.95]\n",
    "            \n",
    "    elif fs==None:\n",
    "        fsl = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    SGDClassifier(n_jobs=-1,loss='log'),\n",
    "    DecisionTreeClassifier(),     \n",
    "    RandomForestClassifier(),\n",
    "        # AdaBoostClassifier(),\n",
    "#         GradientBoostingClassifier(),\n",
    "    SGDClassifier(n_jobs=-1,early_stopping=True,loss='hinge'),\n",
    "    MultinomialNB()\n",
    "        \n",
    "    ]\n",
    "    if Gridsearch ==True:\n",
    "        clf_parameters = [\n",
    "            {\n",
    "                \"clf__n_neighbors\": [500,1000],#np.arange(2,25 ,10),\n",
    "                \"clf__metric\": [\"l1\",\"cosine\"],\n",
    "                \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "         },\n",
    "        {\n",
    "               'clf__penalty':['l1', 'l2'],\n",
    "            'clf__alpha':[0.001,0.01,0.1]\n",
    "        },\n",
    "        {\n",
    "                'clf__criterion' : [\"gini\", \"entropy\"], \n",
    "            # 'clf__max_features':['sqrt', 'log2'],\n",
    "                    'clf__max_depth':[50,100],\n",
    "                    #'clf__ccp_alpha':np.logspace(-3,-2,20),#np.logspace(-2.32,-2.3,20),\n",
    "            \"clf__max_leaf_nodes\" : [100],\n",
    "\n",
    "            # \"clf__splitter\" : [\"best\", \"random\"],\n",
    "            #\"clf__min_samples_split\":np.arange(2,50,10)\n",
    "        },\n",
    "        {\n",
    "                 'clf__n_estimators': [50,80],\n",
    "            # 'clf__max_features': ['sqrt', 'log2'],\n",
    "            'clf__max_depth' : [20,50],#np.arange(4,15,2).tolist(),\n",
    "            'clf__criterion' :['gini', 'entropy'],\n",
    "        #         'clf__ccp_alpha':np.logspace(-2,1,10)\n",
    "        },\n",
    "#             {\n",
    "#                 'clf__base_estimator':[RandomForestClassifier(),DecisionTreeClassifier(criterion='entropy'),SVC(),LogisticRegression(**{'C': 0.017433288221999882, 'penalty': 'l2', 'solver': 'newton-cg'})],\n",
    "#                 'clf__algorithm' : ['SAMME', 'SAMME.R'],\n",
    "#                 'clf__n_estimators': [50,100]\n",
    "#             },\n",
    "#             {\n",
    "#               'clf__loss' :['deviance', 'exponential'],\n",
    "#                 'clf__criterion' : ['friedman_mse', 'squared_error'],\n",
    "#                 'clf__max_features' : [ 'sqrt', 'log2'],\n",
    "#                 'clf__learning_rate': np.logspace(-1,1,5),\n",
    "#                 'clf__n_estimators':np.arange(100,1000,200)\n",
    "\n",
    "#             },\n",
    "            {\n",
    "                'clf__loss' :['hinge' ,'squared_hinge', 'perceptron'],\n",
    "                'clf__alpha':[0.001,0.01,0.1]\n",
    "                #'clf__early_stopping':[True],\n",
    "                \n",
    "            },\n",
    "            {\n",
    "                'clf__alpha':[0] + np.logspace(-2,5,5).tolist(),\n",
    "                \n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        clf_parameters = [{}]*(len(mdls)-1)\n",
    "    # data[name] = {'Model':[],'Accuracy' :[],'f1_micro' :[],'f1_macro' :[]}\n",
    "    # dataint = {'Model':[],'Accuracy' :[],'f1_micro' :[],'f1_macro' :[]}\n",
    "    \n",
    "    i=1\n",
    "    def fit(i,file_name):\n",
    "        if fs!=None:\n",
    "                clf_params[n_comp] = fslk                \n",
    "                pipe = Pipeline(steps = [('fsp', fsl),('clf', classifier)])\n",
    "        else:\n",
    "            pipe = Pipeline(steps=[('clf', classifier)])\n",
    "        grid = GridSearchCV(pipe,clf_params,scoring='f1_macro',cv=cv,n_jobs=-1,verbose=1) #early_stopping=False,use_gpu=True)  \n",
    "        try:\n",
    "            grid.fit(X_train, y_train)   \n",
    "            pred = grid.predict(X_test)\n",
    "            print(\"_\"*32)\n",
    "            print(f'{i}.',classifier)\n",
    "            print(\"_\"*32)\n",
    "            print(grid.best_params_)\n",
    "            print(classification_report(y_test, pred))\n",
    "            i1 = classifier.__class__.__name__\n",
    "            i2 = sklearn.metrics.accuracy_score(y_test,pred)\n",
    "            i3 = sklearn.metrics.f1_score(y_test,pred,average='micro')\n",
    "            i4 = sklearn.metrics.f1_score(y_test,pred,average='macro')\n",
    "            with open(f'{file_name}.txt','a') as f:\n",
    "                f.writelines(f'{method},{i1},{i2},{i3},{i4},{f\"model:{model};Gridsearch:{Gridsearch};cv:{cv};fs:{fs};best_param:{grid.best_params_}\"} \\n')\n",
    "                f.close()\n",
    "            # dataint['Model'].append(i1)\n",
    "            # dataint['Accuracy'].append(i2)\n",
    "            # dataint['f1_micro'].append(i3)\n",
    "            # dataint['f1_macro'].append(i4)\n",
    "            print(\"-\"*80)\n",
    "            print(\"-\"*80)\n",
    "            # data[name] = dataint\n",
    "\n",
    "        except Exception as e: print(e)\n",
    "        return grid\n",
    "    \n",
    "    if model =='all':        \n",
    "        for classifier,clf_params in zip(classifiers,clf_parameters):\n",
    "            fit(i,file_name)\n",
    "            i+=1\n",
    "    else:\n",
    "        _ = mdls.index(model)\n",
    "        # print(_,len(clf_parameters))\n",
    "        classifier,clf_params = classifiers[_],clf_parameters[_]\n",
    "        # print(classifier.get_params())\n",
    "        fit(i,file_name)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac1001-b79f-4680-96c5-d3d4ea9a0a29",
   "metadata": {},
   "source": [
    "### BOW resuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00e5b6c6-246b-487b-a6c1-34dfe730c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "X_train_bow = joblib.load('X_train_bow_13l.joblib', mmap_mode='c')\n",
    "X_valid_bow = joblib.load('X_valid_bow_13l.joblib', mmap_mode='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abc9f4c2-a411-40bc-8cb5-ac4e400ddae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10b85b2f-bf4d-44a4-a497-167528ad96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0999f03-7fe6-40a7-a089-00f6bb3c1893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "________________________________\n",
      "1. SGDClassifier(loss='log', n_jobs=-1)\n",
      "________________________________\n",
      "{}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.71      0.67     80866\n",
      "           1       0.68      0.60      0.64     80867\n",
      "\n",
      "    accuracy                           0.66    161733\n",
      "   macro avg       0.66      0.66      0.65    161733\n",
      "weighted avg       0.66      0.66      0.65    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW',Gridsearch=False,model='lr',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e2659f1-2bf3-4d30-923d-c5f70aeae9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "________________________________\n",
      "1. SGDClassifier(early_stopping=True, n_jobs=-1)\n",
      "________________________________\n",
      "{}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68     80866\n",
      "           1       0.69      0.55      0.61     80867\n",
      "\n",
      "    accuracy                           0.65    161733\n",
      "   macro avg       0.66      0.65      0.65    161733\n",
      "weighted avg       0.66      0.65      0.65    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW',Gridsearch=False,model='svm',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "838f6b1d-688a-4d93-bf5e-3b3880c078e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "________________________________\n",
      "1. MultinomialNB()\n",
      "________________________________\n",
      "{}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.64      0.64     80866\n",
      "           1       0.64      0.64      0.64     80867\n",
      "\n",
      "    accuracy                           0.64    161733\n",
      "   macro avg       0.64      0.64      0.64    161733\n",
      "weighted avg       0.64      0.64      0.64    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW',Gridsearch=False,model='mnb',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7841b8fc-4786-43e6-a770-ce25c9a93f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "________________________________\n",
      "1. DecisionTreeClassifier()\n",
      "________________________________\n",
      "{}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.60      0.59     80866\n",
      "           1       0.58      0.56      0.57     80867\n",
      "\n",
      "    accuracy                           0.58    161733\n",
      "   macro avg       0.58      0.58      0.58    161733\n",
      "weighted avg       0.58      0.58      0.58    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW',Gridsearch=False,model='dtree',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6c2f7c-6311-49fd-b842-49ca6b5884bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW',Gridsearch=False,model='knn',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d638d-ef30-4412-aafd-18ea5250107d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW',Gridsearch=False,model='rf',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b5d97-c1be-4d76-8b12-4dc835cfb874",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9643cb5-83dd-4c1f-b1ea-f319d0d6fd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "________________________________\n",
      "1. SGDClassifier(loss='log', n_jobs=-1)\n",
      "________________________________\n",
      "{'clf__alpha': 0.001, 'clf__penalty': 'l2'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.72      0.67     80866\n",
      "           1       0.67      0.57      0.61     80867\n",
      "\n",
      "    accuracy                           0.64    161733\n",
      "   macro avg       0.65      0.64      0.64    161733\n",
      "weighted avg       0.65      0.64      0.64    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW',Gridsearch=True,model='lr',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5afd12b-2d4b-4a51-b073-37ca24e2686d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "________________________________\n",
      "1. SGDClassifier(early_stopping=True, n_jobs=-1)\n",
      "________________________________\n",
      "{'clf__alpha': 0.001, 'clf__loss': 'hinge'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.81      0.69     80866\n",
      "           1       0.71      0.45      0.55     80867\n",
      "\n",
      "    accuracy                           0.63    161733\n",
      "   macro avg       0.65      0.63      0.62    161733\n",
      "weighted avg       0.65      0.63      0.62    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW',Gridsearch=True,model='svm',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a97706-4306-4834-95ad-43cd7f6715aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f08ba2-9def-4060-a22c-7092b0d6c34c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a03a3319-ed0e-4f48-af49-09ec6b93e9fd",
   "metadata": {},
   "source": [
    "### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f75877ff-1e5b-49ce-bb79-3a218b30699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "X_train_bow = joblib.load('X_train_bow_11l.joblib', mmap_mode='c')\n",
    "X_valid_bow = joblib.load('X_valid_bow_11l.joblib', mmap_mode='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62cd06eb-768e-4ba9-9a71-3ab35ca83d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "________________________________\n",
      "1. SGDClassifier(loss='log', n_jobs=-1)\n",
      "________________________________\n",
      "{}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.91      0.65     80866\n",
      "           1       0.50      0.09      0.15     80867\n",
      "\n",
      "    accuracy                           0.50    161733\n",
      "   macro avg       0.50      0.50      0.40    161733\n",
      "weighted avg       0.50      0.50      0.40    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW-11',Gridsearch=False,model='lr',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50254a2b-0d53-4825-bb25-26937e4e875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "________________________________\n",
      "1. SGDClassifier(early_stopping=True, n_jobs=-1)\n",
      "________________________________\n",
      "{}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.94      0.65     80866\n",
      "           1       0.49      0.05      0.10     80867\n",
      "\n",
      "    accuracy                           0.50    161733\n",
      "   macro avg       0.49      0.50      0.37    161733\n",
      "weighted avg       0.49      0.50      0.37    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW-11',Gridsearch=False,model='svm',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c36b2-8271-497e-924d-87aeb2fce394",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e07bd4a-d0d4-421e-8d68-eb733c0421ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "________________________________\n",
      "1. SGDClassifier(loss='log', n_jobs=-1)\n",
      "________________________________\n",
      "{'clf__alpha': 0.001, 'clf__penalty': 'l2', 'fsp__k': 10000}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.98      0.66     80866\n",
      "           1       0.49      0.02      0.03     80867\n",
      "\n",
      "    accuracy                           0.50    161733\n",
      "   macro avg       0.49      0.50      0.35    161733\n",
      "weighted avg       0.49      0.50      0.35    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW-11',Gridsearch=True,model='lr',cv=5,fs='chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bdfb87a-2b45-416c-990a-90032db804e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "________________________________\n",
      "1. SGDClassifier(early_stopping=True, n_jobs=-1)\n",
      "________________________________\n",
      "{'clf__alpha': 0.01, 'clf__loss': 'hinge', 'fsp__k': 10000}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67     80866\n",
      "           1       0.51      0.00      0.00     80867\n",
      "\n",
      "    accuracy                           0.50    161733\n",
      "   macro avg       0.50      0.50      0.34    161733\n",
      "weighted avg       0.50      0.50      0.34    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW-11',Gridsearch=True,model='svm',cv=5,fs='chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "407df53d-bc3b-4990-ae88-140c18def7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "________________________________\n",
      "1. MultinomialNB()\n",
      "________________________________\n",
      "{'clf__alpha': 31.622776601683793, 'fsp__k': 10000}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.66      0.57     80866\n",
      "           1       0.50      0.35      0.41     80867\n",
      "\n",
      "    accuracy                           0.50    161733\n",
      "   macro avg       0.50      0.50      0.49    161733\n",
      "weighted avg       0.50      0.50      0.49    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW-11',Gridsearch=True,model='mnb',cv=5,fs='chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "252475bd-e408-41e0-8827-c9d3ebeb6251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "________________________________\n",
      "1. DecisionTreeClassifier()\n",
      "________________________________\n",
      "{'clf__criterion': 'gini', 'clf__max_depth': 100, 'clf__max_leaf_nodes': 100, 'fsp__k': 10000}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67     80866\n",
      "           1       0.50      0.00      0.01     80867\n",
      "\n",
      "    accuracy                           0.50    161733\n",
      "   macro avg       0.50      0.50      0.34    161733\n",
      "weighted avg       0.50      0.50      0.34    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW-11',Gridsearch=True,model='dtree',cv=5,fs='chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb03db4-5c9e-4cf0-be62-48f1dc1ed386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW-11',Gridsearch=True,model='rf',cv=5,fs='chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687e1f9-7033-4875-8a85-566b8d6d4195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW-11',Gridsearch=True,model='knn',cv=5,fs='chi2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b50eb-e6db-4657-8209-9388ed273f5a",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b01ce1-3bb7-490f-82b7-c9be16b8b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "X_train_bow = joblib.load('X_train_bow_22l.joblib', mmap_mode='c')\n",
    "X_valid_bow = joblib.load('X_valid_bow_22l.joblib', mmap_mode='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e9a99c-9cec-4b70-8724-415dd895220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW-22',Gridsearch=False,model='lr',cv=5,fs=None)\n",
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW-22',Gridsearch=False,model='svm',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb470dd-617d-4f47-a74b-c2bc59e6f016",
   "metadata": {},
   "source": [
    "### gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7a755-2b17-45e3-906a-a1856e827c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW-22',Gridsearch=True,model='lr',cv=5,fs='chi2')\n",
    "\n",
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW-22',Gridsearch=True,model='svm',cv=5,fs='chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea167f-3778-4c74-ac29-f58e0919ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW-22',Gridsearch=True,model='mnb',cv=5,fs='chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b907e-8613-40bc-9f9e-c807f4b9ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW-22',Gridsearch=True,model='dtree',cv=5,fs='chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b8754-4a14-40ef-8520-85eedb190f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW-22',Gridsearch=True,model='rf',cv=5,fs='chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34816d4-c862-4022-a5e2-d6ed0861d0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2a517-7a9a-4716-b810-b3db023251d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 , 22,13 gridsearch, all, k=10000, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c96d88e-320b-4b30-8214-62779aedc84f",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3783665e-4723-4f16-9cbb-effebadd65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_bow,X_valid_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0f64fda-1926-4487-8ee4-2444ae0cddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_b300_train = joblib.load('w2v_b300_train.joblib', mmap_mode='c')\n",
    "w2v_b300_valid = joblib.load('w2v_b300_valid.joblib', mmap_mode='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf0af85-6342-4c15-b086-45727e64280c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "________________________________\n",
      "1. SGDClassifier(loss='log', n_jobs=-1)\n",
      "________________________________\n",
      "{}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.08      0.15     80866\n",
      "           1       0.51      0.94      0.66     80867\n",
      "\n",
      "    accuracy                           0.51    161733\n",
      "   macro avg       0.55      0.51      0.40    161733\n",
      "weighted avg       0.55      0.51      0.40    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "________________________________\n",
      "1. SGDClassifier(early_stopping=True, n_jobs=-1)\n",
      "________________________________\n",
      "{}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.08      0.14     80866\n",
      "           1       0.51      0.95      0.66     80867\n",
      "\n",
      "    accuracy                           0.51    161733\n",
      "   macro avg       0.55      0.51      0.40    161733\n",
      "weighted avg       0.55      0.51      0.40    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KHADGA JYOTH ALLI\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\KHADGA JYOTH ALLI\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\KHADGA JYOTH ALLI\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\KHADGA JYOTH ALLI\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 690, in fit\n",
      "    self._count(X, Y)\n",
      "  File \"C:\\Users\\KHADGA JYOTH ALLI\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 863, in _count\n",
      "    check_non_negative(X, \"MultinomialNB (input X)\")\n",
      "  File \"C:\\Users\\KHADGA JYOTH ALLI\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1249, in check_non_negative\n",
      "    raise ValueError(\"Negative values in data passed to %s\" % whom)\n",
      "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\KHADGA JYOTH ALLI\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative values in data passed to MultinomialNB (input X)\n",
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "g = Classification(w2v_b300_train,w2v_b300_valid,y_train,y_valid,'Final','W2V_b300',Gridsearch=False,model='lr',cv=5,fs=None)\n",
    "\n",
    "g = Classification(w2v_b300_train,w2v_b300_valid,y_train,y_valid,'Final','W2V_b300',Gridsearch=False,model='svm',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e612f34-cf64-4b12-ac1c-599abcf2dd21",
   "metadata": {},
   "source": [
    "## glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "813579e2-936d-4254-a95b-c1c7abb03d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_glv_train = joblib.load('w2v_glv_train.joblib', mmap_mode='c')\n",
    "w2v_glv_valid = joblib.load('w2v_glv_valid.joblib', mmap_mode='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "088fa80e-69b4-4412-a525-97917d0b9c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "________________________________\n",
      "1. SGDClassifier(loss='log', n_jobs=-1)\n",
      "________________________________\n",
      "{}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.78      0.62     80866\n",
      "           1       0.53      0.25      0.34     80867\n",
      "\n",
      "    accuracy                           0.52    161733\n",
      "   macro avg       0.52      0.52      0.48    161733\n",
      "weighted avg       0.52      0.52      0.48    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "________________________________\n",
      "1. SGDClassifier(early_stopping=True, n_jobs=-1)\n",
      "________________________________\n",
      "{}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.75      0.61     80866\n",
      "           1       0.53      0.29      0.38     80867\n",
      "\n",
      "    accuracy                           0.52    161733\n",
      "   macro avg       0.52      0.52      0.49    161733\n",
      "weighted avg       0.52      0.52      0.49    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "g = Classification(w2v_glv_train,w2v_glv_valid,y_train,y_valid,'Final','W2V_Glove',Gridsearch=False,model='lr',cv=5,fs=None)\n",
    "\n",
    "g = Classification(w2v_glv_train,w2v_glv_valid,y_train,y_valid,'Final','W2V_Glove',Gridsearch=False,model='svm',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a195e8d-4b0f-4cf3-9096-e374a3e0fcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "________________________________\n",
      "1. SGDClassifier(loss='log', n_jobs=-1)\n",
      "________________________________\n",
      "{'clf__penalty': 'l1'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.54      0.53     80866\n",
      "           1       0.53      0.51      0.52     80867\n",
      "\n",
      "    accuracy                           0.53    161733\n",
      "   macro avg       0.53      0.53      0.53    161733\n",
      "weighted avg       0.53      0.53      0.53    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "g = Classification(w2v_glv_train,w2v_glv_valid,y_train,y_valid,'Final','W2V_Glove',Gridsearch=True,model='lr',cv=5,fs=None)\n",
    "\n",
    "# g = Classification(w2v_glv_train,w2v_glv_valid,y_train,y_valid,'Final','W2V_Glove',Gridsearch=False,model='svm',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fed575f6-2dc6-4ac1-969d-664fbc728478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing file found: Appending to File: Final.txt\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "________________________________\n",
      "1. SGDClassifier(early_stopping=True, n_jobs=-1)\n",
      "________________________________\n",
      "{}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.02      0.03     80866\n",
      "           1       0.50      0.99      0.67     80867\n",
      "\n",
      "    accuracy                           0.50    161733\n",
      "   macro avg       0.58      0.50      0.35    161733\n",
      "weighted avg       0.58      0.50      0.35    161733\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "g = Classification(w2v_glv_train,w2v_glv_valid,y_train,y_valid,'Final','W2V_Glove',Gridsearch=True,model='svm',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8436767-40bc-4711-a1d4-9c2c0fbe2029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c841c2-82e7-451c-920a-4ea01002dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Classification(X_train_bow,X_valid_bow,y_train,y_valid,'Final','BoW',Gridsearch=True,model='all',cv=5,fs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "562b850b-02f9-4afc-8e6d-9f0553f821c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>other_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BoW</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.466338</td>\n",
       "      <td>model:svm;Gridsearch:False;cv:2;fs:None;best_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BoW</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.574645</td>\n",
       "      <td>model:svm;Gridsearch:False;cv:2;fs:None;best_p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  method          Model   Accuracy   f1_micro   f1_macro  \\\n",
       "0    BoW  SGDClassifier       0.48       0.48   0.466338   \n",
       "1    BoW  SGDClassifier       0.59       0.59   0.574645   \n",
       "\n",
       "                                         other_info   \n",
       "0  model:svm;Gridsearch:False;cv:2;fs:None;best_p...  \n",
       "1  model:svm;Gridsearch:False;cv:2;fs:None;best_p...  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('demo.txt',delimiter=',',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fee5ce65-4e7f-4611-b31f-bdaa2a7563d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of GridSearchCV(cv=2,\n",
       "             estimator=Pipeline(steps=[('clf',\n",
       "                                        SGDClassifier(early_stopping=True,\n",
       "                                                      n_jobs=-1))]),\n",
       "             n_jobs=-1, param_grid={}, scoring='f1_macro', verbose=1)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0177b01-197f-4bf7-855a-54e06ededc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sp.csr_matrix([1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ad56c1e-c15e-42f8-9d7f-c7c94bdc51c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "values not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28misinstance\u001b[39m(\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241m.\u001b[39mravel(),sp\u001b[38;5;241m.\u001b[39mcsr_matrix)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py:687\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetnnz()\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: values not found"
     ]
    }
   ],
   "source": [
    "isinstance(a.values.ravel(),sp.csr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7a75407-270c-4276-926b-59fe2b613fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    print('\\n\\t ---------- Training Decision Tree Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline2 = Pipeline([('fs', fsl),\n",
    "                          ('clf2', DecisionTreeClassifier(random_state=40))])\n",
    "    clf2_parameters = {\n",
    "        'fs__k' : [500,5000,10000],\n",
    "        'clf2__criterion':['entropy','gini',], \n",
    "        'clf2__max_features':['sqrt', 'log2',None],\n",
    "        'clf2__max_depth':[10,50,100,500],\n",
    "        'clf2__ccp_alpha':[0.002,0.01,0.1,]\n",
    "        }\n",
    "    grid_search2 = GridSearchCV(estimator=pipeline2, param_grid=clf2_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search2.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search2.best_score_}\")\n",
    "    clf2 = grid_search2.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf2)\n",
    "    predicted_class_labels2 = clf2.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels2))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4062c346-0457-46a8-ad89-6a3293925b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    print('\\n\\t ---------- Training KNN Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline3 = Pipeline([('fs', fsl),\n",
    "                          ('clf3', KNeighborsClassifier())])\n",
    "    clf3_parameters = {\n",
    "        'fs__k' : [500,3000,10000],\n",
    "        'clf3__n_neighbors': [3,5,10,20,35],\n",
    "        'clf3__weights':['uniform', 'distance',],\n",
    "        'clf3__p':[1,2,],\n",
    "        'clf3__metric':['euclidean', 'manhattan',] \n",
    "        }\n",
    "    grid_search3 = GridSearchCV(estimator=pipeline3, param_grid=clf3_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search3.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search3.best_score_}\")\n",
    "    clf3 = grid_search3.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf3) \n",
    "    predicted_class_labels3 = clf3.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels3))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "312880c9-7032-464e-bb93-cccb13d9e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianNBb(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    # Gaussian Naive Bayes\n",
    "    print('\\n\\t ---------- Training Gaussian Naive Bayes Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline4 = Pipeline([('fs', fsl),\n",
    "                          ('clf4', GaussianNB())])\n",
    "    clf4_parameters = {\n",
    "        'fs__k' : [500,3000,10000],\n",
    "        'clf4__var_smoothing': np.logspace(0,-9, num=60)\n",
    "        }\n",
    "    grid_search4 = GridSearchCV(estimator=pipeline4, param_grid=clf4_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search4.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search4.best_score_}\")\n",
    "    clf4 = grid_search4.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf4) \n",
    "    predicted_class_labels4 = clf4.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels4))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "62a9a628-b55f-4c29-b899-b760508ed1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultinomialNBb(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    print('\\n\\t ---------- Training Multinomial Naive Bayes Classifier ---------- \\n')  \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline5 = Pipeline([('fs', fsl),\n",
    "                          ('clf5', MultinomialNB(fit_prior=True, class_prior=None))])#('fs', fsl),\n",
    "    clf5_parameters = {\n",
    "        'fs__k' : [10000],#,3000,10000],\n",
    "        'clf5__alpha':[20]\n",
    "        }\n",
    "    grid_search5 = GridSearchCV(estimator=pipeline5, param_grid=clf5_parameters, n_jobs=-1, cv=2, scoring='f1_macro',verbose=1)\n",
    "    grid_search5.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search5.best_score_}\")\n",
    "    clf5 = grid_search5.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf5) \n",
    "    predicted_class_labels5 = clf5.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels5))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18242a-f3b3-4a7a-b85c-9304a68ed079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e90e3426-3da7-47a0-a243-9a101c1ca0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearSVCb(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    print('\\n\\t ---------- Training Linear SVC Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline8 = Pipeline([('fs', fsl),\n",
    "                          ('clf8', LinearSVC(class_weight='balanced'))])\n",
    "    clf8_parameters = {\n",
    "        'fs__k' : [500,3000,10000],\n",
    "        'clf8__C':[0.001,0.01,1,100,],\n",
    "        }  \n",
    "    grid_search8 = GridSearchCV(estimator=pipeline8, param_grid=clf8_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search8.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search8.best_score_}\")\n",
    "    clf8 = grid_search8.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf8) \n",
    "    predicted_class_labels8 = clf8.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels8))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "866e17ab-6d5f-4078-945b-ba45ad831a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVCb(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    print('\\n\\t ---------- Training SVM Classifier ---------- \\n') \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline9 = Pipeline([('fs', fsl),\n",
    "                          ('clf9', SVC(probability=True))])\n",
    "    clf9_parameters = {\n",
    "        'fs__k' : [500,3000,10000],\n",
    "        'clf9__C':[0.1,1,50,80,100,150],\n",
    "        'clf9__kernel':['poly','linear','sigmoid',],\n",
    "        }\n",
    "    grid_search9 = GridSearchCV(estimator=pipeline9, param_grid=clf9_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search9.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search9.best_score_}\")\n",
    "    clf9 = grid_search9.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf9) \n",
    "    predicted_class_labels9 = clf9.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels9))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "872f36c9-9237-41c0-9e97-8263e899f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(x_t,x_v,y_t, y_v,fs):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    fs = fs\n",
    "    \n",
    "    if fs == 'MI' :\n",
    "        fsl =  SelectKBest(score_func = mutual_info_classif)\n",
    "    elif fs == 'chi2':\n",
    "        fsl =  SelectKBest(chi2)\n",
    "    \n",
    "    print('\\n\\t ---------- Training Random Forest Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline10 = Pipeline([('fs', fsl),\n",
    "                           ('clf10', RandomForestClassifier(class_weight='balanced'))])\n",
    "    clf10_parameters = {\n",
    "        'fs__k' : [500,3000,10000],\n",
    "        'clf10__criterion':['entropy','gini',],\n",
    "        'clf10__max_depth':[10,50,80,120],\n",
    "        'clf10__n_estimators':[30,50,],\n",
    "        'clf10__max_features':['sqrt','log2',None,] \n",
    "        } \n",
    "    grid_search10 = GridSearchCV(estimator=pipeline10, param_grid=clf10_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search10.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search10.best_score_}\")\n",
    "    clf10 = grid_search10.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf10) \n",
    "    predicted_class_labels10 = clf10.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels10))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d6906-819b-461c-bb94-ef7e8ade876e",
   "metadata": {},
   "source": [
    "##### bow training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d120bdf9-20cc-45e9-ad6f-2b85932ca0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = joblib.load('X_train_bow_13l.joblib', mmap_mode='c')\n",
    "X_valid_bow = joblib.load('X_valid_bow_13l.joblib', mmap_mode='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25732b7b-190b-4b6a-b6ba-196bfa8a8c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10f33edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t ---------- Training Logistic Regression Classifier ---------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/khadga_19024/khadga_19024/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score on Training set :  0.647427124607335\n",
      "\n",
      "\n",
      " The best set of parameters of the pipeline in Training Phase are: \n",
      "Pipeline(steps=[('fs',\n",
      "                 SelectKBest(k=10000,\n",
      "                             score_func=<function chi2 at 0x7fa19eaafa30>)),\n",
      "                ('clf1', SGDClassifier(loss='log_loss'))])\n",
      "\n",
      " *******  Scores on Validation Data  ******* \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.71      0.67     80866\n",
      "           1       0.67      0.59      0.63     80867\n",
      "\n",
      "    accuracy                           0.65    161733\n",
      "   macro avg       0.65      0.65      0.65    161733\n",
      "weighted avg       0.65      0.65      0.65    161733\n",
      "\n",
      "====================================================================\n",
      "Process Completed and time taken is : 14.34 minutes\n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "clf1bc = logistic(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9776e52-93d9-414e-9a51-73c4cbe8d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1bm = logistic(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b763af64-0c3d-48bf-be69-455d87093cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2bc = DecisionTree(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded35ba-62d3-47d3-8cf0-d383c0d97a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2bm = DecisionTree(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300aeb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3bc = KNN(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da75baa-0b02-487e-93b3-61b7da10f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3bm = KNN(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c25d54-04e5-4196-8ee3-c6619eec9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4bc = GaussianNBb(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e75aa9-2ec1-440c-9e28-d23582bdcf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4bm = GaussianNBb(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "059c76d8-7ac1-4157-bd3f-b30c292adbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t ---------- Training Multinomial Naive Bayes Classifier ---------- \n",
      "\n",
      "Fitting 2 folds for each of 150 candidates, totalling 300 fits\n",
      "Best score on Training set :  0.6364028963396402\n",
      "\n",
      "\n",
      " The best set of parameters of the pipeline in Training Phase are: \n",
      "Pipeline(steps=[('fs',\n",
      "                 SelectKBest(k=10000,\n",
      "                             score_func=<function chi2 at 0x7ff18f10bb50>)),\n",
      "                ('clf5', MultinomialNB(alpha=13))])\n",
      "\n",
      " *******  Scores on Validation Data  ******* \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.65      0.64     80866\n",
      "           1       0.64      0.63      0.64     80867\n",
      "\n",
      "    accuracy                           0.64    161733\n",
      "   macro avg       0.64      0.64      0.64    161733\n",
      "weighted avg       0.64      0.64      0.64    161733\n",
      "\n",
      "====================================================================\n",
      "Process Completed and time taken is : 0.3 minutes\n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "clf5bc = MultinomialNBb(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c76dc36f-0f9e-4098-a3f6-5e18b49e4e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t ---------- Training Multinomial Naive Bayes Classifier ---------- \n",
      "\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "Best score on Training set :  0.6352937636565796\n",
      "\n",
      "\n",
      " The best set of parameters of the pipeline in Training Phase are: \n",
      "Pipeline(steps=[('fs',\n",
      "                 SelectKBest(k=10000,\n",
      "                             score_func=<function mutual_info_classif at 0x7ff18f5427a0>)),\n",
      "                ('clf5', MultinomialNB(alpha=20))])\n",
      "\n",
      " *******  Scores on Validation Data  ******* \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.67      0.65     80866\n",
      "           1       0.65      0.61      0.63     80867\n",
      "\n",
      "    accuracy                           0.64    161733\n",
      "   macro avg       0.64      0.64      0.64    161733\n",
      "weighted avg       0.64      0.64      0.64    161733\n",
      "\n",
      "====================================================================\n",
      "Process Completed and time taken is : 160.78 minutes\n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "clf5bm = MultinomialNBb(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf0c31e-e853-4f83-b390-3460b3ae6dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8bc = LinearSVCb(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd6f57a-9f02-4c31-b731-54cde7be9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8bm = LinearSVCb(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1572afa-c610-4892-b49b-1ed3b9e999ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9bc = SVCb(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56d6c1-b241-499f-a5c7-cc9892707bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9bm = SVCb(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89385cd7-cd92-4f24-bf87-c2d194755270",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10bc = RandomForest(X_train_bow, X_valid_bow, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3992524d-3aad-4d44-be78-9dbcc2329316",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10bm = RandomForest(X_train_bow, X_valid_bow, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ac6e8-90dd-4059-9e20-7061c98baab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_bow,X_valid_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b10c5-0817-4397-b2d8-f6e21f305e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d66e9-bf41-4d82-813e-009ace700d58",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a694d-1420-492a-be26-eaa2d5a9883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = joblib.load('X_train_tfidf.joblib', mmap_mode='c')\n",
    "X_valid_tfidf = joblib.load('X_valid_tfidf.joblib', mmap_mode='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a6c64-c1e0-49a3-b3c4-7bb77604f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1tc = logistic(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22929f-db7a-4493-924f-36036bf39b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1tm = logistic(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4cca78-607b-4713-b538-6cfc946235b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2tc = DecisionTree(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a258a-77fc-4f38-b804-69eb76cef9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2tm = DecisionTree(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a08893d-0d69-4945-9f9d-7f463066a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3tc = KNN(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63dbce-05cf-410b-b8d7-e6b53db1d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3tm = KNN(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9240e4-bc01-44e6-b380-7deab784d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4tc = GaussianNBb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca8879-7609-44a6-8f2c-6bafe0aa6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4tm = GaussianNBb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d252848f-7241-4ad2-b289-7cc70baf1819",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5tc = MultinomialNBb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe10576-a218-40ae-9b0c-7ca4a0224fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5tm = MultinomialNBb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa4884e-b898-4242-a5c7-a450e88cb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8tc = LinearSVCb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb685f67-5a73-477f-834b-f813fa4d47bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8tm = LinearSVCb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0259e95-4ba1-464c-a158-f986aba9b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9tc = SVCb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94b188-e13b-4720-bc27-0777e37265ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9tm = SVCb(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82e641-2540-437c-8168-4654954abd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10tc = RandomForest(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ec61b-a22d-4402-a2dc-64c1f5ae35af",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10tm = RandomForest(X_train_tfidf,X_valid_tfidf, y_train, y_valid,'MI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c183331-d250-4b6c-af39-70719178f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_tfidf,X_valid_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab6c25-ca12-4225-900c-e9570a3dc8bc",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250738ad-0a1b-4f1e-bb68-72e0720da137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticw2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Logistic Regression Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline1 = Pipeline([('clf1', LogisticRegression(class_weight='balanced'))])\n",
    "    clf1_parameters = {\n",
    "        'clf1__penalty':['l1', 'l2', 'elasticnet',],\n",
    "        'clf1__C':[10,0.01,0.001,0.003],     \n",
    "        'clf1__solver':['newton-cg','liblinear','sag',]\n",
    "        }\n",
    "    grid_search1 = GridSearchCV(estimator=pipeline1, param_grid=clf1_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search1.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search1.best_score_}\")\n",
    "    clf1 = grid_search1.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf1) \n",
    "    predicted_class_labels1 = clf1.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels1))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e05092-268b-4298-951b-fba34603fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTreew2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Decision Tree Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline2 = Pipeline([('clf2', DecisionTreeClassifier(random_state=40))])\n",
    "    clf2_parameters = {\n",
    "        'clf2__criterion':['entropy','gini',], \n",
    "        'clf2__max_features':['sqrt', 'log2',None],\n",
    "        'clf2__max_depth':[10,25,40,100],\n",
    "        'clf2__ccp_alpha':[0.002,0.01,0.1,]\n",
    "        }\n",
    "    grid_search2 = GridSearchCV(estimator=pipeline2, param_grid=clf2_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search2.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search2.best_score_}\")\n",
    "    clf2 = grid_search2.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf2)\n",
    "    predicted_class_labels2 = clf2.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels2))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3611085-7329-48b8-bd6b-4c8440a985da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNw2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training KNN Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline3 = Pipeline([('clf3', KNeighborsClassifier())])\n",
    "    clf3_parameters = {\n",
    "        'clf3__n_neighbors': [5,10,20,35,],      \n",
    "        'clf3__weights':['uniform', 'distance',],\n",
    "        'clf3__p':[1,2,],\n",
    "        'clf3__metric':['euclidean', 'manhattan',] \n",
    "        }\n",
    "    grid_search3 = GridSearchCV(estimator=pipeline3, param_grid=clf3_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search3.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search3.best_score_}\")\n",
    "    clf3 = grid_search3.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf3) \n",
    "    predicted_class_labels3 = clf3.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels3))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2abbe-cc53-440c-b5fe-2d6a28575164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianNBw2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    # Gaussian Naive Bayes\n",
    "    print('\\n\\t ---------- Training Gaussian Naive Bayes Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline4 = Pipeline([('clf4', GaussianNB())])\n",
    "    clf4_parameters = {\n",
    "        'clf4__var_smoothing': np.logspace(0,-9, num=60)\n",
    "        }\n",
    "    grid_search4 = GridSearchCV(estimator=pipeline4, param_grid=clf4_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search4.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search4.best_score_}\")\n",
    "    clf4 = grid_search4.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf4) \n",
    "    predicted_class_labels4 = clf4.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels4))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c03430-d20c-4474-8a61-d0d10aff7fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultinomialNBw2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Multinomial Naive Bayes Classifier ---------- \\n')  \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline5 = Pipeline([('clf5', MultinomialNB(fit_prior=True, class_prior=None))])\n",
    "    clf5_parameters = {\n",
    "        'clf5__alpha':[0,1,]\n",
    "        }\n",
    "    grid_search5 = GridSearchCV(estimator=pipeline5, param_grid=clf5_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search5.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search5.best_score_}\")\n",
    "    clf5 = grid_search5.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf5) \n",
    "    predicted_class_labels5 = clf5.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels5))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4248fc5a-d34c-4221-96df-d42e3f57918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearSVCw2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Linear SVC Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline8 = Pipeline([('clf8', LinearSVC(class_weight='balanced'))])\n",
    "    clf8_parameters = {\n",
    "        'clf8__C':[0.001,0.01,1,100,],\n",
    "        }  \n",
    "    grid_search8 = GridSearchCV(estimator=pipeline8, param_grid=clf8_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search8.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search8.best_score_}\")\n",
    "    clf8 = grid_search8.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf8) \n",
    "    predicted_class_labels8 = clf8.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels8))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5046dcce-a2c8-4ecd-a2f4-8069dd816200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVCw2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training SVM Classifier ---------- \\n') \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline9 = Pipeline([('clf9', SVC(probability=True))])\n",
    "    clf9_parameters = {\n",
    "        'clf9__C':[0.1,1,50,80,100,150],\n",
    "        'clf9__kernel':['poly','linear','sigmoid',],\n",
    "        }\n",
    "    grid_search9 = GridSearchCV(estimator=pipeline9, param_grid=clf9_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search9.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search9.best_score_}\")\n",
    "    clf9 = grid_search9.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf9) \n",
    "    predicted_class_labels9 = clf9.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels9))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d82b6b-3ff0-4ab9-ac4c-70d3ed78d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForestw2v(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Random Forest Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline10 = Pipeline([('clf10', RandomForestClassifier(class_weight='balanced'))])\n",
    "    clf10_parameters = {\n",
    "        'clf10__criterion':['entropy','gini',],\n",
    "        'clf10__max_depth':[10,30,80,120,],\n",
    "        'clf10__n_estimators':[30,50,],\n",
    "        'clf10__max_features':['sqrt','log2',None,] \n",
    "        } \n",
    "    grid_search10 = GridSearchCV(estimator=pipeline10, param_grid=clf10_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search10.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search10.best_score_}\")\n",
    "    clf10 = grid_search10.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf10) \n",
    "    predicted_class_labels10 = clf10.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels10))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dabe43-9317-4d69-b825-7e6f90997ea4",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2705d58c-176c-4d06-8856-7c1433879e31",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'w2v_spacy_train.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m w2v_spacy_train \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw2v_spacy_train.joblib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m w2v_spacy_valid \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2v_spacy_valid.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m, mmap_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:579\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    577\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'w2v_spacy_train.joblib'"
     ]
    }
   ],
   "source": [
    "w2v_spacy_train = joblib.load('w2v_spacy_train.joblib', mmap_mode='c')\n",
    "w2v_spacy_valid = joblib.load('w2v_spacy_valid.joblib', mmap_mode='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c660aac-e307-4cc0-9e3f-c92d60ee7b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1w = logisticw2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1b805-c7ed-403d-a547-98f2af6c70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2w = DecisionTreew2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce55bb3e-16c8-446d-baa9-4fe81a36e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3w = KNNw2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0632ef3-30b8-4bb2-9e0c-622ffc7c5c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4w = GaussianNBw2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee2c4a-15c3-4853-a64f-b40606b598e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5w = MultinomialNBw2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36864034-bd77-4299-9eaa-04fddb0365e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8w = LinearSVCw2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf04497-423c-4411-8fdf-ea30a6cd212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9w = SVCw2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e73262-effe-41a0-bbfc-ecdd8fb54afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10w = RandomForestw2v(w2v_spacy_train,w2v_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a918da4-a22b-47f6-985a-9ad919292a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "del w2v_spacy_train,w2v_spacy_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb7c256-b660-4d89-8cd4-e3256a467207",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25760a1d-ca69-4cd6-bcca-984347760b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_spacy_train,glove_spacy_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c5966-516f-4121-a118-6f67792270cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1g = logisticw2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1bd1a-a8a0-4b79-b44e-4b08f288b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2g = DecisionTreew2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069bf096-9318-4e3f-aade-30a66c80466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3g = KNNw2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7440d6d5-d6cd-4a04-a133-82668d479192",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4g = GaussianNBw2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce145f78-fc37-455f-ab1f-daf6dd400424",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5g = MultinomialNBw2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719373de-f089-41e4-b110-ff39f00fe9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8g = LinearSVCw2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3faec-917f-40e3-b9a2-3c24763ce9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9g = SVCw2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee459e29-c359-4ddc-be8b-bc49be276fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10g = RandomForestw2v(glove_spacy_train,glove_spacy_valid, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de72d5c-a1f8-480d-a1db-ef267908e11f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40e34ecc-662f-41db-9061-b2baba0b314a",
   "metadata": {},
   "source": [
    "### with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a32b0f-4d08-4934-bfaa-bbeb9a9656c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticpca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Logistic Regression Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline1 = Pipeline([('fs',PCA(n_components = 0.98)),\n",
    "                          ('clf1', LogisticRegression(class_weight='balanced'))])\n",
    "    clf1_parameters = {\n",
    "        'clf1__penalty':[l1', 'l2', 'elasticnet',],\n",
    "        'clf1__C':[10,0.01,0.001,0.003,],\n",
    "        'clf1__solver':['newton-cg','liblinear','sag',]\n",
    "        }\n",
    "    grid_search1 = GridSearchCV(estimator=pipeline1, param_grid=clf1_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search1.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search1.best_score_}\")\n",
    "    clf1 = grid_search1.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf1) \n",
    "    predicted_class_labels1 = clf1.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels1))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4686ea4-8d38-48a8-95f8-8ea63d5588d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTreepca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Decision Tree Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline2 = Pipeline([('fs', PCA(n_components = 0.98)),\n",
    "                          ('clf2', DecisionTreeClassifier(random_state=40))])\n",
    "    clf2_parameters = {\n",
    "        'clf2__criterion':['entropy','gini',], \n",
    "        'clf2__max_features':['sqrt', 'log2',None],\n",
    "        'clf2__max_depth':[10,25,40,100],\n",
    "        'clf2__ccp_alpha':[0.002,0.01,0.1,]\n",
    "        }\n",
    "    grid_search2 = GridSearchCV(estimator=pipeline2, param_grid=clf2_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search2.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search2.best_score_}\")\n",
    "    clf2 = grid_search2.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf2)\n",
    "    predicted_class_labels2 = clf2.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels2))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bfbaa3-62a0-4858-bef8-4730df7f473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNNpca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training KNN Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline3 = Pipeline([('fs', PCA(n_components = 0.98)),\n",
    "                          ('clf3', KNeighborsClassifier())])\n",
    "    clf3_parameters = {\n",
    "        'clf3__n_neighbors': [3,10,20,35,],      \n",
    "        'clf3__weights':['uniform', 'distance',],\n",
    "        'clf3__p':[1,2,],\n",
    "        'clf3__metric':['euclidean', 'manhattan',] \n",
    "        }\n",
    "    grid_search3 = GridSearchCV(estimator=pipeline3, param_grid=clf3_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search3.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search3.best_score_}\")\n",
    "    clf3 = grid_search3.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf3) \n",
    "    predicted_class_labels3 = clf3.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels3))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad5cf3-1807-416a-8af3-0bc60ff9a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianNBpca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    # Gaussian Naive Bayes\n",
    "    print('\\n\\t ---------- Training Gaussian Naive Bayes Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline4 = Pipeline([('fs', PCA(n_components = 0.98)),\n",
    "                          ('clf4', GaussianNB())])\n",
    "    clf4_parameters = {\n",
    "        'clf4__var_smoothing': np.logspace(0,-9, num=60)\n",
    "        }\n",
    "    grid_search4 = GridSearchCV(estimator=pipeline4, param_grid=clf4_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search4.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search4.best_score_}\")\n",
    "    clf4 = grid_search4.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf4) \n",
    "    predicted_class_labels4 = clf4.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels4))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499b71a-43b2-4323-b3f8-27f9e3242cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultinomialNBpca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Multinomial Naive Bayes Classifier ---------- \\n')  \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline5 = Pipeline([('fs', PCA(n_components = 0.98)),\n",
    "                          ('clf5', MultinomialNB(fit_prior=True, class_prior=None))])\n",
    "    clf5_parameters = {\n",
    "        'clf5__alpha':[0,1,]\n",
    "        }\n",
    "    grid_search5 = GridSearchCV(estimator=pipeline5, param_grid=clf5_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search5.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search5.best_score_}\")\n",
    "    clf5 = grid_search5.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf5) \n",
    "    predicted_class_labels5 = clf5.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels5))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b754d39-b568-4bb8-bd7b-47945dc1fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearSVCpca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Linear SVC Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline8 = Pipeline([('fs', PCA(n_components = 0.98)),\n",
    "                          ('clf8', LinearSVC(class_weight='balanced'))])\n",
    "    clf8_parameters = {\n",
    "        'clf8__C':[0.001,0.01,1,100,],\n",
    "        }  \n",
    "    grid_search8 = GridSearchCV(estimator=pipeline8, param_grid=clf8_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search8.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search8.best_score_}\")\n",
    "    clf8 = grid_search8.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf8) \n",
    "    predicted_class_labels8 = clf8.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels8))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6bf36a-0080-47d6-8f73-4427b5a4cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVCpca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training SVM Classifier ---------- \\n') \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline9 = Pipeline([('fs', PCA(n_components = 0.98)),\n",
    "                          ('clf9', SVC(probability=True))])\n",
    "    clf9_parameters = {\n",
    "        'clf9__C':[0.1,1,50,80,100,150],\n",
    "        'clf9__kernel':['poly','linear','sigmoid',],\n",
    "        }\n",
    "    grid_search9 = GridSearchCV(estimator=pipeline9, param_grid=clf9_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search9.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search9.best_score_}\")\n",
    "    clf9 = grid_search9.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf9) \n",
    "    predicted_class_labels9 = clf9.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels9))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76752c74-653e-4041-97af-030a844b926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForestpca(x_t,x_v,y_t, y_v):\n",
    "    t_start = time.time() # in seconds\n",
    "    x_train = x_t\n",
    "    x_valid = x_v\n",
    "    y_train = y_t\n",
    "    y_valid = y_v\n",
    "    \n",
    "    print('\\n\\t ---------- Training Random Forest Classifier ---------- \\n')\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "    pipeline10 = Pipeline([('fs', PCA(n_components = 0.98)),\n",
    "                           ('clf10', RandomForestClassifier(class_weight='balanced'))])\n",
    "    clf10_parameters = {\n",
    "        'clf10__criterion':['entropy','gini',],\n",
    "        'clf10__max_depth':[10,30,80,120,],\n",
    "        'clf10__n_estimators':[30,50,100,],\n",
    "        'clf10__max_features':['sqrt','log2',None,] \n",
    "        } \n",
    "    grid_search10 = GridSearchCV(estimator=pipeline10, param_grid=clf10_parameters, n_jobs=-1, cv=cv, scoring='f1_macro')\n",
    "    grid_search10.fit(x_train,y_train)\n",
    "    print(f\"Best score on Training set :  {grid_search10.best_score_}\")\n",
    "    clf10 = grid_search10.best_estimator_\n",
    "    print('\\n\\n The best set of parameters of the pipeline in Training Phase are: ')\n",
    "    print(clf10) \n",
    "    predicted_class_labels10 = clf10.predict(x_valid)  # validation\n",
    "\n",
    "    print('\\n *******  Scores on Validation Data  ******* \\n ')\n",
    "    print(classification_report(y_valid, predicted_class_labels10))\n",
    "    \n",
    "    t_ends = time.time() # in seconds\n",
    "    t_net = (t_ends - t_start)/60    # in minutes\n",
    "    net_time = round(t_net,2)\n",
    "    print(\"====================================================================\")\n",
    "    print(f\"Process Completed and time taken is : {net_time} minutes\")\n",
    "    print(\"====================================================================\")\n",
    "    \n",
    "    return clf10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42187c-97ac-4071-ae82-9a95027c7baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1pca = logisticpca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c8474c-2425-4d58-930d-81150a574e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2pca = DecisionTreepca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598c388-0592-4ec5-b8d9-91733c2afe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3pca = KNNpca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b825be-1f4a-4ceb-86fc-41b2b010cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4pca = GaussianNBpca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554bcf3-e80d-467f-8006-5b459d9b9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5pca = MultinomialNBpca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c3748-0679-4107-a423-465de7c34449",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf8pca = LinearSVCpca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b6939-978d-4a99-acaf-409bc892987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf9pca = SVCpca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e225b7-3b85-45ed-9074-7f6ec50c94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10pca = RandomForestpca(X_train_bow, X_valid_bow, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9603bd7f-a292-4a81-913d-4d2a50afc156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e178d-01ad-4057-a655-ccc186cdd76d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7eaae-6933-48b2-90f5-c8e6ccd71b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491a0fb-23dc-47e8-b9d2-f915043a6f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2639019-cdde-4f64-9feb-e027b5ec0150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21f46c-820a-4b81-ac0e-bff95451e696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb3da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "print('\\n Total documents in the training set: '+str(len(trn_data))+'\\n')    \n",
    "print('\\n Total documents in the test set: '+str(len(tst_data))+'\\n') \n",
    "\n",
    "pr=precision_score(tst_cat, predicted, average='binary') \n",
    "print ('\\n Precision:'+str(pr)) \n",
    "\n",
    "rl=recall_score(tst_cat, predicted, average='binary') \n",
    "print ('\\n Recall:'+str(rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a669e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab5936",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1_mis = logistic_fs(x_trains,x_valids,y_train, y_valid,'MI') # by standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7d6e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1_anovas = logistic_fs(x_trains,x_valids,y_train, y_valid,'ANOVA') # Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f6eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895003f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c4eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df5918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4635ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get name of the 5 best features in decresing order i.e 1st feature is high import. then 2nd fet then 3rd so on\n",
    "X_train.columns[sel_five_cols.get_support()] # x_train is the same as my x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffl = logistic_fsf(x_trains,x_valids,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56275aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predl = ffl.predict(x_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving predicted class labels as txt file\n",
    "np.savetxt('Akash_Singh_test_class_labels.txt',predl,fmt='%d',delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c84942b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd43775-a976-4f79-9876-dd262f78515c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
