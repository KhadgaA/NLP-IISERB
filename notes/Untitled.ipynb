{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "251974d1-4995-4293-b048-58e0ca012bbe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e1aae6-eccd-413d-a8fb-c59a5c66ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build tokenizer\n",
    "def tokenizer(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5614f230-59dd-4c08-b6e1-8b158b8f80fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c76a69f2-fff9-4759-a963-f48509b7ab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'ca', \"n't\", 'run', 'whole', 'day']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"I can't run whole day\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d949904-622e-4a14-87a9-d04e3eba79e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.data import *\n",
    "# TEXT = data.Field(tokenize=tokenizer, use_vocab=True, lower=True, batch_first=True, include_lengths=True)\n",
    "# LABEL = data.LabelField(dtype=torch.long, batch_first=True, sequential=False)\n",
    "# fields = [('text', TEXT), ('label', LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "610c1ba4-170b-4eaf-adcb-a053694e9e50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m----> 2\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomTextDataset\u001b[39;00m(Dataset):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, text, labels):\n",
    "        self.labels = labels\n",
    "        self.text = text\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        text = self.text[idx]\n",
    "        sample = {\"Text\": text, \"labels\": label}\n",
    "        return sample\n",
    "\n",
    "TrainingDataObj = CustomTextDataset(train_data['text'], train_data['label'])\n",
    "TestDataObj= CustomTextDataset(test_data['text'], test_data['label'])\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    \n",
    "    for asample in batch:\n",
    "        label_list.append(label_pipeline(asample['labels']))\n",
    "        #print (label_list)\n",
    "        processed_text = torch.tensor(text_pipeline(asample['Text']) , dtype=torch.int64)  \n",
    "        #print (processed_text.size())\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    #text_list = torch.stack(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "Traindataloader = DataLoader(TrainingDataObj, batch_size=5, shuffle=True, collate_fn= collate_batch)\n",
    "\n",
    "for idx, (label, text, offsets) in enumerate(Traindataloader):\n",
    "    print(text.size())\n",
    "    print(label)\n",
    "    print(offsets.int())\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9bee67b-c080-4f2b-9c73-e25ade739530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1adb1cf6-8b7d-4525-b573-016807d13012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "df_prepro_train = pd.read_csv('processed_train.csv',na_filter=False)\n",
    "# Train data\n",
    "X_trn = df_prepro_train['Pre Processed Text']\n",
    "y_trn = df_prepro_train['Class Labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961285d6-e9bc-495c-8752-1319cab4d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.data import datasets_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9632d3-e091-492e-89c2-964fcae1dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775976c0-0263-4122-aa3f-a725d31db430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ec1d29-8b17-40a8-8eb1-843118c6994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "# from skimage import io, transform\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6d27f2-d439-4cd7-bcc6-a687a46b6d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({\"Text\":X_train,'Label':y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef444912-f763-4ffb-9d49-e1fe23625c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9190f79d-6f3c-4279-8b39-160913573daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KHADGA JYOTH ALLI\\anaconda3\\envs\\gpu\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_trn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mX_trn\u001b[49m, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.33\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_trn' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_trn, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48e001-90e4-46e0-b774-3df96437cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Text\":X_train,'Label':y_train})\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df_test = pd.DataFrame({\"Text\":X_test,'Label':y_test})\n",
    "df_test.reset_index(drop=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d155d6-86f0-4ac8-82e0-a5a9fe7602d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c0b5ed0-711f-474d-b948-f93bdc37595c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m get_tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic_english\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m counter_obj \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mCounter()\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m atext \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     11\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer(atext)\n\u001b[0;32m     12\u001b[0m     counter_obj\u001b[38;5;241m.\u001b[39mupdate(tokens)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Now we need to create a vocabulary from our train text\n",
    "#We need two things (i) A tokenizer that will tokenize a given text (ii) A counter object that will count occurance of tokens\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "import collections\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "counter_obj = collections.Counter()\n",
    "\n",
    "for atext in df['Text']:\n",
    "    tokens = tokenizer(atext)\n",
    "    counter_obj.update(tokens)\n",
    "    \n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "vocabobj = vocab(counter_obj, min_freq = 1, specials=['<unk>'] )\n",
    "vocabobj.set_default_index(vocabobj['<unk>'])\n",
    "vocabobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3167e177-7fd3-482a-b63a-63fcfee7096e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m text_pipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: trp(vocabobj(tokenizer(x)),inp_len)\n\u001b[0;32m      6\u001b[0m label_pipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m aline \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;28mlen\u001b[39m(text_pipeline(aline)))\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def trp(l, n):\n",
    "    return l[:n] + [0]*(n-len(l))\n",
    "\n",
    "inp_len = 100\n",
    "text_pipeline = lambda x: trp(vocabobj(tokenizer(x)),inp_len)\n",
    "label_pipeline = lambda x: x\n",
    "\n",
    "for aline in df['Text']:\n",
    "    print (len(text_pipeline(aline)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b987e9c2-e20a-4507-8ab3-5e72916c81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.nn import functional as F\n",
    "class NLPDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.text = data['Text']\n",
    "        self.labels = data['Label']\n",
    "        # self.len_data = data.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # img_name = os.path.join(self.root_dir,\n",
    "        #                         self.landmarks_frame.iloc[idx, 0])\n",
    "        # image = io.imread(img_name)\n",
    "        # landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        # landmarks = np.array([landmarks])\n",
    "        # landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        # data = self.data.iloc[idx]\n",
    "        text = self.text[idx]\n",
    "        label = self.labels[idx]\n",
    "        sample = {'labels': label, 'Text': text}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5df2e20a-5058-4b97-96ff-a997d6e19dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500])\n",
      "tensor([0, 1, 0, 1, 0])\n",
      "tensor([  0, 100, 200, 300, 400], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "TrainingDataObj = NLPDataset(df)\n",
    "TestDataObj= NLPDataset(df_test)\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    \n",
    "    for asample in batch:\n",
    "        label_list.append(label_pipeline(asample['labels']))\n",
    "        #print (label_list)\n",
    "        processed_text = torch.tensor(text_pipeline(asample['Text']) , dtype=torch.int64)  \n",
    "        #print (processed_text.size())\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    #text_list = torch.stack(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "Traindataloader = DataLoader(TrainingDataObj, batch_size=5, shuffle=True, collate_fn= collate_batch)\n",
    "\n",
    "for idx, (label, text, offsets) in enumerate(Traindataloader):\n",
    "    print(text.size())\n",
    "    print(label)\n",
    "    print(offsets.int())\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eff6e62b-57dd-4331-ac7c-4c4d3b0bd130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': 0,\n",
       " 'Text': 'away day view parties everyone watch match sunday get bus come watch'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainingDataObj[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e78ef5aa-c58a-4175-999d-a4a48b0bc854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf5ece9-15a8-4fcc-a624-626c896de813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efcb698e-218c-4d2a-8589-c7e81b28980e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc472ef9-9abb-49d6-8bee-d42bfa87d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(set(TrainingDataObj.labels))\n",
    "vocab_size = len(vocabobj)\n",
    "emsize = 64\n",
    "input_size = inp_len\n",
    "hidden_size=20\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61b62347-41ed-4000-b661-72225f7d0ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142524"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10ca8994-a321-4fe0-b4bf-de45561ae740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 1000\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad46d64-d424-4a74-81d0-0e30a1433993",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(TrainingDataObj, batch_size=256, shuffle=True, collate_fn= collate_batch)\n",
    "test_dataloader = DataLoader(TestDataObj, batch_size=256, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(test_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'test accuracy {:8.3f} '.format(epoch, time.time() - epoch_start_time, accu_val))\n",
    "    print('-' * 59)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24581e5c-a239-4d4c-8231-3e96cdc6fb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "36bd483dee05de20fc9cccdc42740228031800ee4850f9b0962b2a23ef254cc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
